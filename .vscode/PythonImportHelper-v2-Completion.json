[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tqdm",
        "description": "tqdm",
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "PIL",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PIL",
        "description": "PIL",
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "functools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "functools",
        "description": "functools",
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "cache",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "Pool",
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "isExtraImport": true,
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "MultiviewHabitatSimGenerator",
        "importPath": "datasets.habitat_sim.multiview_habitat_sim_generator",
        "description": "datasets.habitat_sim.multiview_habitat_sim_generator",
        "isExtraImport": true,
        "detail": "datasets.habitat_sim.multiview_habitat_sim_generator",
        "documentation": {}
    },
    {
        "label": "MultiviewHabitatSimGenerator",
        "importPath": "datasets.habitat_sim.multiview_habitat_sim_generator",
        "description": "datasets.habitat_sim.multiview_habitat_sim_generator",
        "isExtraImport": true,
        "detail": "datasets.habitat_sim.multiview_habitat_sim_generator",
        "documentation": {}
    },
    {
        "label": "NoNaviguableSpaceError",
        "importPath": "datasets.habitat_sim.multiview_habitat_sim_generator",
        "description": "datasets.habitat_sim.multiview_habitat_sim_generator",
        "isExtraImport": true,
        "detail": "datasets.habitat_sim.multiview_habitat_sim_generator",
        "documentation": {}
    },
    {
        "label": "SCENES_DATASET",
        "importPath": "datasets.habitat_sim.paths",
        "description": "datasets.habitat_sim.paths",
        "isExtraImport": true,
        "detail": "datasets.habitat_sim.paths",
        "documentation": {}
    },
    {
        "label": "list_scenes_available",
        "importPath": "datasets.habitat_sim.paths",
        "description": "datasets.habitat_sim.paths",
        "isExtraImport": true,
        "detail": "datasets.habitat_sim.paths",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "datasets.habitat_sim.paths",
        "description": "datasets.habitat_sim.paths",
        "isExtraImport": true,
        "detail": "datasets.habitat_sim.paths",
        "documentation": {}
    },
    {
        "label": "quaternion",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "quaternion",
        "description": "quaternion",
        "detail": "quaternion",
        "documentation": {}
    },
    {
        "label": "PIL.Image",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PIL.Image",
        "description": "PIL.Image",
        "detail": "PIL.Image",
        "documentation": {}
    },
    {
        "label": "cv2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cv2",
        "description": "cv2",
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "glob",
        "importPath": "glob",
        "description": "glob",
        "isExtraImport": true,
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "habitat_sim",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "habitat_sim",
        "description": "habitat_sim",
        "detail": "habitat_sim",
        "documentation": {}
    },
    {
        "label": "NearestNeighbors",
        "importPath": "sklearn.neighbors",
        "description": "sklearn.neighbors",
        "isExtraImport": true,
        "detail": "sklearn.neighbors",
        "documentation": {}
    },
    {
        "label": "collections",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections",
        "description": "collections",
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "default_collate",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "default_collate",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "get_pair_transforms",
        "importPath": "datasets.transforms",
        "description": "datasets.transforms",
        "isExtraImport": true,
        "detail": "datasets.transforms",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "cuda",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "inf",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torchvision.transforms",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "ColorJitter",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "ToTensor",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Normalize",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "torchvision.transforms.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "BuildExtension",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "CUDAExtension",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "itertools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itertools",
        "description": "itertools",
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "collections.abc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections.abc",
        "description": "collections.abc",
        "detail": "collections.abc",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "scaled_dot_product_attention",
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "isExtraImport": true,
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "Block",
        "importPath": "models.blocks",
        "description": "models.blocks",
        "isExtraImport": true,
        "detail": "models.blocks",
        "documentation": {}
    },
    {
        "label": "DecoderBlock",
        "importPath": "models.blocks",
        "description": "models.blocks",
        "isExtraImport": true,
        "detail": "models.blocks",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "importPath": "models.blocks",
        "description": "models.blocks",
        "isExtraImport": true,
        "detail": "models.blocks",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "importPath": "models.blocks",
        "description": "models.blocks",
        "isExtraImport": true,
        "detail": "models.blocks",
        "documentation": {}
    },
    {
        "label": "get_2d_sincos_pos_embed",
        "importPath": "models.pos_embed",
        "description": "models.pos_embed",
        "isExtraImport": true,
        "detail": "models.pos_embed",
        "documentation": {}
    },
    {
        "label": "RoPE2D",
        "importPath": "models.pos_embed",
        "description": "models.pos_embed",
        "isExtraImport": true,
        "detail": "models.pos_embed",
        "documentation": {}
    },
    {
        "label": "interpolate_pos_embed",
        "importPath": "models.pos_embed",
        "description": "models.pos_embed",
        "isExtraImport": true,
        "detail": "models.pos_embed",
        "documentation": {}
    },
    {
        "label": "RandomMask",
        "importPath": "models.masking",
        "description": "models.masking",
        "isExtraImport": true,
        "detail": "models.masking",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sized",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sized",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "os.path",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.path",
        "description": "os.path",
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "struct",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "struct",
        "description": "struct",
        "detail": "struct",
        "documentation": {}
    },
    {
        "label": "h5py",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "h5py",
        "description": "h5py",
        "detail": "h5py",
        "documentation": {}
    },
    {
        "label": "data",
        "importPath": "torch.utils",
        "description": "torch.utils",
        "isExtraImport": true,
        "detail": "torch.utils",
        "documentation": {}
    },
    {
        "label": "data",
        "importPath": "torch.utils",
        "description": "torch.utils",
        "isExtraImport": true,
        "detail": "torch.utils",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "copy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "sys,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys.",
        "description": "sys.",
        "detail": "sys.",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "torchvision",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision",
        "description": "torchvision",
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "utils",
        "description": "utils",
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "misc",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "utils.misc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "utils.misc",
        "description": "utils.misc",
        "detail": "utils.misc",
        "documentation": {}
    },
    {
        "label": "NativeScalerWithGradNormCount",
        "importPath": "utils.misc",
        "description": "utils.misc",
        "isExtraImport": true,
        "detail": "utils.misc",
        "documentation": {}
    },
    {
        "label": "NativeScalerWithGradNormCount",
        "importPath": "utils.misc",
        "description": "utils.misc",
        "isExtraImport": true,
        "detail": "utils.misc",
        "documentation": {}
    },
    {
        "label": "CroCoDownstreamBinocular",
        "importPath": "models.croco_downstream",
        "description": "models.croco_downstream",
        "isExtraImport": true,
        "detail": "models.croco_downstream",
        "documentation": {}
    },
    {
        "label": "CroCoDownstreamBinocular",
        "importPath": "models.croco_downstream",
        "description": "models.croco_downstream",
        "isExtraImport": true,
        "detail": "models.croco_downstream",
        "documentation": {}
    },
    {
        "label": "croco_args_from_ckpt",
        "importPath": "models.croco_downstream",
        "description": "models.croco_downstream",
        "isExtraImport": true,
        "detail": "models.croco_downstream",
        "documentation": {}
    },
    {
        "label": "PixelwiseTaskWithDPT",
        "importPath": "models.head_downstream",
        "description": "models.head_downstream",
        "isExtraImport": true,
        "detail": "models.head_downstream",
        "documentation": {}
    },
    {
        "label": "PixelwiseTaskWithDPT",
        "importPath": "models.head_downstream",
        "description": "models.head_downstream",
        "isExtraImport": true,
        "detail": "models.head_downstream",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "stereoflow.criterion",
        "description": "stereoflow.criterion",
        "isExtraImport": true,
        "detail": "stereoflow.criterion",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "stereoflow.criterion",
        "description": "stereoflow.criterion",
        "isExtraImport": true,
        "detail": "stereoflow.criterion",
        "documentation": {}
    },
    {
        "label": "get_test_datasets_stereo",
        "importPath": "stereoflow.datasets_stereo",
        "description": "stereoflow.datasets_stereo",
        "isExtraImport": true,
        "detail": "stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "vis_disparity",
        "importPath": "stereoflow.datasets_stereo",
        "description": "stereoflow.datasets_stereo",
        "isExtraImport": true,
        "detail": "stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "get_train_dataset_stereo",
        "importPath": "stereoflow.datasets_stereo",
        "description": "stereoflow.datasets_stereo",
        "isExtraImport": true,
        "detail": "stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "get_test_datasets_stereo",
        "importPath": "stereoflow.datasets_stereo",
        "description": "stereoflow.datasets_stereo",
        "isExtraImport": true,
        "detail": "stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "get_test_datasets_flow",
        "importPath": "stereoflow.datasets_flow",
        "description": "stereoflow.datasets_flow",
        "isExtraImport": true,
        "detail": "stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "flowToColor",
        "importPath": "stereoflow.datasets_flow",
        "description": "stereoflow.datasets_flow",
        "isExtraImport": true,
        "detail": "stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "get_train_dataset_flow",
        "importPath": "stereoflow.datasets_flow",
        "description": "stereoflow.datasets_flow",
        "isExtraImport": true,
        "detail": "stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "get_test_datasets_flow",
        "importPath": "stereoflow.datasets_flow",
        "description": "stereoflow.datasets_flow",
        "isExtraImport": true,
        "detail": "stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "tiled_pred",
        "importPath": "stereoflow.engine",
        "description": "stereoflow.engine",
        "isExtraImport": true,
        "detail": "stereoflow.engine",
        "documentation": {}
    },
    {
        "label": "train_one_epoch",
        "importPath": "stereoflow.engine",
        "description": "stereoflow.engine",
        "isExtraImport": true,
        "detail": "stereoflow.engine",
        "documentation": {}
    },
    {
        "label": "validate_one_epoch",
        "importPath": "stereoflow.engine",
        "description": "stereoflow.engine",
        "isExtraImport": true,
        "detail": "stereoflow.engine",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "torch.distributed",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.distributed",
        "description": "torch.distributed",
        "detail": "torch.distributed",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.backends.cudnn",
        "description": "torch.backends.cudnn",
        "detail": "torch.backends.cudnn",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "torchvision.datasets",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.datasets",
        "description": "torchvision.datasets",
        "detail": "torchvision.datasets",
        "documentation": {}
    },
    {
        "label": "builtins",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "builtins",
        "description": "builtins",
        "detail": "builtins",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "CroCoNet",
        "importPath": "models.croco",
        "description": "models.croco",
        "isExtraImport": true,
        "detail": "models.croco",
        "documentation": {}
    },
    {
        "label": "CroCoNet",
        "importPath": "models.croco",
        "description": "models.croco",
        "isExtraImport": true,
        "detail": "models.croco",
        "documentation": {}
    },
    {
        "label": "CroCoNet",
        "importPath": "models.croco",
        "description": "models.croco",
        "isExtraImport": true,
        "detail": "models.croco",
        "documentation": {}
    },
    {
        "label": "MaskedMSE",
        "importPath": "models.criterion",
        "description": "models.criterion",
        "isExtraImport": true,
        "detail": "models.criterion",
        "documentation": {}
    },
    {
        "label": "PairsDataset",
        "importPath": "datasets.pairs_dataset",
        "description": "datasets.pairs_dataset",
        "isExtraImport": true,
        "detail": "datasets.pairs_dataset",
        "documentation": {}
    },
    {
        "label": "os,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.",
        "description": "os.",
        "detail": "os.",
        "documentation": {}
    },
    {
        "label": "glob,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob.",
        "description": "glob.",
        "detail": "glob.",
        "documentation": {}
    },
    {
        "label": "imageio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "imageio",
        "description": "imageio",
        "detail": "imageio",
        "documentation": {}
    },
    {
        "label": "depthmap_to_absolute_camera_coordinates",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "depthmap_to_absolute_camera_coordinates",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "inv",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "geotrf",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "geotrf",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "inv",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "get_med_dist_between_poses",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "xy_grid",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "geotrf",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "depthmap_to_pts3d",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "xy_grid",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "geotrf",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "inv",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "geotrf",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "depthmap_to_absolute_camera_coordinates",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "depthmap_to_absolute_camera_coordinates",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "colmap_to_opencv_intrinsics",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "opencv_to_colmap_intrinsics",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "depthmap_to_pts3d",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "geotrf",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "xy_grid",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "inv",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "geotrf",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "normalize_pointcloud",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "normalize_pointclouds",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "get_joint_pointcloud_depth",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "get_joint_pointcloud_center_scale",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "get_joint_pointcloud_depths",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "get_joint_pointcloud_center_scales",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "xy_grid",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "geotrf",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "get_med_dist_between_poses",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "inv",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "geotrf",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "inv",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "geotrf",
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "isExtraImport": true,
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "roma",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "roma",
        "description": "roma",
        "detail": "roma",
        "documentation": {}
    },
    {
        "label": "to_numpy",
        "importPath": "dust3r.utils.device",
        "description": "dust3r.utils.device",
        "isExtraImport": true,
        "detail": "dust3r.utils.device",
        "documentation": {}
    },
    {
        "label": "to_cpu",
        "importPath": "dust3r.utils.device",
        "description": "dust3r.utils.device",
        "isExtraImport": true,
        "detail": "dust3r.utils.device",
        "documentation": {}
    },
    {
        "label": "to_numpy",
        "importPath": "dust3r.utils.device",
        "description": "dust3r.utils.device",
        "isExtraImport": true,
        "detail": "dust3r.utils.device",
        "documentation": {}
    },
    {
        "label": "to_cpu",
        "importPath": "dust3r.utils.device",
        "description": "dust3r.utils.device",
        "isExtraImport": true,
        "detail": "dust3r.utils.device",
        "documentation": {}
    },
    {
        "label": "to_numpy",
        "importPath": "dust3r.utils.device",
        "description": "dust3r.utils.device",
        "isExtraImport": true,
        "detail": "dust3r.utils.device",
        "documentation": {}
    },
    {
        "label": "to_numpy",
        "importPath": "dust3r.utils.device",
        "description": "dust3r.utils.device",
        "isExtraImport": true,
        "detail": "dust3r.utils.device",
        "documentation": {}
    },
    {
        "label": "to_cpu",
        "importPath": "dust3r.utils.device",
        "description": "dust3r.utils.device",
        "isExtraImport": true,
        "detail": "dust3r.utils.device",
        "documentation": {}
    },
    {
        "label": "collate_with_cat",
        "importPath": "dust3r.utils.device",
        "description": "dust3r.utils.device",
        "isExtraImport": true,
        "detail": "dust3r.utils.device",
        "documentation": {}
    },
    {
        "label": "to_numpy",
        "importPath": "dust3r.utils.device",
        "description": "dust3r.utils.device",
        "isExtraImport": true,
        "detail": "dust3r.utils.device",
        "documentation": {}
    },
    {
        "label": "to_numpy",
        "importPath": "dust3r.utils.device",
        "description": "dust3r.utils.device",
        "isExtraImport": true,
        "detail": "dust3r.utils.device",
        "documentation": {}
    },
    {
        "label": "to_numpy",
        "importPath": "dust3r.utils.device",
        "description": "dust3r.utils.device",
        "isExtraImport": true,
        "detail": "dust3r.utils.device",
        "documentation": {}
    },
    {
        "label": "to_numpy",
        "importPath": "dust3r.utils.device",
        "description": "dust3r.utils.device",
        "isExtraImport": true,
        "detail": "dust3r.utils.device",
        "documentation": {}
    },
    {
        "label": "to_numpy",
        "importPath": "dust3r.utils.device",
        "description": "dust3r.utils.device",
        "isExtraImport": true,
        "detail": "dust3r.utils.device",
        "documentation": {}
    },
    {
        "label": "rgb",
        "importPath": "dust3r.utils.image",
        "description": "dust3r.utils.image",
        "isExtraImport": true,
        "detail": "dust3r.utils.image",
        "documentation": {}
    },
    {
        "label": "ImgNorm",
        "importPath": "dust3r.utils.image",
        "description": "dust3r.utils.image",
        "isExtraImport": true,
        "detail": "dust3r.utils.image",
        "documentation": {}
    },
    {
        "label": "imread_cv2",
        "importPath": "dust3r.utils.image",
        "description": "dust3r.utils.image",
        "isExtraImport": true,
        "detail": "dust3r.utils.image",
        "documentation": {}
    },
    {
        "label": "rgb",
        "importPath": "dust3r.utils.image",
        "description": "dust3r.utils.image",
        "isExtraImport": true,
        "detail": "dust3r.utils.image",
        "documentation": {}
    },
    {
        "label": "load_images",
        "importPath": "dust3r.utils.image",
        "description": "dust3r.utils.image",
        "isExtraImport": true,
        "detail": "dust3r.utils.image",
        "documentation": {}
    },
    {
        "label": "load_images",
        "importPath": "dust3r.utils.image",
        "description": "dust3r.utils.image",
        "isExtraImport": true,
        "detail": "dust3r.utils.image",
        "documentation": {}
    },
    {
        "label": "load_images",
        "importPath": "dust3r.utils.image",
        "description": "dust3r.utils.image",
        "isExtraImport": true,
        "detail": "dust3r.utils.image",
        "documentation": {}
    },
    {
        "label": "rgb",
        "importPath": "dust3r.utils.image",
        "description": "dust3r.utils.image",
        "isExtraImport": true,
        "detail": "dust3r.utils.image",
        "documentation": {}
    },
    {
        "label": "load_images",
        "importPath": "dust3r.utils.image",
        "description": "dust3r.utils.image",
        "isExtraImport": true,
        "detail": "dust3r.utils.image",
        "documentation": {}
    },
    {
        "label": "rgb",
        "importPath": "dust3r.utils.image",
        "description": "dust3r.utils.image",
        "isExtraImport": true,
        "detail": "dust3r.utils.image",
        "documentation": {}
    },
    {
        "label": "SceneViz",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "segment_sky",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "auto_cam_size",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "to_numpy",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "add_scene_cam",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "CAM_COLORS",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "cat_meshes",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "OPENGL",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "pts3d_to_trimesh",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "add_scene_cam",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "CAM_COLORS",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "cat_meshes",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "OPENGL",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "pts3d_to_trimesh",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "add_scene_cam",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "CAM_COLORS",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "cat_meshes",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "OPENGL",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "pts3d_to_trimesh",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "add_scene_cam",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "CAM_COLORS",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "OPENGL",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "pts3d_to_trimesh",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "cat_meshes",
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "isExtraImport": true,
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "adjust_learning_rate_by_lr",
        "importPath": "dust3r.optim_factory",
        "description": "dust3r.optim_factory",
        "isExtraImport": true,
        "detail": "dust3r.optim_factory",
        "documentation": {}
    },
    {
        "label": "edge_str",
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "isExtraImport": true,
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "ALL_DISTS",
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "isExtraImport": true,
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "NoGradParamDict",
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "isExtraImport": true,
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "get_imshapes",
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "isExtraImport": true,
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "signed_expm1",
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "isExtraImport": true,
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "signed_log1p",
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "isExtraImport": true,
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "cosine_schedule",
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "isExtraImport": true,
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "linear_schedule",
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "isExtraImport": true,
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "get_conf_trf",
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "isExtraImport": true,
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "edge_str",
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "isExtraImport": true,
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "i_j_ij",
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "isExtraImport": true,
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "compute_edge_scores",
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "isExtraImport": true,
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "edge_str",
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "isExtraImport": true,
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "dust3r.cloud_opt.init_im_poses",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dust3r.cloud_opt.init_im_poses",
        "description": "dust3r.cloud_opt.init_im_poses",
        "detail": "dust3r.cloud_opt.init_im_poses",
        "documentation": {}
    },
    {
        "label": "scipy.sparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scipy.sparse",
        "description": "scipy.sparse",
        "detail": "scipy.sparse",
        "documentation": {}
    },
    {
        "label": "estimate_focal_knowing_depth",
        "importPath": "dust3r.post_process",
        "description": "dust3r.post_process",
        "isExtraImport": true,
        "detail": "dust3r.post_process",
        "documentation": {}
    },
    {
        "label": "estimate_focal_knowing_depth",
        "importPath": "dust3r.post_process",
        "description": "dust3r.post_process",
        "isExtraImport": true,
        "detail": "dust3r.post_process",
        "documentation": {}
    },
    {
        "label": "BasePCOptimizer",
        "importPath": "dust3r.cloud_opt.base_opt",
        "description": "dust3r.cloud_opt.base_opt",
        "isExtraImport": true,
        "detail": "dust3r.cloud_opt.base_opt",
        "documentation": {}
    },
    {
        "label": "BasePCOptimizer",
        "importPath": "dust3r.cloud_opt.base_opt",
        "description": "dust3r.cloud_opt.base_opt",
        "isExtraImport": true,
        "detail": "dust3r.cloud_opt.base_opt",
        "documentation": {}
    },
    {
        "label": "BasePCOptimizer",
        "importPath": "dust3r.cloud_opt.base_opt",
        "description": "dust3r.cloud_opt.base_opt",
        "isExtraImport": true,
        "detail": "dust3r.cloud_opt.base_opt",
        "documentation": {}
    },
    {
        "label": "EasyDataset",
        "importPath": "dust3r.datasets.base.easy_dataset",
        "description": "dust3r.datasets.base.easy_dataset",
        "isExtraImport": true,
        "detail": "dust3r.datasets.base.easy_dataset",
        "documentation": {}
    },
    {
        "label": "ImgNorm",
        "importPath": "dust3r.datasets.utils.transforms",
        "description": "dust3r.datasets.utils.transforms",
        "isExtraImport": true,
        "detail": "dust3r.datasets.utils.transforms",
        "documentation": {}
    },
    {
        "label": "dust3r.datasets.utils.cropping",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dust3r.datasets.utils.cropping",
        "description": "dust3r.datasets.utils.cropping",
        "detail": "dust3r.datasets.utils.cropping",
        "documentation": {}
    },
    {
        "label": "BatchedRandomSampler",
        "importPath": "dust3r.datasets.base.batched_sampler",
        "description": "dust3r.datasets.base.batched_sampler",
        "isExtraImport": true,
        "detail": "dust3r.datasets.base.batched_sampler",
        "documentation": {}
    },
    {
        "label": "BaseStereoViewDataset",
        "importPath": "dust3r.datasets.base.base_stereo_view_dataset",
        "description": "dust3r.datasets.base.base_stereo_view_dataset",
        "isExtraImport": true,
        "detail": "dust3r.datasets.base.base_stereo_view_dataset",
        "documentation": {}
    },
    {
        "label": "postprocess",
        "importPath": "dust3r.heads.postprocess",
        "description": "dust3r.heads.postprocess",
        "isExtraImport": true,
        "detail": "dust3r.heads.postprocess",
        "documentation": {}
    },
    {
        "label": "postprocess",
        "importPath": "dust3r.heads.postprocess",
        "description": "dust3r.heads.postprocess",
        "isExtraImport": true,
        "detail": "dust3r.heads.postprocess",
        "documentation": {}
    },
    {
        "label": "dust3r.utils.path_to_croco",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dust3r.utils.path_to_croco",
        "description": "dust3r.utils.path_to_croco",
        "detail": "dust3r.utils.path_to_croco",
        "documentation": {}
    },
    {
        "label": "DPTOutputAdapter",
        "importPath": "models.dpt_block",
        "description": "models.dpt_block",
        "isExtraImport": true,
        "detail": "models.dpt_block",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "cKDTree",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "invalid_to_zeros",
        "importPath": "dust3r.utils.misc",
        "description": "dust3r.utils.misc",
        "isExtraImport": true,
        "detail": "dust3r.utils.misc",
        "documentation": {}
    },
    {
        "label": "invalid_to_nans",
        "importPath": "dust3r.utils.misc",
        "description": "dust3r.utils.misc",
        "isExtraImport": true,
        "detail": "dust3r.utils.misc",
        "documentation": {}
    },
    {
        "label": "invalid_to_nans",
        "importPath": "dust3r.utils.misc",
        "description": "dust3r.utils.misc",
        "isExtraImport": true,
        "detail": "dust3r.utils.misc",
        "documentation": {}
    },
    {
        "label": "exif_transpose",
        "importPath": "PIL.ImageOps",
        "description": "PIL.ImageOps",
        "isExtraImport": true,
        "detail": "PIL.ImageOps",
        "documentation": {}
    },
    {
        "label": "spiral_cam_gen",
        "importPath": "dust3r.pcd_render",
        "description": "dust3r.pcd_render",
        "isExtraImport": true,
        "detail": "dust3r.pcd_render",
        "documentation": {}
    },
    {
        "label": "save_video_combined",
        "importPath": "dust3r.pcd_render",
        "description": "dust3r.pcd_render",
        "isExtraImport": true,
        "detail": "dust3r.pcd_render",
        "documentation": {}
    },
    {
        "label": "pcd_render",
        "importPath": "dust3r.pcd_render",
        "description": "dust3r.pcd_render",
        "isExtraImport": true,
        "detail": "dust3r.pcd_render",
        "documentation": {}
    },
    {
        "label": "pcd_render",
        "importPath": "dust3r.pcd_render",
        "description": "dust3r.pcd_render",
        "isExtraImport": true,
        "detail": "dust3r.pcd_render",
        "documentation": {}
    },
    {
        "label": "save_image_manifold",
        "importPath": "dust3r.pcd_render",
        "description": "dust3r.pcd_render",
        "isExtraImport": true,
        "detail": "dust3r.pcd_render",
        "documentation": {}
    },
    {
        "label": "save_video_combined",
        "importPath": "dust3r.pcd_render",
        "description": "dust3r.pcd_render",
        "isExtraImport": true,
        "detail": "dust3r.pcd_render",
        "documentation": {}
    },
    {
        "label": "pcd_render",
        "importPath": "dust3r.pcd_render",
        "description": "dust3r.pcd_render",
        "isExtraImport": true,
        "detail": "dust3r.pcd_render",
        "documentation": {}
    },
    {
        "label": "save_video_combined",
        "importPath": "dust3r.pcd_render",
        "description": "dust3r.pcd_render",
        "isExtraImport": true,
        "detail": "dust3r.pcd_render",
        "documentation": {}
    },
    {
        "label": "rasterization",
        "importPath": "gsplat.rendering",
        "description": "gsplat.rendering",
        "isExtraImport": true,
        "detail": "gsplat.rendering",
        "documentation": {}
    },
    {
        "label": "spherical_harmonics",
        "importPath": "gsplat.rendering",
        "description": "gsplat.rendering",
        "isExtraImport": true,
        "detail": "gsplat.rendering",
        "documentation": {}
    },
    {
        "label": "knn_points",
        "importPath": "pytorch3d.ops",
        "description": "pytorch3d.ops",
        "isExtraImport": true,
        "detail": "pytorch3d.ops",
        "documentation": {}
    },
    {
        "label": "get_pred_pts3d",
        "importPath": "dust3r.inference",
        "description": "dust3r.inference",
        "isExtraImport": true,
        "detail": "dust3r.inference",
        "documentation": {}
    },
    {
        "label": "find_opt_scaling",
        "importPath": "dust3r.inference",
        "description": "dust3r.inference",
        "isExtraImport": true,
        "detail": "dust3r.inference",
        "documentation": {}
    },
    {
        "label": "inference_mv",
        "importPath": "dust3r.inference",
        "description": "dust3r.inference",
        "isExtraImport": true,
        "detail": "dust3r.inference",
        "documentation": {}
    },
    {
        "label": "inference_mv",
        "importPath": "dust3r.inference",
        "description": "dust3r.inference",
        "isExtraImport": true,
        "detail": "dust3r.inference",
        "documentation": {}
    },
    {
        "label": "inference",
        "importPath": "dust3r.inference",
        "description": "dust3r.inference",
        "isExtraImport": true,
        "detail": "dust3r.inference",
        "documentation": {}
    },
    {
        "label": "inference_mv",
        "importPath": "dust3r.inference",
        "description": "dust3r.inference",
        "isExtraImport": true,
        "detail": "dust3r.inference",
        "documentation": {}
    },
    {
        "label": "inference",
        "importPath": "dust3r.inference",
        "description": "dust3r.inference",
        "isExtraImport": true,
        "detail": "dust3r.inference",
        "documentation": {}
    },
    {
        "label": "loss_of_one_batch",
        "importPath": "dust3r.inference",
        "description": "dust3r.inference",
        "isExtraImport": true,
        "detail": "dust3r.inference",
        "documentation": {}
    },
    {
        "label": "loss_of_one_batch",
        "importPath": "dust3r.inference",
        "description": "dust3r.inference",
        "isExtraImport": true,
        "detail": "dust3r.inference",
        "documentation": {}
    },
    {
        "label": "so3_relative_angle",
        "importPath": "pytorch3d.transforms",
        "description": "pytorch3d.transforms",
        "isExtraImport": true,
        "detail": "pytorch3d.transforms",
        "documentation": {}
    },
    {
        "label": "version",
        "importPath": "packaging",
        "description": "packaging",
        "isExtraImport": true,
        "detail": "packaging",
        "documentation": {}
    },
    {
        "label": "huggingface_hub",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "huggingface_hub",
        "description": "huggingface_hub",
        "detail": "huggingface_hub",
        "documentation": {}
    },
    {
        "label": "get_patch_embed",
        "importPath": "dust3r.patch_embed",
        "description": "dust3r.patch_embed",
        "isExtraImport": true,
        "detail": "dust3r.patch_embed",
        "documentation": {}
    },
    {
        "label": "swap",
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "isExtraImport": true,
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "swap_ref",
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "isExtraImport": true,
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "calibrate_camera_pnpransac",
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "isExtraImport": true,
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "estimate_focal_knowing_depth",
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "isExtraImport": true,
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "calibrate_camera_pnpransac",
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "isExtraImport": true,
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "estimate_focal_knowing_depth",
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "isExtraImport": true,
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "calibrate_camera_pnpransac",
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "isExtraImport": true,
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "estimate_focal_knowing_depth",
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "isExtraImport": true,
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "isExtraImport": true,
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "isExtraImport": true,
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "Pointclouds",
        "importPath": "pytorch3d.structures",
        "description": "pytorch3d.structures",
        "isExtraImport": true,
        "detail": "pytorch3d.structures",
        "documentation": {}
    },
    {
        "label": "Pointclouds",
        "importPath": "pytorch3d.structures",
        "description": "pytorch3d.structures",
        "isExtraImport": true,
        "detail": "pytorch3d.structures",
        "documentation": {}
    },
    {
        "label": "look_at_view_transform",
        "importPath": "pytorch3d.renderer",
        "description": "pytorch3d.renderer",
        "isExtraImport": true,
        "detail": "pytorch3d.renderer",
        "documentation": {}
    },
    {
        "label": "FoVOrthographicCameras",
        "importPath": "pytorch3d.renderer",
        "description": "pytorch3d.renderer",
        "isExtraImport": true,
        "detail": "pytorch3d.renderer",
        "documentation": {}
    },
    {
        "label": "PointsRasterizationSettings",
        "importPath": "pytorch3d.renderer",
        "description": "pytorch3d.renderer",
        "isExtraImport": true,
        "detail": "pytorch3d.renderer",
        "documentation": {}
    },
    {
        "label": "PointsRenderer",
        "importPath": "pytorch3d.renderer",
        "description": "pytorch3d.renderer",
        "isExtraImport": true,
        "detail": "pytorch3d.renderer",
        "documentation": {}
    },
    {
        "label": "PulsarPointsRenderer",
        "importPath": "pytorch3d.renderer",
        "description": "pytorch3d.renderer",
        "isExtraImport": true,
        "detail": "pytorch3d.renderer",
        "documentation": {}
    },
    {
        "label": "PointsRasterizer",
        "importPath": "pytorch3d.renderer",
        "description": "pytorch3d.renderer",
        "isExtraImport": true,
        "detail": "pytorch3d.renderer",
        "documentation": {}
    },
    {
        "label": "AlphaCompositor",
        "importPath": "pytorch3d.renderer",
        "description": "pytorch3d.renderer",
        "isExtraImport": true,
        "detail": "pytorch3d.renderer",
        "documentation": {}
    },
    {
        "label": "NormWeightedCompositor",
        "importPath": "pytorch3d.renderer",
        "description": "pytorch3d.renderer",
        "isExtraImport": true,
        "detail": "pytorch3d.renderer",
        "documentation": {}
    },
    {
        "label": "Rotation",
        "importPath": "scipy.spatial.transform",
        "description": "scipy.spatial.transform",
        "isExtraImport": true,
        "detail": "scipy.spatial.transform",
        "documentation": {}
    },
    {
        "label": "Rotation",
        "importPath": "scipy.spatial.transform",
        "description": "scipy.spatial.transform",
        "isExtraImport": true,
        "detail": "scipy.spatial.transform",
        "documentation": {}
    },
    {
        "label": "Rotation",
        "importPath": "scipy.spatial.transform",
        "description": "scipy.spatial.transform",
        "isExtraImport": true,
        "detail": "scipy.spatial.transform",
        "documentation": {}
    },
    {
        "label": "Rotation",
        "importPath": "scipy.spatial.transform",
        "description": "scipy.spatial.transform",
        "isExtraImport": true,
        "detail": "scipy.spatial.transform",
        "documentation": {}
    },
    {
        "label": "Rotation",
        "importPath": "scipy.spatial.transform",
        "description": "scipy.spatial.transform",
        "isExtraImport": true,
        "detail": "scipy.spatial.transform",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "dust3r.dummy_io",
        "description": "dust3r.dummy_io",
        "isExtraImport": true,
        "detail": "dust3r.dummy_io",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "dust3r.dummy_io",
        "description": "dust3r.dummy_io",
        "isExtraImport": true,
        "detail": "dust3r.dummy_io",
        "documentation": {}
    },
    {
        "label": "trimesh",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "trimesh",
        "description": "trimesh",
        "detail": "trimesh",
        "documentation": {}
    },
    {
        "label": "open3d",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "open3d",
        "description": "open3d",
        "detail": "open3d",
        "documentation": {}
    },
    {
        "label": "AsymmetricCroCo3DStereoMultiView",
        "importPath": "dust3r.model",
        "description": "dust3r.model",
        "isExtraImport": true,
        "detail": "dust3r.model",
        "documentation": {}
    },
    {
        "label": "AsymmetricCroCo3DStereoMultiView",
        "importPath": "dust3r.model",
        "description": "dust3r.model",
        "isExtraImport": true,
        "detail": "dust3r.model",
        "documentation": {}
    },
    {
        "label": "AsymmetricCroCo3DStereoMultiView",
        "importPath": "dust3r.model",
        "description": "dust3r.model",
        "isExtraImport": true,
        "detail": "dust3r.model",
        "documentation": {}
    },
    {
        "label": "AsymmetricCroCo3DStereo",
        "importPath": "dust3r.model",
        "description": "dust3r.model",
        "isExtraImport": true,
        "detail": "dust3r.model",
        "documentation": {}
    },
    {
        "label": "AsymmetricCroCo3DStereo",
        "importPath": "dust3r.model",
        "description": "dust3r.model",
        "isExtraImport": true,
        "detail": "dust3r.model",
        "documentation": {}
    },
    {
        "label": "AsymmetricCroCo3DStereoMultiView",
        "importPath": "dust3r.model",
        "description": "dust3r.model",
        "isExtraImport": true,
        "detail": "dust3r.model",
        "documentation": {}
    },
    {
        "label": "inf",
        "importPath": "dust3r.model",
        "description": "dust3r.model",
        "isExtraImport": true,
        "detail": "dust3r.model",
        "documentation": {}
    },
    {
        "label": "AsymmetricCroCo3DStereo",
        "importPath": "dust3r.model",
        "description": "dust3r.model",
        "isExtraImport": true,
        "detail": "dust3r.model",
        "documentation": {}
    },
    {
        "label": "AsymmetricCroCo3DStereoMultiView",
        "importPath": "dust3r.model",
        "description": "dust3r.model",
        "isExtraImport": true,
        "detail": "dust3r.model",
        "documentation": {}
    },
    {
        "label": "inf",
        "importPath": "dust3r.model",
        "description": "dust3r.model",
        "isExtraImport": true,
        "detail": "dust3r.model",
        "documentation": {}
    },
    {
        "label": "root_file_io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "string",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "string",
        "description": "string",
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "gradio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gradio",
        "description": "gradio",
        "detail": "gradio",
        "documentation": {}
    },
    {
        "label": "make_pairs",
        "importPath": "dust3r.image_pairs",
        "description": "dust3r.image_pairs",
        "isExtraImport": true,
        "detail": "dust3r.image_pairs",
        "documentation": {}
    },
    {
        "label": "global_aligner",
        "importPath": "dust3r.cloud_opt",
        "description": "dust3r.cloud_opt",
        "isExtraImport": true,
        "detail": "dust3r.cloud_opt",
        "documentation": {}
    },
    {
        "label": "GlobalAlignerMode",
        "importPath": "dust3r.cloud_opt",
        "description": "dust3r.cloud_opt",
        "isExtraImport": true,
        "detail": "dust3r.cloud_opt",
        "documentation": {}
    },
    {
        "label": "get_data_loader",
        "importPath": "dust3r.datasets",
        "description": "dust3r.datasets",
        "isExtraImport": true,
        "detail": "dust3r.datasets",
        "documentation": {}
    },
    {
        "label": "get_data_loader",
        "importPath": "dust3r.datasets",
        "description": "dust3r.datasets",
        "isExtraImport": true,
        "detail": "dust3r.datasets",
        "documentation": {}
    },
    {
        "label": "loss_of_one_batch_go_mv",
        "importPath": "inference_global_optimization",
        "description": "inference_global_optimization",
        "isExtraImport": true,
        "detail": "inference_global_optimization",
        "documentation": {}
    },
    {
        "label": "gs_render",
        "importPath": "dust3r.gs",
        "description": "dust3r.gs",
        "isExtraImport": true,
        "detail": "dust3r.gs",
        "documentation": {}
    },
    {
        "label": "gs_render",
        "importPath": "dust3r.gs",
        "description": "dust3r.gs",
        "isExtraImport": true,
        "detail": "dust3r.gs",
        "documentation": {}
    },
    {
        "label": "croco.utils.misc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "croco.utils.misc",
        "description": "croco.utils.misc",
        "detail": "croco.utils.misc",
        "documentation": {}
    },
    {
        "label": "NativeScalerWithGradNormCount",
        "importPath": "croco.utils.misc",
        "description": "croco.utils.misc",
        "isExtraImport": true,
        "detail": "croco.utils.misc",
        "documentation": {}
    },
    {
        "label": "NativeScalerWithGradNormCount",
        "importPath": "croco.utils.misc",
        "description": "croco.utils.misc",
        "isExtraImport": true,
        "detail": "croco.utils.misc",
        "documentation": {}
    },
    {
        "label": "inspect",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "inspect",
        "description": "inspect",
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "csv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv",
        "description": "csv",
        "detail": "csv",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "copy_tree",
        "importPath": "distutils.dir_util",
        "description": "distutils.dir_util",
        "isExtraImport": true,
        "detail": "distutils.dir_util",
        "documentation": {}
    },
    {
        "label": "arg_parser",
        "kind": 2,
        "importPath": "croco.datasets.crops.extract_crops_from_images",
        "description": "croco.datasets.crops.extract_crops_from_images",
        "peekOfCode": "def arg_parser():\n    parser = argparse.ArgumentParser('Generate cropped image pairs from image crop list')\n    parser.add_argument('--crops', type=str, required=True, help='crop file')\n    parser.add_argument('--root-dir', type=str, required=True, help='root directory')\n    parser.add_argument('--output-dir', type=str, required=True, help='output directory')\n    parser.add_argument('--imsize', type=int, default=256, help='size of the crops')\n    parser.add_argument('--nthread', type=int, required=True, help='number of simultaneous threads')\n    parser.add_argument('--max-subdir-levels', type=int, default=5, help='maximum number of subdirectories')\n    parser.add_argument('--ideal-number-pairs-in-dir', type=int, default=500, help='number of pairs stored in a dir')\n    return parser",
        "detail": "croco.datasets.crops.extract_crops_from_images",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "croco.datasets.crops.extract_crops_from_images",
        "description": "croco.datasets.crops.extract_crops_from_images",
        "peekOfCode": "def main(args):\n    listing_path = os.path.join(args.output_dir, 'listing.txt')\n    print(f'Loading list of crops ... ({args.nthread} threads)')\n    crops, num_crops_to_generate = load_crop_file(args.crops)\n    print(f'Preparing jobs ({len(crops)} candidate image pairs)...')\n    num_levels = min(math.ceil(math.log(num_crops_to_generate, args.ideal_number_pairs_in_dir)), args.max_subdir_levels)\n    num_pairs_in_dir = math.ceil(num_crops_to_generate ** (1/num_levels))\n    jobs = prepare_jobs(crops, num_levels, num_pairs_in_dir)\n    del crops\n    os.makedirs(args.output_dir, exist_ok=True)",
        "detail": "croco.datasets.crops.extract_crops_from_images",
        "documentation": {}
    },
    {
        "label": "load_crop_file",
        "kind": 2,
        "importPath": "croco.datasets.crops.extract_crops_from_images",
        "description": "croco.datasets.crops.extract_crops_from_images",
        "peekOfCode": "def load_crop_file(path):\n    data = open(path).read().splitlines()\n    pairs = []\n    num_crops_to_generate = 0\n    for line in tqdm(data):\n        if line.startswith('#'):\n            continue\n        line = line.split(', ')\n        if len(line) < 8:\n            img1, img2, rotation = line",
        "detail": "croco.datasets.crops.extract_crops_from_images",
        "documentation": {}
    },
    {
        "label": "prepare_jobs",
        "kind": 2,
        "importPath": "croco.datasets.crops.extract_crops_from_images",
        "description": "croco.datasets.crops.extract_crops_from_images",
        "peekOfCode": "def prepare_jobs(pairs, num_levels, num_pairs_in_dir):\n    jobs = []\n    powers = [num_pairs_in_dir**level for level in reversed(range(num_levels))]\n    def get_path(idx):\n        idx_array = []\n        d = idx\n        for level in range(num_levels - 1):\n            idx_array.append(idx // powers[level])\n            idx = idx % powers[level]\n        idx_array.append(d)",
        "detail": "croco.datasets.crops.extract_crops_from_images",
        "documentation": {}
    },
    {
        "label": "load_image",
        "kind": 2,
        "importPath": "croco.datasets.crops.extract_crops_from_images",
        "description": "croco.datasets.crops.extract_crops_from_images",
        "peekOfCode": "def load_image(path):\n    try:\n        return Image.open(path).convert('RGB')\n    except Exception as e:\n        print('skipping', path, e)\n        raise OSError()\ndef save_image_crops(args, data):\n    # load images\n    img_pair, rot, crops, paths = data\n    try:",
        "detail": "croco.datasets.crops.extract_crops_from_images",
        "documentation": {}
    },
    {
        "label": "save_image_crops",
        "kind": 2,
        "importPath": "croco.datasets.crops.extract_crops_from_images",
        "description": "croco.datasets.crops.extract_crops_from_images",
        "peekOfCode": "def save_image_crops(args, data):\n    # load images\n    img_pair, rot, crops, paths = data\n    try:\n        img1, img2 = [load_image(os.path.join(args.root_dir, impath)) for impath in img_pair]\n    except OSError as e:\n        return []\n    def area(sz):\n        return sz[0] * sz[1]\n    tgt_size = (args.imsize, args.imsize)",
        "detail": "croco.datasets.crops.extract_crops_from_images",
        "documentation": {}
    },
    {
        "label": "generate_multiview_images_from_metadata",
        "kind": 2,
        "importPath": "croco.datasets.habitat_sim.generate_from_metadata",
        "description": "croco.datasets.habitat_sim.generate_from_metadata",
        "peekOfCode": "def generate_multiview_images_from_metadata(metadata_filename,\n                                            output_dir,\n                                            overload_params = dict(),\n                                            scene_datasets_paths=None,\n                                            exist_ok=False):   \n    \"\"\"\n    Generate images from a metadata file for reproducibility purposes.\n    \"\"\"\n    # Reorder paths by decreasing label length, to avoid collisions when testing if a string by such label\n    if scene_datasets_paths is not None:",
        "detail": "croco.datasets.habitat_sim.generate_from_metadata",
        "documentation": {}
    },
    {
        "label": "generate_multiview_images_for_scene",
        "kind": 2,
        "importPath": "croco.datasets.habitat_sim.generate_multiview_images",
        "description": "croco.datasets.habitat_sim.generate_multiview_images",
        "peekOfCode": "def generate_multiview_images_for_scene(scene_dataset_config_file,\n                                        scene,\n                                        navmesh,\n                                        output_dir, \n                                        views_count,\n                                        size, \n                                        exist_ok=False, \n                                        generate_depth=False,\n                                        **kwargs):\n    \"\"\"",
        "detail": "croco.datasets.habitat_sim.generate_multiview_images",
        "documentation": {}
    },
    {
        "label": "create_commandline",
        "kind": 2,
        "importPath": "croco.datasets.habitat_sim.generate_multiview_images",
        "description": "croco.datasets.habitat_sim.generate_multiview_images",
        "peekOfCode": "def create_commandline(scene_data, generate_depth, exist_ok=False):\n    \"\"\"\n    Create a commandline string to generate a scene.\n    \"\"\"\n    def my_formatting(val):\n        if val is None or val == \"\":\n            return '\"\"'\n        else:\n            return val\n    commandline = f\"\"\"python {__file__} --scene {my_formatting(scene_data.scene)} ",
        "detail": "croco.datasets.habitat_sim.generate_multiview_images",
        "documentation": {}
    },
    {
        "label": "NoNaviguableSpaceError",
        "kind": 6,
        "importPath": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "description": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "peekOfCode": "class NoNaviguableSpaceError(RuntimeError):\n    def __init__(self, *args):\n            super().__init__(*args)\nclass MultiviewHabitatSimGenerator:\n    def __init__(self,\n                scene,\n                navmesh,\n                scene_dataset_config_file,\n                resolution = (240, 320),\n                views_count=2,",
        "detail": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "documentation": {}
    },
    {
        "label": "MultiviewHabitatSimGenerator",
        "kind": 6,
        "importPath": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "description": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "peekOfCode": "class MultiviewHabitatSimGenerator:\n    def __init__(self,\n                scene,\n                navmesh,\n                scene_dataset_config_file,\n                resolution = (240, 320),\n                views_count=2,\n                hfov = 60,\n                gpu_id = 0,\n                size = 10000,",
        "detail": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "documentation": {}
    },
    {
        "label": "compute_camera_intrinsics",
        "kind": 2,
        "importPath": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "description": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "peekOfCode": "def compute_camera_intrinsics(height, width, hfov):\n    f = width/2 / np.tan(hfov/2 * np.pi/180)\n    cu, cv = width/2, height/2\n    return f, cu, cv\ndef compute_camera_pose_opencv_convention(camera_position, camera_orientation):\n    R_cam2world = quaternion.as_rotation_matrix(camera_orientation) @ R_OPENCV2HABITAT\n    t_cam2world = np.asarray(camera_position)\n    return R_cam2world, t_cam2world\ndef compute_pointmap(depthmap, hfov):\n    \"\"\" Compute a HxWx3 pointmap in camera frame from a HxW depth map.\"\"\"",
        "detail": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "documentation": {}
    },
    {
        "label": "compute_camera_pose_opencv_convention",
        "kind": 2,
        "importPath": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "description": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "peekOfCode": "def compute_camera_pose_opencv_convention(camera_position, camera_orientation):\n    R_cam2world = quaternion.as_rotation_matrix(camera_orientation) @ R_OPENCV2HABITAT\n    t_cam2world = np.asarray(camera_position)\n    return R_cam2world, t_cam2world\ndef compute_pointmap(depthmap, hfov):\n    \"\"\" Compute a HxWx3 pointmap in camera frame from a HxW depth map.\"\"\"\n    height, width = depthmap.shape\n    f, cu, cv = compute_camera_intrinsics(height, width, hfov)\n    # Cast depth map to point\n    z_cam = depthmap",
        "detail": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "documentation": {}
    },
    {
        "label": "compute_pointmap",
        "kind": 2,
        "importPath": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "description": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "peekOfCode": "def compute_pointmap(depthmap, hfov):\n    \"\"\" Compute a HxWx3 pointmap in camera frame from a HxW depth map.\"\"\"\n    height, width = depthmap.shape\n    f, cu, cv = compute_camera_intrinsics(height, width, hfov)\n    # Cast depth map to point\n    z_cam = depthmap\n    u, v = np.meshgrid(range(width), range(height))\n    x_cam = (u - cu) / f * z_cam\n    y_cam = (v - cv) / f * z_cam\n    X_cam = np.stack((x_cam, y_cam, z_cam), axis=-1)",
        "detail": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "documentation": {}
    },
    {
        "label": "compute_pointcloud",
        "kind": 2,
        "importPath": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "description": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "peekOfCode": "def compute_pointcloud(depthmap, hfov, camera_position, camera_rotation):\n    \"\"\"Return a 3D point cloud corresponding to valid pixels of the depth map\"\"\"\n    R_cam2world, t_cam2world = compute_camera_pose_opencv_convention(camera_position, camera_rotation)\n    X_cam = compute_pointmap(depthmap=depthmap, hfov=hfov)\n    valid_mask = (X_cam[:,:,2] != 0.0)\n    X_cam = X_cam.reshape(-1, 3)[valid_mask.flatten()]\n    X_world = X_cam @ R_cam2world.T + t_cam2world.reshape(1, 3)\n    return X_world\ndef compute_pointcloud_overlaps_scikit(pointcloud1, pointcloud2, distance_threshold, compute_symmetric=False):\n    \"\"\"",
        "detail": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "documentation": {}
    },
    {
        "label": "compute_pointcloud_overlaps_scikit",
        "kind": 2,
        "importPath": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "description": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "peekOfCode": "def compute_pointcloud_overlaps_scikit(pointcloud1, pointcloud2, distance_threshold, compute_symmetric=False):\n    \"\"\"\n    Compute 'overlapping' metrics based on a distance threshold between two point clouds.\n    \"\"\"\n    nbrs = NearestNeighbors(n_neighbors=1, algorithm = 'kd_tree').fit(pointcloud2)\n    distances, indices = nbrs.kneighbors(pointcloud1)\n    intersection1 = np.count_nonzero(distances.flatten() < distance_threshold)\n    data = {\"intersection1\": intersection1,\n            \"size1\": len(pointcloud1)}\n    if compute_symmetric:",
        "detail": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "documentation": {}
    },
    {
        "label": "look_at",
        "kind": 2,
        "importPath": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "description": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "peekOfCode": "def look_at(eye, center, up, return_cam2world=True):\n    \"\"\"\n    Return camera pose looking at a given center point.\n    Analogous of gluLookAt function, using OpenCV camera convention.\n    \"\"\"\n    z = center - eye\n    z /= np.linalg.norm(z, axis=-1, keepdims=True)\n    y = -up\n    y = y - np.sum(y * z, axis=-1, keepdims=True) * z\n    y /= np.linalg.norm(y, axis=-1, keepdims=True)",
        "detail": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "documentation": {}
    },
    {
        "label": "look_at_for_habitat",
        "kind": 2,
        "importPath": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "description": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "peekOfCode": "def look_at_for_habitat(eye, center, up, return_cam2world=True):\n    R, t = look_at(eye, center, up)\n    orientation = quaternion.from_rotation_matrix(R @ R_OPENCV2HABITAT.T)\n    return orientation, t\ndef generate_orientation_noise(pan_range, tilt_range, roll_range):\n    return (quaternion.from_rotation_vector(np.random.uniform(*pan_range) * DEG2RAD * habitat_sim.geo.UP)\n            * quaternion.from_rotation_vector(np.random.uniform(*tilt_range) * DEG2RAD * habitat_sim.geo.RIGHT)\n            * quaternion.from_rotation_vector(np.random.uniform(*roll_range) * DEG2RAD * habitat_sim.geo.FRONT))\nclass NoNaviguableSpaceError(RuntimeError):\n    def __init__(self, *args):",
        "detail": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "documentation": {}
    },
    {
        "label": "generate_orientation_noise",
        "kind": 2,
        "importPath": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "description": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "peekOfCode": "def generate_orientation_noise(pan_range, tilt_range, roll_range):\n    return (quaternion.from_rotation_vector(np.random.uniform(*pan_range) * DEG2RAD * habitat_sim.geo.UP)\n            * quaternion.from_rotation_vector(np.random.uniform(*tilt_range) * DEG2RAD * habitat_sim.geo.RIGHT)\n            * quaternion.from_rotation_vector(np.random.uniform(*roll_range) * DEG2RAD * habitat_sim.geo.FRONT))\nclass NoNaviguableSpaceError(RuntimeError):\n    def __init__(self, *args):\n            super().__init__(*args)\nclass MultiviewHabitatSimGenerator:\n    def __init__(self,\n                scene,",
        "detail": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "documentation": {}
    },
    {
        "label": "R_OPENCV2HABITAT",
        "kind": 5,
        "importPath": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "description": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "peekOfCode": "R_OPENCV2HABITAT = np.stack((habitat_sim.geo.RIGHT, -habitat_sim.geo.UP, habitat_sim.geo.FRONT), axis=0)\nR_HABITAT2OPENCV = R_OPENCV2HABITAT.T\nDEG2RAD = np.pi / 180\ndef compute_camera_intrinsics(height, width, hfov):\n    f = width/2 / np.tan(hfov/2 * np.pi/180)\n    cu, cv = width/2, height/2\n    return f, cu, cv\ndef compute_camera_pose_opencv_convention(camera_position, camera_orientation):\n    R_cam2world = quaternion.as_rotation_matrix(camera_orientation) @ R_OPENCV2HABITAT\n    t_cam2world = np.asarray(camera_position)",
        "detail": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "documentation": {}
    },
    {
        "label": "R_HABITAT2OPENCV",
        "kind": 5,
        "importPath": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "description": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "peekOfCode": "R_HABITAT2OPENCV = R_OPENCV2HABITAT.T\nDEG2RAD = np.pi / 180\ndef compute_camera_intrinsics(height, width, hfov):\n    f = width/2 / np.tan(hfov/2 * np.pi/180)\n    cu, cv = width/2, height/2\n    return f, cu, cv\ndef compute_camera_pose_opencv_convention(camera_position, camera_orientation):\n    R_cam2world = quaternion.as_rotation_matrix(camera_orientation) @ R_OPENCV2HABITAT\n    t_cam2world = np.asarray(camera_position)\n    return R_cam2world, t_cam2world",
        "detail": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "documentation": {}
    },
    {
        "label": "DEG2RAD",
        "kind": 5,
        "importPath": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "description": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "peekOfCode": "DEG2RAD = np.pi / 180\ndef compute_camera_intrinsics(height, width, hfov):\n    f = width/2 / np.tan(hfov/2 * np.pi/180)\n    cu, cv = width/2, height/2\n    return f, cu, cv\ndef compute_camera_pose_opencv_convention(camera_position, camera_orientation):\n    R_cam2world = quaternion.as_rotation_matrix(camera_orientation) @ R_OPENCV2HABITAT\n    t_cam2world = np.asarray(camera_position)\n    return R_cam2world, t_cam2world\ndef compute_pointmap(depthmap, hfov):",
        "detail": "croco.datasets.habitat_sim.multiview_habitat_sim_generator",
        "documentation": {}
    },
    {
        "label": "list_replicacad_scenes",
        "kind": 2,
        "importPath": "croco.datasets.habitat_sim.paths",
        "description": "croco.datasets.habitat_sim.paths",
        "peekOfCode": "def list_replicacad_scenes(base_output_dir, base_path=SCENES_DATASET[\"replica_cad\"]):\n    scene_dataset_config_file = os.path.join(base_path, \"replicaCAD.scene_dataset_config.json\")\n    scenes = [f\"apt_{i}\" for i in range(6)] + [\"empty_stage\"]\n    navmeshes = [f\"navmeshes/apt_{i}_static_furniture.navmesh\" for i in range(6)] + [\"empty_stage.navmesh\"]\n    scenes_data = []\n    for idx in range(len(scenes)):\n        output_dir = os.path.join(base_output_dir, \"ReplicaCAD\", scenes[idx])\n        # Add scene\n        data = SceneData(scene_dataset_config_file=scene_dataset_config_file,\n                    scene = scenes[idx] + \".scene_instance.json\",",
        "detail": "croco.datasets.habitat_sim.paths",
        "documentation": {}
    },
    {
        "label": "list_replica_cad_baked_lighting_scenes",
        "kind": 2,
        "importPath": "croco.datasets.habitat_sim.paths",
        "description": "croco.datasets.habitat_sim.paths",
        "peekOfCode": "def list_replica_cad_baked_lighting_scenes(base_output_dir, base_path=SCENES_DATASET[\"replica_cad_baked_lighting\"]):\n    scene_dataset_config_file = os.path.join(base_path, \"replicaCAD_baked.scene_dataset_config.json\")\n    scenes = sum([[f\"Baked_sc{i}_staging_{j:02}\" for i in range(5)] for j in range(21)], [])\n    navmeshes = \"\"#[f\"navmeshes/apt_{i}_static_furniture.navmesh\" for i in range(6)] + [\"empty_stage.navmesh\"]\n    scenes_data = []\n    for idx in range(len(scenes)):\n        output_dir = os.path.join(base_output_dir, \"replica_cad_baked_lighting\", scenes[idx])\n        data = SceneData(scene_dataset_config_file=scene_dataset_config_file,\n                    scene = scenes[idx],\n                    navmesh = \"\",",
        "detail": "croco.datasets.habitat_sim.paths",
        "documentation": {}
    },
    {
        "label": "list_replica_scenes",
        "kind": 2,
        "importPath": "croco.datasets.habitat_sim.paths",
        "description": "croco.datasets.habitat_sim.paths",
        "peekOfCode": "def list_replica_scenes(base_output_dir, base_path):\n    scenes_data = []\n    for scene_id in os.listdir(base_path):\n        scene = os.path.join(base_path, scene_id, \"mesh.ply\")\n        navmesh = os.path.join(base_path, scene_id, \"habitat/mesh_preseg_semantic.navmesh\") # Not sure if I should use it\n        scene_dataset_config_file = \"\"\n        output_dir = os.path.join(base_output_dir, scene_id)\n        # Add scene only if it does not exist already, or if exist_ok\n        data = SceneData(scene_dataset_config_file = scene_dataset_config_file,\n                    scene = scene,",
        "detail": "croco.datasets.habitat_sim.paths",
        "documentation": {}
    },
    {
        "label": "list_scenes",
        "kind": 2,
        "importPath": "croco.datasets.habitat_sim.paths",
        "description": "croco.datasets.habitat_sim.paths",
        "peekOfCode": "def list_scenes(base_output_dir, base_path):\n    \"\"\"\n    Generic method iterating through a base_path folder to find scenes.\n    \"\"\"\n    scenes_data = []\n    for root, dirs, files in os.walk(base_path, followlinks=True):\n        folder_scenes_data = []\n        for file in files:\n            name, ext = os.path.splitext(file)\n            if ext == \".glb\":",
        "detail": "croco.datasets.habitat_sim.paths",
        "documentation": {}
    },
    {
        "label": "list_scenes_available",
        "kind": 2,
        "importPath": "croco.datasets.habitat_sim.paths",
        "description": "croco.datasets.habitat_sim.paths",
        "peekOfCode": "def list_scenes_available(base_output_dir, scenes_dataset_paths=SCENES_DATASET):\n    scenes_data = []\n    # HM3D\n    for split in (\"minival\", \"train\", \"val\", \"examples\"):\n        scenes_data += list_scenes(base_output_dir=os.path.join(base_output_dir, f\"hm3d/{split}/\"),\n                                    base_path=f\"{scenes_dataset_paths['hm3d']}/{split}\")\n    # Gibson\n    scenes_data += list_scenes(base_output_dir=os.path.join(base_output_dir, \"gibson\"),\n                                base_path=scenes_dataset_paths[\"gibson\"])\n    # Habitat test scenes (just a few)",
        "detail": "croco.datasets.habitat_sim.paths",
        "documentation": {}
    },
    {
        "label": "SCENES_DATASET",
        "kind": 5,
        "importPath": "croco.datasets.habitat_sim.paths",
        "description": "croco.datasets.habitat_sim.paths",
        "peekOfCode": "SCENES_DATASET = {\n    \"hm3d\": \"./data/habitat-sim-data/scene_datasets/hm3d/\",\n    \"gibson\": \"./data/habitat-sim-data/scene_datasets/gibson/\",\n    \"habitat-test-scenes\": \"./data/habitat-sim/scene_datasets/habitat-test-scenes/\",\n    \"replica_cad_baked_lighting\": \"./data/habitat-sim/scene_datasets/replica_cad_baked_lighting/\",\n    \"replica_cad\": \"./data/habitat-sim/scene_datasets/replica_cad/\",\n    \"replica\": \"./data/habitat-sim/scene_datasets/ReplicaDataset/\",\n    \"scannet\": \"./data/habitat-sim/scene_datasets/scannet/\"\n}\nSceneData = collections.namedtuple(\"SceneData\", [\"scene_dataset_config_file\", \"scene\", \"navmesh\", \"output_dir\"])",
        "detail": "croco.datasets.habitat_sim.paths",
        "documentation": {}
    },
    {
        "label": "SceneData",
        "kind": 5,
        "importPath": "croco.datasets.habitat_sim.paths",
        "description": "croco.datasets.habitat_sim.paths",
        "peekOfCode": "SceneData = collections.namedtuple(\"SceneData\", [\"scene_dataset_config_file\", \"scene\", \"navmesh\", \"output_dir\"])\ndef list_replicacad_scenes(base_output_dir, base_path=SCENES_DATASET[\"replica_cad\"]):\n    scene_dataset_config_file = os.path.join(base_path, \"replicaCAD.scene_dataset_config.json\")\n    scenes = [f\"apt_{i}\" for i in range(6)] + [\"empty_stage\"]\n    navmeshes = [f\"navmeshes/apt_{i}_static_furniture.navmesh\" for i in range(6)] + [\"empty_stage.navmesh\"]\n    scenes_data = []\n    for idx in range(len(scenes)):\n        output_dir = os.path.join(base_output_dir, \"ReplicaCAD\", scenes[idx])\n        # Add scene\n        data = SceneData(scene_dataset_config_file=scene_dataset_config_file,",
        "detail": "croco.datasets.habitat_sim.paths",
        "documentation": {}
    },
    {
        "label": "PairsDataset",
        "kind": 6,
        "importPath": "croco.datasets.pairs_dataset",
        "description": "croco.datasets.pairs_dataset",
        "peekOfCode": "class PairsDataset(Dataset):\n    def __init__(self, dnames, trfs='', totensor=True, normalize=True, data_dir='./data/'):\n        super().__init__()\n        self.image_pairs = dnames_to_image_pairs(dnames, data_dir=data_dir)\n        self.transforms = get_pair_transforms(transform_str=trfs, totensor=totensor, normalize=normalize)\n    def __len__(self):\n        return len(self.image_pairs)\n    def __getitem__(self, index):\n        im1path, im2path = self.image_pairs[index]\n        im1 = load_image(im1path)",
        "detail": "croco.datasets.pairs_dataset",
        "documentation": {}
    },
    {
        "label": "load_image",
        "kind": 2,
        "importPath": "croco.datasets.pairs_dataset",
        "description": "croco.datasets.pairs_dataset",
        "peekOfCode": "def load_image(impath):\n    return Image.open(impath)\ndef load_pairs_from_cache_file(fname, root=''):\n    assert os.path.isfile(fname), \"cannot parse pairs from {:s}, file does not exist\".format(fname)\n    with open(fname, 'r') as fid:\n        lines = fid.read().strip().splitlines()\n    pairs = [ (os.path.join(root,l.split()[0]), os.path.join(root,l.split()[1])) for l in lines]\n    return pairs\ndef load_pairs_from_list_file(fname, root=''):\n    assert os.path.isfile(fname), \"cannot parse pairs from {:s}, file does not exist\".format(fname)",
        "detail": "croco.datasets.pairs_dataset",
        "documentation": {}
    },
    {
        "label": "load_pairs_from_cache_file",
        "kind": 2,
        "importPath": "croco.datasets.pairs_dataset",
        "description": "croco.datasets.pairs_dataset",
        "peekOfCode": "def load_pairs_from_cache_file(fname, root=''):\n    assert os.path.isfile(fname), \"cannot parse pairs from {:s}, file does not exist\".format(fname)\n    with open(fname, 'r') as fid:\n        lines = fid.read().strip().splitlines()\n    pairs = [ (os.path.join(root,l.split()[0]), os.path.join(root,l.split()[1])) for l in lines]\n    return pairs\ndef load_pairs_from_list_file(fname, root=''):\n    assert os.path.isfile(fname), \"cannot parse pairs from {:s}, file does not exist\".format(fname)\n    with open(fname, 'r') as fid:\n        lines = fid.read().strip().splitlines()",
        "detail": "croco.datasets.pairs_dataset",
        "documentation": {}
    },
    {
        "label": "load_pairs_from_list_file",
        "kind": 2,
        "importPath": "croco.datasets.pairs_dataset",
        "description": "croco.datasets.pairs_dataset",
        "peekOfCode": "def load_pairs_from_list_file(fname, root=''):\n    assert os.path.isfile(fname), \"cannot parse pairs from {:s}, file does not exist\".format(fname)\n    with open(fname, 'r') as fid:\n        lines = fid.read().strip().splitlines()\n    pairs = [ (os.path.join(root,l+'_1.jpg'), os.path.join(root,l+'_2.jpg')) for l in lines if not l.startswith('#')]\n    return pairs\ndef write_cache_file(fname, pairs, root=''):\n    if len(root)>0:\n        if not root.endswith('/'): root+='/'\n        assert os.path.isdir(root)",
        "detail": "croco.datasets.pairs_dataset",
        "documentation": {}
    },
    {
        "label": "write_cache_file",
        "kind": 2,
        "importPath": "croco.datasets.pairs_dataset",
        "description": "croco.datasets.pairs_dataset",
        "peekOfCode": "def write_cache_file(fname, pairs, root=''):\n    if len(root)>0:\n        if not root.endswith('/'): root+='/'\n        assert os.path.isdir(root)\n    s = ''\n    for im1, im2 in pairs:\n        if len(root)>0:\n            assert im1.startswith(root), im1\n            assert im2.startswith(root), im2\n        s += '{:s} {:s}\\n'.format(im1[len(root):], im2[len(root):])",
        "detail": "croco.datasets.pairs_dataset",
        "documentation": {}
    },
    {
        "label": "parse_and_cache_all_pairs",
        "kind": 2,
        "importPath": "croco.datasets.pairs_dataset",
        "description": "croco.datasets.pairs_dataset",
        "peekOfCode": "def parse_and_cache_all_pairs(dname, data_dir='./data/'):\n    if dname=='habitat_release':\n        dirname = os.path.join(data_dir, 'habitat_release')\n        assert os.path.isdir(dirname), \"cannot find folder for habitat_release pairs: \"+dirname\n        cache_file = os.path.join(dirname, 'pairs.txt')\n        assert not os.path.isfile(cache_file), \"cache file already exists: \"+cache_file\n        print('Parsing pairs for dataset: '+dname)\n        pairs = []\n        for root, dirs, files in os.walk(dirname):\n            if 'val' in root: continue",
        "detail": "croco.datasets.pairs_dataset",
        "documentation": {}
    },
    {
        "label": "dnames_to_image_pairs",
        "kind": 2,
        "importPath": "croco.datasets.pairs_dataset",
        "description": "croco.datasets.pairs_dataset",
        "peekOfCode": "def dnames_to_image_pairs(dnames, data_dir='./data/'):\n    \"\"\"\n    dnames: list of datasets with image pairs, separated by +\n    \"\"\"\n    all_pairs = []\n    for dname in dnames.split('+'):\n        if dname=='habitat_release':\n            dirname = os.path.join(data_dir, 'habitat_release')\n            assert os.path.isdir(dirname), \"cannot find folder for habitat_release pairs: \"+dirname\n            cache_file = os.path.join(dirname, 'pairs.txt')",
        "detail": "croco.datasets.pairs_dataset",
        "documentation": {}
    },
    {
        "label": "ComposePair",
        "kind": 6,
        "importPath": "croco.datasets.transforms",
        "description": "croco.datasets.transforms",
        "peekOfCode": "class ComposePair(torchvision.transforms.Compose):\n    def __call__(self, img1, img2):\n        for t in self.transforms:\n            img1, img2 = t(img1, img2)\n        return img1, img2\nclass NormalizeBoth(torchvision.transforms.Normalize):\n    def forward(self, img1, img2):\n        img1 = super().forward(img1)\n        img2 = super().forward(img2)\n        return img1, img2",
        "detail": "croco.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "NormalizeBoth",
        "kind": 6,
        "importPath": "croco.datasets.transforms",
        "description": "croco.datasets.transforms",
        "peekOfCode": "class NormalizeBoth(torchvision.transforms.Normalize):\n    def forward(self, img1, img2):\n        img1 = super().forward(img1)\n        img2 = super().forward(img2)\n        return img1, img2\nclass ToTensorBoth(torchvision.transforms.ToTensor):\n    def __call__(self, img1, img2):\n        img1 = super().__call__(img1)\n        img2 = super().__call__(img2)\n        return img1, img2",
        "detail": "croco.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "ToTensorBoth",
        "kind": 6,
        "importPath": "croco.datasets.transforms",
        "description": "croco.datasets.transforms",
        "peekOfCode": "class ToTensorBoth(torchvision.transforms.ToTensor):\n    def __call__(self, img1, img2):\n        img1 = super().__call__(img1)\n        img2 = super().__call__(img2)\n        return img1, img2\nclass RandomCropPair(torchvision.transforms.RandomCrop): \n    # the crop will be intentionally different for the two images with this class\n    def forward(self, img1, img2):\n        img1 = super().forward(img1)\n        img2 = super().forward(img2)",
        "detail": "croco.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "RandomCropPair",
        "kind": 6,
        "importPath": "croco.datasets.transforms",
        "description": "croco.datasets.transforms",
        "peekOfCode": "class RandomCropPair(torchvision.transforms.RandomCrop): \n    # the crop will be intentionally different for the two images with this class\n    def forward(self, img1, img2):\n        img1 = super().forward(img1)\n        img2 = super().forward(img2)\n        return img1, img2\nclass ColorJitterPair(torchvision.transforms.ColorJitter): \n    # can be symmetric (same for both images) or assymetric (different jitter params for each image) depending on assymetric_prob  \n    def __init__(self, assymetric_prob, **kwargs):\n        super().__init__(**kwargs)",
        "detail": "croco.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "ColorJitterPair",
        "kind": 6,
        "importPath": "croco.datasets.transforms",
        "description": "croco.datasets.transforms",
        "peekOfCode": "class ColorJitterPair(torchvision.transforms.ColorJitter): \n    # can be symmetric (same for both images) or assymetric (different jitter params for each image) depending on assymetric_prob  \n    def __init__(self, assymetric_prob, **kwargs):\n        super().__init__(**kwargs)\n        self.assymetric_prob = assymetric_prob\n    def jitter_one(self, img, fn_idx, brightness_factor, contrast_factor, saturation_factor, hue_factor):\n        for fn_id in fn_idx:\n            if fn_id == 0 and brightness_factor is not None:\n                img = F.adjust_brightness(img, brightness_factor)\n            elif fn_id == 1 and contrast_factor is not None:",
        "detail": "croco.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "get_pair_transforms",
        "kind": 2,
        "importPath": "croco.datasets.transforms",
        "description": "croco.datasets.transforms",
        "peekOfCode": "def get_pair_transforms(transform_str, totensor=True, normalize=True):\n    # transform_str is eg    crop224+color\n    trfs = []\n    for s in transform_str.split('+'):\n        if s.startswith('crop'):\n            size = int(s[len('crop'):])\n            trfs.append(RandomCropPair(size))\n        elif s=='acolor':\n            trfs.append(ColorJitterPair(assymetric_prob=1.0, brightness=(0.6, 1.4), contrast=(0.6, 1.4), saturation=(0.6, 1.4), hue=0.0))\n        elif s=='': # if transform_str was \"\"",
        "detail": "croco.datasets.transforms",
        "documentation": {}
    },
    {
        "label": "cuRoPE2D_fun",
        "kind": 6,
        "importPath": "croco.models.curope.curope2d",
        "description": "croco.models.curope.curope2d",
        "peekOfCode": "class cuRoPE2D_func (torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, tokens, positions, base, F0=1):\n        ctx.save_for_backward(positions)\n        ctx.saved_base = base\n        ctx.saved_F0 = F0\n        # tokens = tokens.clone() # uncomment this if inplace doesn't work\n        _kernels.rope_2d( tokens, positions, base, F0 )\n        ctx.mark_dirty(tokens)\n        return tokens",
        "detail": "croco.models.curope.curope2d",
        "documentation": {}
    },
    {
        "label": "cuRoPE2D",
        "kind": 6,
        "importPath": "croco.models.curope.curope2d",
        "description": "croco.models.curope.curope2d",
        "peekOfCode": "class cuRoPE2D(torch.nn.Module):\n    def __init__(self, freq=100.0, F0=1.0):\n        super().__init__()\n        self.base = freq \n        self.F0 = F0\n    def forward(self, tokens, positions): \n        cuRoPE2D_func.apply( tokens.transpose(1,2), positions, self.base, self.F0 )\n        return tokens",
        "detail": "croco.models.curope.curope2d",
        "documentation": {}
    },
    {
        "label": "all_cuda_archs",
        "kind": 5,
        "importPath": "croco.models.curope.setup",
        "description": "croco.models.curope.setup",
        "peekOfCode": "all_cuda_archs = cuda.get_gencode_flags().replace('compute=','arch=').split()\n# alternatively, you can list cuda archs that you want, eg:\n# all_cuda_archs = [\n    # '-gencode', 'arch=compute_70,code=sm_70',\n    # '-gencode', 'arch=compute_75,code=sm_75',\n    # '-gencode', 'arch=compute_80,code=sm_80',\n    # '-gencode', 'arch=compute_86,code=sm_86'\n# ]\nsetup(\n    name = 'curope',",
        "detail": "croco.models.curope.setup",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "kind": 6,
        "importPath": "croco.models.blocks",
        "description": "croco.models.blocks",
        "peekOfCode": "class DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n        self.scale_by_keep = scale_by_keep\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n    def extra_repr(self):",
        "detail": "croco.models.blocks",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "croco.models.blocks",
        "description": "croco.models.blocks",
        "peekOfCode": "class Mlp(nn.Module):\n    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\"\"\"\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, bias=True, drop=0., zero_init_last = False):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        bias = to_2tuple(bias)\n        drop_probs = to_2tuple(drop)\n        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias[0])\n        self.act = act_layer()",
        "detail": "croco.models.blocks",
        "documentation": {}
    },
    {
        "label": "Attention",
        "kind": 6,
        "importPath": "croco.models.blocks",
        "description": "croco.models.blocks",
        "peekOfCode": "class Attention(nn.Module):\n    def __init__(self, dim, rope=None, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0., zero_init_last = False):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.head_dim = head_dim\n        self.scale = head_dim ** -0.5\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)",
        "detail": "croco.models.blocks",
        "documentation": {}
    },
    {
        "label": "Block",
        "kind": 6,
        "importPath": "croco.models.blocks",
        "description": "croco.models.blocks",
        "peekOfCode": "class Block(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, rope=None):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(dim, rope=rope, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)",
        "detail": "croco.models.blocks",
        "documentation": {}
    },
    {
        "label": "CrossAttention",
        "kind": 6,
        "importPath": "croco.models.blocks",
        "description": "croco.models.blocks",
        "peekOfCode": "class CrossAttention(nn.Module):\n    def __init__(self, dim, rope=None, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0., zero_init_last = False):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n        self.projq = nn.Linear(dim, dim, bias=qkv_bias)\n        self.projk = nn.Linear(dim, dim, bias=qkv_bias)\n        self.projv = nn.Linear(dim, dim, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)",
        "detail": "croco.models.blocks",
        "documentation": {}
    },
    {
        "label": "DecoderBlock",
        "kind": 6,
        "importPath": "croco.models.blocks",
        "description": "croco.models.blocks",
        "peekOfCode": "class DecoderBlock(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, norm_mem=True, rope=None, zero_init_last = False):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(dim, rope=rope, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop, zero_init_last = zero_init_last)\n        self.cross_attn = CrossAttention(dim, rope=rope, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop, zero_init_last = zero_init_last)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        self.norm3 = norm_layer(dim)",
        "detail": "croco.models.blocks",
        "documentation": {}
    },
    {
        "label": "PositionGetter",
        "kind": 6,
        "importPath": "croco.models.blocks",
        "description": "croco.models.blocks",
        "peekOfCode": "class PositionGetter(object):\n    \"\"\" return positions of patches \"\"\"\n    def __init__(self):\n        self.cache_positions = {}\n    def __call__(self, b, h, w, device):\n        if not (h,w) in self.cache_positions:\n            x = torch.arange(w, device=device)\n            y = torch.arange(h, device=device)\n            self.cache_positions[h,w] = torch.cartesian_prod(y, x) # (h, w, 2)\n        pos = self.cache_positions[h,w].view(1, h*w, 2).expand(b, -1, 2).clone()",
        "detail": "croco.models.blocks",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "croco.models.blocks",
        "description": "croco.models.blocks",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    \"\"\" just adding _init_weights + position getter compared to timm.models.layers.patch_embed.PatchEmbed\"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.num_patches = self.grid_size[0] * self.grid_size[1]",
        "detail": "croco.models.blocks",
        "documentation": {}
    },
    {
        "label": "drop_path",
        "kind": 2,
        "importPath": "croco.models.blocks",
        "description": "croco.models.blocks",
        "peekOfCode": "def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n    \"\"\"\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n    if keep_prob > 0.0 and scale_by_keep:\n        random_tensor.div_(keep_prob)",
        "detail": "croco.models.blocks",
        "documentation": {}
    },
    {
        "label": "to_2tuple",
        "kind": 5,
        "importPath": "croco.models.blocks",
        "description": "croco.models.blocks",
        "peekOfCode": "to_2tuple = _ntuple(2)\ndef drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n    \"\"\"\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n    if keep_prob > 0.0 and scale_by_keep:",
        "detail": "croco.models.blocks",
        "documentation": {}
    },
    {
        "label": "MaskedMSE",
        "kind": 6,
        "importPath": "croco.models.criterion",
        "description": "croco.models.criterion",
        "peekOfCode": "class MaskedMSE(torch.nn.Module):\n    def __init__(self, norm_pix_loss=False, masked=True):\n        \"\"\"\n            norm_pix_loss: normalize each patch by their pixel mean and variance\n            masked: compute loss over the masked patches only \n        \"\"\"\n        super().__init__()\n        self.norm_pix_loss = norm_pix_loss\n        self.masked = masked \n    def forward(self, pred, mask, target):",
        "detail": "croco.models.criterion",
        "documentation": {}
    },
    {
        "label": "CroCoNet",
        "kind": 6,
        "importPath": "croco.models.croco",
        "description": "croco.models.croco",
        "peekOfCode": "class CroCoNet(nn.Module):\n    def __init__(self, # (pos_embed='RoPE100', img_size=(224, 224), enc_embed_dim=1024, enc_depth=24, enc_num_heads=16, dec_embed_dim=768, dec_depth=12, dec_num_heads=12)\n                 img_size=224,           # input image size\n                 patch_size=16,          # patch_size \n                 mask_ratio=0.9,         # ratios of masked tokens \n                 enc_embed_dim=768,      # encoder feature dimension\n                 enc_depth=12,           # encoder depth \n                 enc_num_heads=12,       # encoder number of heads in the transformer block \n                 dec_embed_dim=512,      # decoder feature dimension \n                 dec_depth=8,            # decoder depth ",
        "detail": "croco.models.croco",
        "documentation": {}
    },
    {
        "label": "torch.backends.cuda.matmul.allow_tf32",
        "kind": 5,
        "importPath": "croco.models.croco",
        "description": "croco.models.croco",
        "peekOfCode": "torch.backends.cuda.matmul.allow_tf32 = True # for gpu >= Ampere and pytorch >= 1.12\nfrom functools import partial\nfrom models.blocks import Block, DecoderBlock, PatchEmbed\nfrom models.pos_embed import get_2d_sincos_pos_embed, RoPE2D \nfrom models.masking import RandomMask\nclass CroCoNet(nn.Module):\n    def __init__(self, # (pos_embed='RoPE100', img_size=(224, 224), enc_embed_dim=1024, enc_depth=24, enc_num_heads=16, dec_embed_dim=768, dec_depth=12, dec_num_heads=12)\n                 img_size=224,           # input image size\n                 patch_size=16,          # patch_size \n                 mask_ratio=0.9,         # ratios of masked tokens ",
        "detail": "croco.models.croco",
        "documentation": {}
    },
    {
        "label": "CroCoDownstreamMonocularEncoder",
        "kind": 6,
        "importPath": "croco.models.croco_downstream",
        "description": "croco.models.croco_downstream",
        "peekOfCode": "class CroCoDownstreamMonocularEncoder(CroCoNet):\n    def __init__(self,\n                 head,\n                 **kwargs):\n        \"\"\" Build network for monocular downstream task, only using the encoder.\n        It takes an extra argument head, that is called with the features \n          and a dictionary img_info containing 'width' and 'height' keys\n        The head is setup with the croconet arguments in this init function\n        NOTE: It works by *calling super().__init__() but with redefined setters\n        \"\"\"",
        "detail": "croco.models.croco_downstream",
        "documentation": {}
    },
    {
        "label": "CroCoDownstreamBinocular",
        "kind": 6,
        "importPath": "croco.models.croco_downstream",
        "description": "croco.models.croco_downstream",
        "peekOfCode": "class CroCoDownstreamBinocular(CroCoNet):\n    def __init__(self,\n                 head,\n                 **kwargs):\n        \"\"\" Build network for binocular downstream task\n        It takes an extra argument head, that is called with the features \n          and a dictionary img_info containing 'width' and 'height' keys\n        The head is setup with the croconet arguments in this init function\n        \"\"\"\n        super(CroCoDownstreamBinocular, self).__init__(**kwargs)",
        "detail": "croco.models.croco_downstream",
        "documentation": {}
    },
    {
        "label": "croco_args_from_ckpt",
        "kind": 2,
        "importPath": "croco.models.croco_downstream",
        "description": "croco.models.croco_downstream",
        "peekOfCode": "def croco_args_from_ckpt(ckpt):\n    if 'croco_kwargs' in ckpt: # CroCo v2 released models\n        return ckpt['croco_kwargs']\n    elif 'args' in ckpt and hasattr(ckpt['args'], 'model'): # pretrained using the official code release\n        s = ckpt['args'].model # eg \"CroCoNet(enc_embed_dim=1024, enc_num_heads=16, enc_depth=24)\"\n        assert s.startswith('CroCoNet(')\n        return eval('dict'+s[len('CroCoNet'):]) # transform it into the string of a dictionary and evaluate it\n    else: # CroCo v1 released models\n        return dict()\nclass CroCoDownstreamMonocularEncoder(CroCoNet):",
        "detail": "croco.models.croco_downstream",
        "documentation": {}
    },
    {
        "label": "ResidualConvUnit_custom",
        "kind": 6,
        "importPath": "croco.models.dpt_block",
        "description": "croco.models.dpt_block",
        "peekOfCode": "class ResidualConvUnit_custom(nn.Module):\n    \"\"\"Residual convolution module.\"\"\"\n    def __init__(self, features, activation, bn):\n        \"\"\"Init.\n        Args:\n            features (int): number of features\n        \"\"\"\n        super().__init__()\n        self.bn = bn\n        self.groups = 1",
        "detail": "croco.models.dpt_block",
        "documentation": {}
    },
    {
        "label": "FeatureFusionBlock_custom",
        "kind": 6,
        "importPath": "croco.models.dpt_block",
        "description": "croco.models.dpt_block",
        "peekOfCode": "class FeatureFusionBlock_custom(nn.Module):\n    \"\"\"Feature fusion block.\"\"\"\n    def __init__(\n        self,\n        features,\n        activation,\n        deconv=False,\n        bn=False,\n        expand=False,\n        align_corners=True,",
        "detail": "croco.models.dpt_block",
        "documentation": {}
    },
    {
        "label": "Interpolate",
        "kind": 6,
        "importPath": "croco.models.dpt_block",
        "description": "croco.models.dpt_block",
        "peekOfCode": "class Interpolate(nn.Module):\n    \"\"\"Interpolation module.\"\"\"\n    def __init__(self, scale_factor, mode, align_corners=False):\n        \"\"\"Init.\n        Args:\n            scale_factor (float): scaling\n            mode (str): interpolation mode\n        \"\"\"\n        super(Interpolate, self).__init__()\n        self.interp = nn.functional.interpolate",
        "detail": "croco.models.dpt_block",
        "documentation": {}
    },
    {
        "label": "DPTOutputAdapter",
        "kind": 6,
        "importPath": "croco.models.dpt_block",
        "description": "croco.models.dpt_block",
        "peekOfCode": "class DPTOutputAdapter(nn.Module):\n    \"\"\"DPT output adapter.\n    :param num_cahnnels: Number of output channels\n    :param stride_level: tride level compared to the full-sized image.\n        E.g. 4 for 1/4th the size of the image.\n    :param patch_size_full: Int or tuple of the patch size over the full image size.\n        Patch size for smaller inputs will be computed accordingly.\n    :param hooks: Index of intermediate layers\n    :param layer_dims: Dimension of intermediate layers\n    :param feature_dim: Feature dimension",
        "detail": "croco.models.dpt_block",
        "documentation": {}
    },
    {
        "label": "pair",
        "kind": 2,
        "importPath": "croco.models.dpt_block",
        "description": "croco.models.dpt_block",
        "peekOfCode": "def pair(t):\n    return t if isinstance(t, tuple) else (t, t)\ndef make_scratch(in_shape, out_shape, groups=1, expand=False):\n    scratch = nn.Module()\n    out_shape1 = out_shape\n    out_shape2 = out_shape\n    out_shape3 = out_shape\n    out_shape4 = out_shape\n    if expand == True:\n        out_shape1 = out_shape",
        "detail": "croco.models.dpt_block",
        "documentation": {}
    },
    {
        "label": "make_scratch",
        "kind": 2,
        "importPath": "croco.models.dpt_block",
        "description": "croco.models.dpt_block",
        "peekOfCode": "def make_scratch(in_shape, out_shape, groups=1, expand=False):\n    scratch = nn.Module()\n    out_shape1 = out_shape\n    out_shape2 = out_shape\n    out_shape3 = out_shape\n    out_shape4 = out_shape\n    if expand == True:\n        out_shape1 = out_shape\n        out_shape2 = out_shape * 2\n        out_shape3 = out_shape * 4",
        "detail": "croco.models.dpt_block",
        "documentation": {}
    },
    {
        "label": "make_fusion_block",
        "kind": 2,
        "importPath": "croco.models.dpt_block",
        "description": "croco.models.dpt_block",
        "peekOfCode": "def make_fusion_block(features, use_bn, width_ratio=1):\n    return FeatureFusionBlock_custom(\n        features,\n        nn.ReLU(False),\n        deconv=False,\n        bn=use_bn,\n        expand=False,\n        align_corners=True,\n        width_ratio=width_ratio,\n    )",
        "detail": "croco.models.dpt_block",
        "documentation": {}
    },
    {
        "label": "PixelwiseTaskWithDPT",
        "kind": 6,
        "importPath": "croco.models.head_downstream",
        "description": "croco.models.head_downstream",
        "peekOfCode": "class PixelwiseTaskWithDPT(nn.Module):\n    \"\"\" DPT module for CroCo.\n    by default, hooks_idx will be equal to:\n    * for encoder-only: 4 equally spread layers\n    * for encoder+decoder: last encoder + 3 equally spread layers of the decoder \n    \"\"\"\n    def __init__(self, *, hooks_idx=None, layer_dims=[96,192,384,768],\n                 output_width_ratio=1, num_channels=1, postprocess=None, **kwargs):\n        super(PixelwiseTaskWithDPT, self).__init__()\n        self.return_all_blocks = True # backbone needs to return all layers ",
        "detail": "croco.models.head_downstream",
        "documentation": {}
    },
    {
        "label": "RandomMask",
        "kind": 6,
        "importPath": "croco.models.masking",
        "description": "croco.models.masking",
        "peekOfCode": "class RandomMask(nn.Module):\n    \"\"\"\n    random masking\n    \"\"\"\n    def __init__(self, num_patches, mask_ratio):\n        super().__init__()\n        self.num_patches = num_patches\n        self.num_mask = int(mask_ratio * self.num_patches)\n    def __call__(self, x):\n        noise = torch.rand(x.size(0), self.num_patches, device=x.device) ",
        "detail": "croco.models.masking",
        "documentation": {}
    },
    {
        "label": "get_2d_sincos_pos_embed",
        "kind": 2,
        "importPath": "croco.models.pos_embed",
        "description": "croco.models.pos_embed",
        "peekOfCode": "def get_2d_sincos_pos_embed(embed_dim, grid_size, n_cls_token=0):\n    \"\"\"\n    grid_size: int of the grid height and width\n    return:\n    pos_embed: [grid_size*grid_size, embed_dim] or [n_cls_token+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n    \"\"\"\n    grid_h = np.arange(grid_size, dtype=np.float32)\n    grid_w = np.arange(grid_size, dtype=np.float32)\n    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n    grid = np.stack(grid, axis=0)",
        "detail": "croco.models.pos_embed",
        "documentation": {}
    },
    {
        "label": "get_2d_sincos_pos_embed_from_grid",
        "kind": 2,
        "importPath": "croco.models.pos_embed",
        "description": "croco.models.pos_embed",
        "peekOfCode": "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n    assert embed_dim % 2 == 0\n    # use half of dimensions to encode grid_h\n    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n    return emb\ndef get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n    \"\"\"\n    embed_dim: output dimension for each position",
        "detail": "croco.models.pos_embed",
        "documentation": {}
    },
    {
        "label": "get_1d_sincos_pos_embed_from_grid",
        "kind": 2,
        "importPath": "croco.models.pos_embed",
        "description": "croco.models.pos_embed",
        "peekOfCode": "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n    \"\"\"\n    embed_dim: output dimension for each position\n    pos: a list of positions to be encoded: size (M,)\n    out: (M, D)\n    \"\"\"\n    assert embed_dim % 2 == 0\n    omega = np.arange(embed_dim // 2, dtype=float)\n    omega /= embed_dim / 2.\n    omega = 1. / 10000**omega  # (D/2,)",
        "detail": "croco.models.pos_embed",
        "documentation": {}
    },
    {
        "label": "interpolate_pos_embed",
        "kind": 2,
        "importPath": "croco.models.pos_embed",
        "description": "croco.models.pos_embed",
        "peekOfCode": "def interpolate_pos_embed(model, checkpoint_model):\n    if 'pos_embed' in checkpoint_model:\n        pos_embed_checkpoint = checkpoint_model['pos_embed']\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        num_patches = model.patch_embed.num_patches\n        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n        # height (== width) for the checkpoint position embedding\n        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n        # height (== width) for the new position embedding\n        new_size = int(num_patches ** 0.5)",
        "detail": "croco.models.pos_embed",
        "documentation": {}
    },
    {
        "label": "StereoAugmentor",
        "kind": 6,
        "importPath": "croco.stereoflow.augmentor",
        "description": "croco.stereoflow.augmentor",
        "peekOfCode": "class StereoAugmentor(object):\n    def __init__(self, crop_size, scale_prob=0.5, scale_xonly=True, lhth=800., lminscale=0.0, lmaxscale=1.0, hminscale=-0.2, hmaxscale=0.4, scale_interp_nearest=True, rightjitterprob=0.5, v_flip_prob=0.5, color_aug_asym=True, color_choice_prob=0.5):\n        self.crop_size = crop_size\n        self.scale_prob = scale_prob\n        self.scale_xonly = scale_xonly\n        self.lhth = lhth\n        self.lminscale = lminscale\n        self.lmaxscale = lmaxscale\n        self.hminscale = hminscale\n        self.hmaxscale = hmaxscale",
        "detail": "croco.stereoflow.augmentor",
        "documentation": {}
    },
    {
        "label": "FlowAugmentor",
        "kind": 6,
        "importPath": "croco.stereoflow.augmentor",
        "description": "croco.stereoflow.augmentor",
        "peekOfCode": "class FlowAugmentor:\n    def __init__(self, crop_size, min_scale=-0.2, max_scale=0.5, spatial_aug_prob=0.8, stretch_prob=0.8, max_stretch=0.2, h_flip_prob=0.5, v_flip_prob=0.1, asymmetric_color_aug_prob=0.2):\n        # spatial augmentation params\n        self.crop_size = crop_size\n        self.min_scale = min_scale\n        self.max_scale = max_scale\n        self.spatial_aug_prob = spatial_aug_prob\n        self.stretch_prob = stretch_prob\n        self.max_stretch = max_stretch\n        # flip augmentation params",
        "detail": "croco.stereoflow.augmentor",
        "documentation": {}
    },
    {
        "label": "L1Loss",
        "kind": 6,
        "importPath": "croco.stereoflow.criterion",
        "description": "croco.stereoflow.criterion",
        "peekOfCode": "class L1Loss(nn.Module):\n    def __init__(self, max_gtnorm=None):\n        super().__init__()\n        self.max_gtnorm = max_gtnorm\n        self.with_conf = False \n    def _error(self, gt, predictions):\n        return torch.abs(gt-predictions)\n    def forward(self, predictions, gt, inspect=False):\n        mask = torch.isfinite(gt)\n        if self.max_gtnorm is not None: ",
        "detail": "croco.stereoflow.criterion",
        "documentation": {}
    },
    {
        "label": "LaplacianLoss",
        "kind": 6,
        "importPath": "croco.stereoflow.criterion",
        "description": "croco.stereoflow.criterion",
        "peekOfCode": "class LaplacianLoss(nn.Module): # used for CroCo-Stereo on ETH3D, d'=exp(d)\n    def __init__(self, max_gtnorm=None):\n        super().__init__()\n        self.max_gtnorm = max_gtnorm\n        self.with_conf = True\n    def forward(self, predictions, gt, conf):\n        mask = torch.isfinite(gt)\n        mask = mask[:,0,:,:]\n        if self.max_gtnorm is not None: mask *= _get_gtnorm(gt)[:,0,:,:]<self.max_gtnorm\n        conf = conf.squeeze(1)",
        "detail": "croco.stereoflow.criterion",
        "documentation": {}
    },
    {
        "label": "LaplacianLossBounded",
        "kind": 6,
        "importPath": "croco.stereoflow.criterion",
        "description": "croco.stereoflow.criterion",
        "peekOfCode": "class LaplacianLossBounded(nn.Module): # used for CroCo-Flow ; in the equation of the paper, we have a=1/b\n    def __init__(self, max_gtnorm=10000., a=0.25, b=4.):\n        super().__init__()\n        self.max_gtnorm = max_gtnorm\n        self.with_conf = True\n        self.a, self.b = a, b\n    def forward(self, predictions, gt, conf):\n        mask = torch.isfinite(gt)\n        mask = mask[:,0,:,:]\n        if self.max_gtnorm is not None: mask *= _get_gtnorm(gt)[:,0,:,:]<self.max_gtnorm",
        "detail": "croco.stereoflow.criterion",
        "documentation": {}
    },
    {
        "label": "LaplacianLossBounded2",
        "kind": 6,
        "importPath": "croco.stereoflow.criterion",
        "description": "croco.stereoflow.criterion",
        "peekOfCode": "class LaplacianLossBounded2(nn.Module): # used for CroCo-Stereo (except for ETH3D) ; in the equation of the paper, we have a=b\n    def __init__(self, max_gtnorm=None, a=3.0, b=3.0):\n        super().__init__()\n        self.max_gtnorm = max_gtnorm\n        self.with_conf = True\n        self.a, self.b = a, b\n    def forward(self, predictions, gt, conf):\n        mask = torch.isfinite(gt)\n        mask = mask[:,0,:,:]\n        if self.max_gtnorm is not None: mask *= _get_gtnorm(gt)[:,0,:,:]<self.max_gtnorm",
        "detail": "croco.stereoflow.criterion",
        "documentation": {}
    },
    {
        "label": "StereoMetrics",
        "kind": 6,
        "importPath": "croco.stereoflow.criterion",
        "description": "croco.stereoflow.criterion",
        "peekOfCode": "class StereoMetrics(nn.Module):\n    def __init__(self, do_quantile=False):\n        super().__init__()\n        self.bad_ths = [0.5,1,2,3]\n        self.do_quantile = do_quantile\n    def forward(self, predictions, gt):\n        B = predictions.size(0)\n        metrics = {}\n        gtcopy = gt.clone() \n        mask = torch.isfinite(gtcopy)",
        "detail": "croco.stereoflow.criterion",
        "documentation": {}
    },
    {
        "label": "FlowMetrics",
        "kind": 6,
        "importPath": "croco.stereoflow.criterion",
        "description": "croco.stereoflow.criterion",
        "peekOfCode": "class FlowMetrics(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bad_ths = [1,3,5]\n    def forward(self, predictions, gt):\n        B = predictions.size(0)        \n        metrics = {}\n        mask = torch.isfinite(gt[:,0,:,:]) # both x and y would be infinite\n        Npx = mask.view(B,-1).sum(dim=1)\n        gtcopy = gt.clone() # to compute L1/L2 error, we need to have non-infinite value, the error computed at this locations will be ignored",
        "detail": "croco.stereoflow.criterion",
        "documentation": {}
    },
    {
        "label": "StereoDatasetMetrics",
        "kind": 6,
        "importPath": "croco.stereoflow.criterion",
        "description": "croco.stereoflow.criterion",
        "peekOfCode": "class StereoDatasetMetrics(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bad_ths = [0.5,1,2,3]\n    def reset(self):\n        self.agg_N = 0 # number of pixels so far \n        self.agg_L1err = torch.tensor(0.0) # L1 error so far \n        self.agg_Nbad = [0 for _ in self.bad_ths] # counter of bad pixels \n        self._metrics = None\n    def add_batch(self, predictions, gt):",
        "detail": "croco.stereoflow.criterion",
        "documentation": {}
    },
    {
        "label": "FlowDatasetMetrics",
        "kind": 6,
        "importPath": "croco.stereoflow.criterion",
        "description": "croco.stereoflow.criterion",
        "peekOfCode": "class FlowDatasetMetrics(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bad_ths = [0.5,1,3,5]\n        self.speed_ths = [(0,10),(10,40),(40,torch.inf)]\n    def reset(self):\n        self.agg_N = 0 # number of pixels so far \n        self.agg_L1err = torch.tensor(0.0) # L1 error so far \n        self.agg_L2err = torch.tensor(0.0) # L2 (=EPE) error so far \n        self.agg_Nbad = [0 for _ in self.bad_ths] # counter of bad pixels ",
        "detail": "croco.stereoflow.criterion",
        "documentation": {}
    },
    {
        "label": "FlowDataset",
        "kind": 6,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "class FlowDataset(data.Dataset):\n    def __init__(self, split, augmentor=False, crop_size=None, totensor=True):\n        self.split = split\n        if not augmentor: assert crop_size is None \n        if crop_size is not None: assert augmentor\n        self.crop_size = crop_size\n        self.augmentor_str = augmentor\n        self.augmentor = FlowAugmentor(crop_size) if augmentor else None\n        self.totensor = totensor\n        self.rmul = 1 # keep track of rmul",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "TartanAirDataset",
        "kind": 6,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "class TartanAirDataset(FlowDataset):\n    def _prepare_data(self):\n        self.name = \"TartanAir\"\n        self._set_root()\n        assert self.split in ['train']\n        self.pairname_to_img1name = lambda pairname: osp.join(self.root, pairname[0], 'image_left/{:06d}_left.png'.format(pairname[1]))\n        self.pairname_to_img2name = lambda pairname: osp.join(self.root, pairname[0], 'image_left/{:06d}_left.png'.format(pairname[2]))\n        self.pairname_to_flowname = lambda pairname: osp.join(self.root, pairname[0], 'flow/{:06d}_{:06d}_flow.npy'.format(pairname[1],pairname[2]))\n        self.pairname_to_str = lambda pairname: os.path.join(pairname[0][pairname[0].find('/')+1:], '{:06d}_{:06d}'.format(pairname[1], pairname[2]))\n        self.load_flow = _read_numpy_flow",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "FlyingChairsDataset",
        "kind": 6,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "class FlyingChairsDataset(FlowDataset):\n    def _prepare_data(self):\n        self.name = \"FlyingChairs\"\n        self._set_root()\n        assert self.split in ['train','val']\n        self.pairname_to_img1name = lambda pairname: osp.join(self.root, 'data', pairname+'_img1.ppm')\n        self.pairname_to_img2name = lambda pairname: osp.join(self.root, 'data', pairname+'_img2.ppm')\n        self.pairname_to_flowname = lambda pairname: osp.join(self.root, 'data', pairname+'_flow.flo')\n        self.pairname_to_str = lambda pairname: pairname\n        self.load_flow = _read_flo_file",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "FlyingThingsDataset",
        "kind": 6,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "class FlyingThingsDataset(FlowDataset):\n    def _prepare_data(self):\n        self.name = \"FlyingThings\"\n        self._set_root()\n        assert self.split in [f'{set_}_{pass_}pass{camstr}' for set_ in ['train','test','test1024'] for camstr in ['','_rightcam'] for pass_ in ['clean','final','all']]\n        self.pairname_to_img1name = lambda pairname: osp.join(self.root, f'frames_{pairname[3]}pass', pairname[0].replace('into_future','').replace('into_past',''), '{:04d}.png'.format(pairname[1]))\n        self.pairname_to_img2name = lambda pairname: osp.join(self.root, f'frames_{pairname[3]}pass', pairname[0].replace('into_future','').replace('into_past',''), '{:04d}.png'.format(pairname[2]))\n        self.pairname_to_flowname = lambda pairname: osp.join(self.root, 'optical_flow', pairname[0], 'OpticalFlowInto{f:s}_{i:04d}_{c:s}.pfm'.format(f='Future' if 'future' in pairname[0] else 'Past', i=pairname[1], c='L' if 'left' in pairname[0] else 'R' ))\n        self.pairname_to_str = lambda pairname: os.path.join(pairname[3]+'pass', pairname[0], 'Into{f:s}_{i:04d}_{c:s}'.format(f='Future' if 'future' in pairname[0] else 'Past',  i=pairname[1], c='L' if 'left' in pairname[0] else 'R' ))\n        self.load_flow = _read_pfm_flow",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "MPISintelDataset",
        "kind": 6,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "class MPISintelDataset(FlowDataset):\n    def _prepare_data(self):\n        self.name = \"MPISintel\"\n        self._set_root()\n        assert self.split in [s+'_'+p for s in ['train','test','subval','subtrain'] for p in ['cleanpass','finalpass','allpass']]\n        self.pairname_to_img1name = lambda pairname: osp.join(self.root, pairname[0], 'frame_{:04d}.png'.format(pairname[1]))\n        self.pairname_to_img2name = lambda pairname: osp.join(self.root, pairname[0], 'frame_{:04d}.png'.format(pairname[1]+1))\n        self.pairname_to_flowname = lambda pairname: None if pairname[0].startswith('test/') else osp.join(self.root, pairname[0].replace('/clean/','/flow/').replace('/final/','/flow/'), 'frame_{:04d}.flo'.format(pairname[1]))\n        self.pairname_to_str = lambda pairname: osp.join(pairname[0], 'frame_{:04d}'.format(pairname[1]))\n        self.load_flow = _read_flo_file",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "SpringDataset",
        "kind": 6,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "class SpringDataset(FlowDataset):\n    def _prepare_data(self):\n        self.name = \"Spring\"\n        self._set_root()\n        assert self.split in ['train','test','subtrain','subval']\n        self.pairname_to_img1name = lambda pairname: osp.join(self.root, pairname[0], pairname[1], 'frame_'+pairname[3], 'frame_{:s}_{:04d}.png'.format(pairname[3], pairname[4]))\n        self.pairname_to_img2name = lambda pairname: osp.join(self.root, pairname[0], pairname[1], 'frame_'+pairname[3], 'frame_{:s}_{:04d}.png'.format(pairname[3], pairname[4]+(1 if pairname[2]=='FW' else -1)))\n        self.pairname_to_flowname = lambda pairname: None if pairname[0]=='test' else osp.join(self.root, pairname[0], pairname[1], f'flow_{pairname[2]}_{pairname[3]}', f'flow_{pairname[2]}_{pairname[3]}_{pairname[4]:04d}.flo5')\n        self.pairname_to_str = lambda pairname: osp.join(pairname[0], pairname[1], f'flow_{pairname[2]}_{pairname[3]}', f'flow_{pairname[2]}_{pairname[3]}_{pairname[4]:04d}')\n        self.load_flow = _read_hdf5_flow",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "Kitti12Dataset",
        "kind": 6,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "class Kitti12Dataset(FlowDataset):\n    def _prepare_data(self):\n        self.name = \"Kitti12\"\n        self._set_root()\n        assert self.split in ['train','test']\n        self.pairname_to_img1name = lambda pairname: osp.join(self.root, pairname+'_10.png')\n        self.pairname_to_img2name = lambda pairname: osp.join(self.root, pairname+'_11.png')\n        self.pairname_to_flowname = None if self.split=='test' else lambda pairname: osp.join(self.root, pairname.replace('/colored_0/','/flow_occ/')+'_10.png')\n        self.pairname_to_str = lambda pairname: pairname.replace('/colored_0/','/')\n        self.load_flow = _read_kitti_flow",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "Kitti15Dataset",
        "kind": 6,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "class Kitti15Dataset(FlowDataset):\n    def _prepare_data(self):\n        self.name = \"Kitti15\"\n        self._set_root()\n        assert self.split in ['train','subtrain','subval','test']\n        self.pairname_to_img1name = lambda pairname: osp.join(self.root, pairname+'_10.png')\n        self.pairname_to_img2name = lambda pairname: osp.join(self.root, pairname+'_11.png')\n        self.pairname_to_flowname = None if self.split=='test' else lambda pairname: osp.join(self.root, pairname.replace('/image_2/','/flow_occ/')+'_10.png')\n        self.pairname_to_str = lambda pairname: pairname.replace('/image_2/','/')\n        self.load_flow = _read_kitti_flow",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "flow_to_tensor",
        "kind": 2,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "def flow_to_tensor(disp):\n    return torch.from_numpy(disp).float().permute(2, 0, 1)\nclass FlowDataset(data.Dataset):\n    def __init__(self, split, augmentor=False, crop_size=None, totensor=True):\n        self.split = split\n        if not augmentor: assert crop_size is None \n        if crop_size is not None: assert augmentor\n        self.crop_size = crop_size\n        self.augmentor_str = augmentor\n        self.augmentor = FlowAugmentor(crop_size) if augmentor else None",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "readFlowFile",
        "kind": 2,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "def readFlowFile(filename):\n    \"\"\"\n    readFlowFile(<FILENAME>) reads a flow file <FILENAME> into a 2-band np.array.\n    if <FILENAME> does not exist, an IOError is raised.\n    if <FILENAME> does not finish by '.flo' or the tag, the width, the height or the file's size is illegal, an Expcetion is raised.\n    ---- PARAMETERS ----\n        filename: string containg the name of the file to read a flow\n    ---- OUTPUTS ----\n        a np.array of dimension (height x width x 2) containing the flow of type 'float32'\n    \"\"\"",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "writeFlowFile",
        "kind": 2,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "def writeFlowFile(flow,filename):\n    \"\"\"\n    writeFlowFile(flow,<FILENAME>) write flow to the file <FILENAME>.\n    if <FILENAME> does not exist, an IOError is raised.\n    if <FILENAME> does not finish with '.flo' or the flow has not 2 bands, an Exception is raised.\n    ---- PARAMETERS ----\n        flow: np.array of dimension (height x width x 2) containing the flow to write\n        filename: string containg the name of the file to write a flow\n    \"\"\"\n    # check filename",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "writeFlowKitti",
        "kind": 2,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "def writeFlowKitti(filename, uv):\n    uv = 64.0 * uv + 2 ** 15\n    valid = np.ones([uv.shape[0], uv.shape[1], 1])\n    uv = np.concatenate([uv, valid], axis=-1).astype(np.uint16)\n    cv2.imwrite(filename, uv[..., ::-1])\ndef writeFlo5File(flow, filename):\n    with h5py.File(filename, \"w\") as f:\n        f.create_dataset(\"flow\", data=flow, compression=\"gzip\", compression_opts=5)\ndef _read_hdf5_flow(filename):\n    flow = np.asarray(h5py.File(filename)['flow'])",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "writeFlo5File",
        "kind": 2,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "def writeFlo5File(flow, filename):\n    with h5py.File(filename, \"w\") as f:\n        f.create_dataset(\"flow\", data=flow, compression=\"gzip\", compression_opts=5)\ndef _read_hdf5_flow(filename):\n    flow = np.asarray(h5py.File(filename)['flow'])\n    flow[np.isnan(flow)] = np.inf # make invalid values as +inf\n    return flow.astype(np.float32)\n# flow visualization\nRY = 15\nYG = 6",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "colorTest",
        "kind": 2,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "def colorTest():\n    \"\"\"\n    flow_utils.colorTest(): display an example of image showing the color encoding scheme\n    \"\"\"\n    import matplotlib.pylab as plt\n    truerange = 1\n    h,w = 151,151\n    trange = truerange*1.04\n    s2 = round(h/2)\n    x,y = np.meshgrid(range(w),range(h))",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "flowToColor",
        "kind": 2,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "def flowToColor(flow, maxflow=None, maxmaxflow=None, saturate=False):\n    \"\"\"\n    flow_utils.flowToColor(flow): return a color code flow field, normalized based on the maximum l2-norm of the flow\n    flow_utils.flowToColor(flow,maxflow): return a color code flow field, normalized by maxflow\n    ---- PARAMETERS ----\n        flow: flow to display of shape (height x width x 2)\n        maxflow (default:None): if given, normalize the flow by its value, otherwise by the flow norm\n        maxmaxflow (default:None): if given, normalize the flow by the max of its value and the flow norm\n    ---- OUTPUT ----\n        an np.array of shape (height x width x 3) of type uint8 containing a color code of the flow",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "flowMaxNorm",
        "kind": 2,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "def flowMaxNorm(flow):\n    \"\"\"\n    flow_utils.flowMaxNorm(flow): return the maximum of the l2-norm of the given flow\n    ---- PARAMETERS ----\n        flow: the flow\n    ---- OUTPUT ----\n        a float containing the maximum of the l2-norm of the flow\n    \"\"\"\n    return np.max( np.sqrt( np.sum( np.square( flow ) , 2) ) )\ndef _computeColor(flow, saturate=True):",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "get_train_dataset_flow",
        "kind": 2,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "def get_train_dataset_flow(dataset_str, augmentor=True, crop_size=None):\n    dataset_str = dataset_str.replace('(','Dataset(')\n    if augmentor:\n        dataset_str = dataset_str.replace(')',', augmentor=True)')\n    if crop_size is not None:\n        dataset_str = dataset_str.replace(')',', crop_size={:s})'.format(str(crop_size)))\n    return eval(dataset_str)\ndef get_test_datasets_flow(dataset_str):\n    dataset_str = dataset_str.replace('(','Dataset(')\n    return [eval(s) for s in dataset_str.split('+')]",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "get_test_datasets_flow",
        "kind": 2,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "def get_test_datasets_flow(dataset_str):\n    dataset_str = dataset_str.replace('(','Dataset(')\n    return [eval(s) for s in dataset_str.split('+')]",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "dataset_to_root",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "dataset_to_root = deepcopy(dataset_to_root)\ndataset_to_root.update(**{\n    'TartanAir': './data/stereoflow/TartanAir',\n    'FlyingChairs': './data/stereoflow/FlyingChairs/',\n    'FlyingThings': osp.join(dataset_to_root['SceneFlow'],'FlyingThings')+'/',\n    'MPISintel': './data/stereoflow//MPI-Sintel/'+'/',\n})\ncache_dir = \"./data/stereoflow/datasets_flow_cache/\"\ndef flow_to_tensor(disp):\n    return torch.from_numpy(disp).float().permute(2, 0, 1)",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "cache_dir",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "cache_dir = \"./data/stereoflow/datasets_flow_cache/\"\ndef flow_to_tensor(disp):\n    return torch.from_numpy(disp).float().permute(2, 0, 1)\nclass FlowDataset(data.Dataset):\n    def __init__(self, split, augmentor=False, crop_size=None, totensor=True):\n        self.split = split\n        if not augmentor: assert crop_size is None \n        if crop_size is not None: assert augmentor\n        self.crop_size = crop_size\n        self.augmentor_str = augmentor",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "TAG_FLOAT",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "TAG_FLOAT = 202021.25 # tag to check the sanity of the file\nTAG_STRING = 'PIEH'   # string containing the tag\nMIN_WIDTH = 1\nMAX_WIDTH = 99999\nMIN_HEIGHT = 1\nMAX_HEIGHT = 99999\ndef readFlowFile(filename):\n    \"\"\"\n    readFlowFile(<FILENAME>) reads a flow file <FILENAME> into a 2-band np.array.\n    if <FILENAME> does not exist, an IOError is raised.",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "TAG_STRING",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "TAG_STRING = 'PIEH'   # string containing the tag\nMIN_WIDTH = 1\nMAX_WIDTH = 99999\nMIN_HEIGHT = 1\nMAX_HEIGHT = 99999\ndef readFlowFile(filename):\n    \"\"\"\n    readFlowFile(<FILENAME>) reads a flow file <FILENAME> into a 2-band np.array.\n    if <FILENAME> does not exist, an IOError is raised.\n    if <FILENAME> does not finish by '.flo' or the tag, the width, the height or the file's size is illegal, an Expcetion is raised.",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "MIN_WIDTH",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "MIN_WIDTH = 1\nMAX_WIDTH = 99999\nMIN_HEIGHT = 1\nMAX_HEIGHT = 99999\ndef readFlowFile(filename):\n    \"\"\"\n    readFlowFile(<FILENAME>) reads a flow file <FILENAME> into a 2-band np.array.\n    if <FILENAME> does not exist, an IOError is raised.\n    if <FILENAME> does not finish by '.flo' or the tag, the width, the height or the file's size is illegal, an Expcetion is raised.\n    ---- PARAMETERS ----",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "MAX_WIDTH",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "MAX_WIDTH = 99999\nMIN_HEIGHT = 1\nMAX_HEIGHT = 99999\ndef readFlowFile(filename):\n    \"\"\"\n    readFlowFile(<FILENAME>) reads a flow file <FILENAME> into a 2-band np.array.\n    if <FILENAME> does not exist, an IOError is raised.\n    if <FILENAME> does not finish by '.flo' or the tag, the width, the height or the file's size is illegal, an Expcetion is raised.\n    ---- PARAMETERS ----\n        filename: string containg the name of the file to read a flow",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "MIN_HEIGHT",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "MIN_HEIGHT = 1\nMAX_HEIGHT = 99999\ndef readFlowFile(filename):\n    \"\"\"\n    readFlowFile(<FILENAME>) reads a flow file <FILENAME> into a 2-band np.array.\n    if <FILENAME> does not exist, an IOError is raised.\n    if <FILENAME> does not finish by '.flo' or the tag, the width, the height or the file's size is illegal, an Expcetion is raised.\n    ---- PARAMETERS ----\n        filename: string containg the name of the file to read a flow\n    ---- OUTPUTS ----",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "MAX_HEIGHT",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "MAX_HEIGHT = 99999\ndef readFlowFile(filename):\n    \"\"\"\n    readFlowFile(<FILENAME>) reads a flow file <FILENAME> into a 2-band np.array.\n    if <FILENAME> does not exist, an IOError is raised.\n    if <FILENAME> does not finish by '.flo' or the tag, the width, the height or the file's size is illegal, an Expcetion is raised.\n    ---- PARAMETERS ----\n        filename: string containg the name of the file to read a flow\n    ---- OUTPUTS ----\n        a np.array of dimension (height x width x 2) containing the flow of type 'float32'",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "_read_flo_file",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "_read_flo_file = readFlowFile\ndef _read_kitti_flow(filename):\n    flow = cv2.imread(filename, cv2.IMREAD_ANYDEPTH | cv2.IMREAD_COLOR)\n    flow = flow[:, :, ::-1].astype(np.float32)\n    valid = flow[:, :, 2]>0\n    flow = flow[:, :, :2]\n    flow = (flow - 2 ** 15) / 64.0\n    flow[~valid,0] = np.inf\n    flow[~valid,1] = np.inf\n    return flow",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "_read_hd1k_flow",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "_read_hd1k_flow = _read_kitti_flow\ndef writeFlowKitti(filename, uv):\n    uv = 64.0 * uv + 2 ** 15\n    valid = np.ones([uv.shape[0], uv.shape[1], 1])\n    uv = np.concatenate([uv, valid], axis=-1).astype(np.uint16)\n    cv2.imwrite(filename, uv[..., ::-1])\ndef writeFlo5File(flow, filename):\n    with h5py.File(filename, \"w\") as f:\n        f.create_dataset(\"flow\", data=flow, compression=\"gzip\", compression_opts=5)\ndef _read_hdf5_flow(filename):",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "RY",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "RY = 15\nYG = 6\nGC = 4\nCB = 11\nBM = 13\nMR = 6\nUNKNOWN_THRESH = 1e9\ndef colorTest():\n    \"\"\"\n    flow_utils.colorTest(): display an example of image showing the color encoding scheme",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "YG",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "YG = 6\nGC = 4\nCB = 11\nBM = 13\nMR = 6\nUNKNOWN_THRESH = 1e9\ndef colorTest():\n    \"\"\"\n    flow_utils.colorTest(): display an example of image showing the color encoding scheme\n    \"\"\"",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "GC",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "GC = 4\nCB = 11\nBM = 13\nMR = 6\nUNKNOWN_THRESH = 1e9\ndef colorTest():\n    \"\"\"\n    flow_utils.colorTest(): display an example of image showing the color encoding scheme\n    \"\"\"\n    import matplotlib.pylab as plt",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "CB",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "CB = 11\nBM = 13\nMR = 6\nUNKNOWN_THRESH = 1e9\ndef colorTest():\n    \"\"\"\n    flow_utils.colorTest(): display an example of image showing the color encoding scheme\n    \"\"\"\n    import matplotlib.pylab as plt\n    truerange = 1",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "BM",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "BM = 13\nMR = 6\nUNKNOWN_THRESH = 1e9\ndef colorTest():\n    \"\"\"\n    flow_utils.colorTest(): display an example of image showing the color encoding scheme\n    \"\"\"\n    import matplotlib.pylab as plt\n    truerange = 1\n    h,w = 151,151",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "MR",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "MR = 6\nUNKNOWN_THRESH = 1e9\ndef colorTest():\n    \"\"\"\n    flow_utils.colorTest(): display an example of image showing the color encoding scheme\n    \"\"\"\n    import matplotlib.pylab as plt\n    truerange = 1\n    h,w = 151,151\n    trange = truerange*1.04",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "UNKNOWN_THRESH",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_flow",
        "description": "croco.stereoflow.datasets_flow",
        "peekOfCode": "UNKNOWN_THRESH = 1e9\ndef colorTest():\n    \"\"\"\n    flow_utils.colorTest(): display an example of image showing the color encoding scheme\n    \"\"\"\n    import matplotlib.pylab as plt\n    truerange = 1\n    h,w = 151,151\n    trange = truerange*1.04\n    s2 = round(h/2)",
        "detail": "croco.stereoflow.datasets_flow",
        "documentation": {}
    },
    {
        "label": "StereoDataset",
        "kind": 6,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "class StereoDataset(data.Dataset):\n    def __init__(self, split, augmentor=False, crop_size=None, totensor=True):\n        self.split = split\n        if not augmentor: assert crop_size is None \n        if crop_size: assert augmentor\n        self.crop_size = crop_size\n        self.augmentor_str = augmentor\n        self.augmentor = StereoAugmentor(crop_size) if augmentor else None\n        self.totensor = totensor\n        self.rmul = 1 # keep track of rmul",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "CREStereoDataset",
        "kind": 6,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "class CREStereoDataset(StereoDataset):\n    def _prepare_data(self):\n        self.name = 'CREStereo'\n        self._set_root()\n        assert self.split in ['train']\n        self.pairname_to_Limgname = lambda pairname: osp.join(self.root, pairname+'_left.jpg')\n        self.pairname_to_Rimgname = lambda pairname: osp.join(self.root, pairname+'_right.jpg')\n        self.pairname_to_Ldispname = lambda pairname: osp.join(self.root, pairname+'_left.disp.png')\n        self.pairname_to_str = lambda pairname: pairname\n        self.load_disparity = _read_crestereo_disp",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "SceneFlowDataset",
        "kind": 6,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "class SceneFlowDataset(StereoDataset):\n    def _prepare_data(self):\n        self.name = \"SceneFlow\"\n        self._set_root()\n        assert self.split in ['train_finalpass','train_cleanpass','train_allpass','test_finalpass','test_cleanpass','test_allpass','test1of100_cleanpass','test1of100_finalpass']\n        self.pairname_to_Limgname = lambda pairname: osp.join(self.root, pairname)\n        self.pairname_to_Rimgname = lambda pairname: osp.join(self.root, pairname).replace('/left/','/right/')\n        self.pairname_to_Ldispname = lambda pairname: osp.join(self.root, pairname).replace('/frames_finalpass/','/disparity/').replace('/frames_cleanpass/','/disparity/')[:-4]+'.pfm'\n        self.pairname_to_str = lambda pairname: pairname[:-4]\n        self.load_disparity = _read_sceneflow_disp",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "Md21Dataset",
        "kind": 6,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "class Md21Dataset(StereoDataset):\n    def _prepare_data(self):\n        self.name = \"Middlebury2021\"\n        self._set_root()\n        assert self.split in ['train','subtrain','subval']\n        self.pairname_to_Limgname = lambda pairname: osp.join(self.root, pairname)\n        self.pairname_to_Rimgname = lambda pairname: osp.join(self.root, pairname.replace('/im0','/im1'))\n        self.pairname_to_Ldispname = lambda pairname: osp.join(self.root, pairname.split('/')[0], 'disp0.pfm')\n        self.pairname_to_str = lambda pairname: pairname[:-4]\n        self.load_disparity = _read_middlebury_disp",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "Md14Dataset",
        "kind": 6,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "class Md14Dataset(StereoDataset):\n    def _prepare_data(self):\n        self.name = \"Middlebury2014\"\n        self._set_root()\n        assert self.split in ['train','subtrain','subval']\n        self.pairname_to_Limgname = lambda pairname: osp.join(self.root, osp.dirname(pairname), 'im0.png')\n        self.pairname_to_Rimgname = lambda pairname: osp.join(self.root, pairname)\n        self.pairname_to_Ldispname = lambda pairname: osp.join(self.root, osp.dirname(pairname), 'disp0.pfm')\n        self.pairname_to_str = lambda pairname: pairname[:-4]\n        self.load_disparity = _read_middlebury_disp",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "Md06Dataset",
        "kind": 6,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "class Md06Dataset(StereoDataset):\n    def _prepare_data(self):\n        self.name = \"Middlebury2006\"\n        self._set_root()\n        assert self.split in ['train','subtrain','subval']\n        self.pairname_to_Limgname = lambda pairname: osp.join(self.root, pairname)\n        self.pairname_to_Rimgname = lambda pairname: osp.join(self.root, osp.dirname(pairname), 'view5.png')\n        self.pairname_to_Ldispname = lambda pairname: osp.join(self.root, pairname.split('/')[0], 'disp1.png')\n        self.load_disparity = _read_middlebury20052006_disp\n        self.has_constant_resolution = False",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "Md05Dataset",
        "kind": 6,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "class Md05Dataset(StereoDataset):\n    def _prepare_data(self):\n        self.name = \"Middlebury2005\"\n        self._set_root()\n        assert self.split in ['train','subtrain','subval']\n        self.pairname_to_Limgname = lambda pairname: osp.join(self.root, pairname)\n        self.pairname_to_Rimgname = lambda pairname: osp.join(self.root, osp.dirname(pairname), 'view5.png')\n        self.pairname_to_Ldispname = lambda pairname: osp.join(self.root, pairname.split('/')[0], 'disp1.png')\n        self.pairname_to_str = lambda pairname: pairname[:-4]\n        self.load_disparity = _read_middlebury20052006_disp",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "MdEval3Dataset",
        "kind": 6,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "class MdEval3Dataset(StereoDataset):\n    def _prepare_data(self):\n        self.name = \"MiddleburyEval3\"\n        self._set_root()\n        assert self.split in [s+'_'+r for s in ['train','subtrain','subval','test','all'] for r in ['full','half','quarter']]\n        if self.split.endswith('_full'):\n            self.root = self.root.replace('/MiddEval3','/MiddEval3_F')\n        elif self.split.endswith('_half'):        \n            self.root = self.root.replace('/MiddEval3','/MiddEval3_H')\n        else:",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "ETH3DLowResDataset",
        "kind": 6,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "class ETH3DLowResDataset(StereoDataset):\n    def _prepare_data(self):\n        self.name = \"ETH3DLowRes\"\n        self._set_root()\n        assert self.split in ['train','test','subtrain','subval','all']\n        self.pairname_to_Limgname = lambda pairname: osp.join(self.root, pairname, 'im0.png')\n        self.pairname_to_Rimgname = lambda pairname: osp.join(self.root, pairname, 'im1.png')\n        self.pairname_to_Ldispname = None if self.split=='test' else lambda pairname: None if pairname.startswith('test/') else osp.join(self.root, pairname.replace('train/','train_gt/'), 'disp0GT.pfm')\n        self.pairname_to_str = lambda pairname: pairname\n        self.load_disparity = _read_eth3d_disp",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "BoosterDataset",
        "kind": 6,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "class BoosterDataset(StereoDataset):\n    def _prepare_data(self):\n        self.name = \"Booster\"\n        self._set_root()\n        assert self.split in ['train_balanced','test_balanced','subtrain_balanced','subval_balanced'] # we use only the balanced version\n        self.pairname_to_Limgname = lambda pairname: osp.join(self.root, pairname)\n        self.pairname_to_Rimgname = lambda pairname: osp.join(self.root, pairname).replace('/camera_00/','/camera_02/')\n        self.pairname_to_Ldispname = lambda pairname: osp.join(self.root, osp.dirname(pairname), '../disp_00.npy') # same images with different colors, same gt per sequence\n        self.pairname_to_str = lambda pairname: pairname[:-4].replace('/camera_00/','/')\n        self.load_disparity = _read_booster_disp",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "SpringDataset",
        "kind": 6,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "class SpringDataset(StereoDataset):\n    def _prepare_data(self):\n        self.name = \"Spring\"\n        self._set_root()\n        assert self.split in ['train', 'test', 'subtrain', 'subval']\n        self.pairname_to_Limgname = lambda pairname: osp.join(self.root, pairname+'.png')\n        self.pairname_to_Rimgname = lambda pairname: osp.join(self.root, pairname+'.png').replace('frame_right','<frame_right>').replace('frame_left','frame_right').replace('<frame_right>','frame_left')\n        self.pairname_to_Ldispname = lambda pairname: None if pairname.startswith('test') else osp.join(self.root, pairname+'.dsp5').replace('frame_left','disp1_left').replace('frame_right','disp1_right')\n        self.pairname_to_str = lambda pairname: pairname\n        self.load_disparity = _read_hdf5_disp        ",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "Kitti12Dataset",
        "kind": 6,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "class Kitti12Dataset(StereoDataset):\n    def _prepare_data(self):\n        self.name = \"Kitti12\"\n        self._set_root()\n        assert self.split in ['train','test']\n        self.pairname_to_Limgname = lambda pairname: osp.join(self.root, pairname+'_10.png')\n        self.pairname_to_Rimgname = lambda pairname: osp.join(self.root, pairname.replace('/colored_0/','/colored_1/')+'_10.png')\n        self.pairname_to_Ldispname = None if self.split=='test' else lambda pairname: osp.join(self.root, pairname.replace('/colored_0/','/disp_occ/')+'_10.png')\n        self.pairname_to_str = lambda pairname: pairname.replace('/colored_0/','/')\n        self.load_disparity = _read_kitti_disp",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "Kitti15Dataset",
        "kind": 6,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "class Kitti15Dataset(StereoDataset):\n    def _prepare_data(self):\n        self.name = \"Kitti15\"\n        self._set_root()\n        assert self.split in ['train','subtrain','subval','test']\n        self.pairname_to_Limgname = lambda pairname: osp.join(self.root, pairname+'_10.png')\n        self.pairname_to_Rimgname = lambda pairname: osp.join(self.root, pairname.replace('/image_2/','/image_3/')+'_10.png')\n        self.pairname_to_Ldispname = None if self.split=='test' else lambda pairname: osp.join(self.root, pairname.replace('/image_2/','/disp_occ_0/')+'_10.png')\n        self.pairname_to_str = lambda pairname: pairname.replace('/image_2/','/')\n        self.load_disparity = _read_kitti_disp",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "img_to_tensor",
        "kind": 2,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "def img_to_tensor(img):\n    img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.\n    img = (img-in1k_mean)/in1k_std\n    return img\ndef disp_to_tensor(disp):\n    return torch.from_numpy(disp)[None,:,:]\nclass StereoDataset(data.Dataset):\n    def __init__(self, split, augmentor=False, crop_size=None, totensor=True):\n        self.split = split\n        if not augmentor: assert crop_size is None ",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "disp_to_tensor",
        "kind": 2,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "def disp_to_tensor(disp):\n    return torch.from_numpy(disp)[None,:,:]\nclass StereoDataset(data.Dataset):\n    def __init__(self, split, augmentor=False, crop_size=None, totensor=True):\n        self.split = split\n        if not augmentor: assert crop_size is None \n        if crop_size: assert augmentor\n        self.crop_size = crop_size\n        self.augmentor_str = augmentor\n        self.augmentor = StereoAugmentor(crop_size) if augmentor else None",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "writePFM",
        "kind": 2,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "def writePFM(file, image, scale=1):\n    file = open(file, 'wb')\n    color = None\n    if image.dtype.name != 'float32':\n        raise Exception('Image dtype must be float32.')\n    image = np.flipud(image)\n    if len(image.shape) == 3 and image.shape[2] == 3:  # color image\n        color = True\n    elif len(image.shape) == 2 or len(image.shape) == 3 and image.shape[2] == 1:  # greyscale\n        color = False",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "writeDsp5File",
        "kind": 2,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "def writeDsp5File(disp, filename):\n    with h5py.File(filename, \"w\") as f:\n        f.create_dataset(\"disparity\", data=disp, compression=\"gzip\", compression_opts=5)\n# disp visualization\ndef vis_disparity(disp, m=None, M=None):\n    if m is None: m = disp.min()\n    if M is None: M = disp.max()\n    disp_vis = (disp - m) / (M-m) * 255.0\n    disp_vis = disp_vis.astype(\"uint8\")\n    disp_vis = cv2.applyColorMap(disp_vis, cv2.COLORMAP_INFERNO)",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "vis_disparity",
        "kind": 2,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "def vis_disparity(disp, m=None, M=None):\n    if m is None: m = disp.min()\n    if M is None: M = disp.max()\n    disp_vis = (disp - m) / (M-m) * 255.0\n    disp_vis = disp_vis.astype(\"uint8\")\n    disp_vis = cv2.applyColorMap(disp_vis, cv2.COLORMAP_INFERNO)\n    return disp_vis\n# dataset getter \ndef get_train_dataset_stereo(dataset_str, augmentor=True, crop_size=None):\n    dataset_str = dataset_str.replace('(','Dataset(')",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "get_train_dataset_stereo",
        "kind": 2,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "def get_train_dataset_stereo(dataset_str, augmentor=True, crop_size=None):\n    dataset_str = dataset_str.replace('(','Dataset(')\n    if augmentor:\n        dataset_str = dataset_str.replace(')',', augmentor=True)')\n    if crop_size is not None:\n        dataset_str = dataset_str.replace(')',', crop_size={:s})'.format(str(crop_size)))\n    return eval(dataset_str)\ndef get_test_datasets_stereo(dataset_str):\n    dataset_str = dataset_str.replace('(','Dataset(')\n    return [eval(s) for s in dataset_str.split('+')]",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "get_test_datasets_stereo",
        "kind": 2,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "def get_test_datasets_stereo(dataset_str):\n    dataset_str = dataset_str.replace('(','Dataset(')\n    return [eval(s) for s in dataset_str.split('+')]",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "dataset_to_root",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "dataset_to_root = {\n    'CREStereo': './data/stereoflow//crenet_stereo_trainset/stereo_trainset/crestereo/',\n    'SceneFlow': './data/stereoflow//SceneFlow/',\n    'ETH3DLowRes': './data/stereoflow/eth3d_lowres/',\n    'Booster': './data/stereoflow/booster_gt/',\n    'Middlebury2021': './data/stereoflow/middlebury/2021/data/',\n    'Middlebury2014': './data/stereoflow/middlebury/2014/',\n    'Middlebury2006': './data/stereoflow/middlebury/2006/',\n    'Middlebury2005': './data/stereoflow/middlebury/2005/train/',\n    'MiddleburyEval3':  './data/stereoflow/middlebury/MiddEval3/',",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "cache_dir",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "cache_dir = \"./data/stereoflow/datasets_stereo_cache/\"\nin1k_mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\nin1k_std =  torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\ndef img_to_tensor(img):\n    img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.\n    img = (img-in1k_mean)/in1k_std\n    return img\ndef disp_to_tensor(disp):\n    return torch.from_numpy(disp)[None,:,:]\nclass StereoDataset(data.Dataset):",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "in1k_mean",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "in1k_mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\nin1k_std =  torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\ndef img_to_tensor(img):\n    img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.\n    img = (img-in1k_mean)/in1k_std\n    return img\ndef disp_to_tensor(disp):\n    return torch.from_numpy(disp)[None,:,:]\nclass StereoDataset(data.Dataset):\n    def __init__(self, split, augmentor=False, crop_size=None, totensor=True):",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "in1k_std",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "in1k_std =  torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\ndef img_to_tensor(img):\n    img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.\n    img = (img-in1k_mean)/in1k_std\n    return img\ndef disp_to_tensor(disp):\n    return torch.from_numpy(disp)[None,:,:]\nclass StereoDataset(data.Dataset):\n    def __init__(self, split, augmentor=False, crop_size=None, totensor=True):\n        self.split = split",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "_read_sceneflow_disp",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "_read_sceneflow_disp = _read_pfm_disp\n_read_eth3d_disp = _read_pfm_disp\n_read_middlebury_disp = _read_pfm_disp\n_read_carla_disp = _read_pfm_disp\n_read_tartanair_disp = _read_npy_disp\ndef _read_hdf5_disp(filename):\n    disp = np.asarray(h5py.File(filename)['disparity'])\n    disp[np.isnan(disp)] = np.inf # make invalid values as +inf\n    #disp[disp==0.0] = np.inf # make invalid values as +inf\n    return disp.astype(np.float32)",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "_read_eth3d_disp",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "_read_eth3d_disp = _read_pfm_disp\n_read_middlebury_disp = _read_pfm_disp\n_read_carla_disp = _read_pfm_disp\n_read_tartanair_disp = _read_npy_disp\ndef _read_hdf5_disp(filename):\n    disp = np.asarray(h5py.File(filename)['disparity'])\n    disp[np.isnan(disp)] = np.inf # make invalid values as +inf\n    #disp[disp==0.0] = np.inf # make invalid values as +inf\n    return disp.astype(np.float32)\nimport re",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "_read_middlebury_disp",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "_read_middlebury_disp = _read_pfm_disp\n_read_carla_disp = _read_pfm_disp\n_read_tartanair_disp = _read_npy_disp\ndef _read_hdf5_disp(filename):\n    disp = np.asarray(h5py.File(filename)['disparity'])\n    disp[np.isnan(disp)] = np.inf # make invalid values as +inf\n    #disp[disp==0.0] = np.inf # make invalid values as +inf\n    return disp.astype(np.float32)\nimport re\ndef _read_pfm(file):",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "_read_carla_disp",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "_read_carla_disp = _read_pfm_disp\n_read_tartanair_disp = _read_npy_disp\ndef _read_hdf5_disp(filename):\n    disp = np.asarray(h5py.File(filename)['disparity'])\n    disp[np.isnan(disp)] = np.inf # make invalid values as +inf\n    #disp[disp==0.0] = np.inf # make invalid values as +inf\n    return disp.astype(np.float32)\nimport re\ndef _read_pfm(file):\n    file = open(file, 'rb')",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "_read_tartanair_disp",
        "kind": 5,
        "importPath": "croco.stereoflow.datasets_stereo",
        "description": "croco.stereoflow.datasets_stereo",
        "peekOfCode": "_read_tartanair_disp = _read_npy_disp\ndef _read_hdf5_disp(filename):\n    disp = np.asarray(h5py.File(filename)['disparity'])\n    disp[np.isnan(disp)] = np.inf # make invalid values as +inf\n    #disp[disp==0.0] = np.inf # make invalid values as +inf\n    return disp.astype(np.float32)\nimport re\ndef _read_pfm(file):\n    file = open(file, 'rb')\n    color = None",
        "detail": "croco.stereoflow.datasets_stereo",
        "documentation": {}
    },
    {
        "label": "split_prediction_conf",
        "kind": 2,
        "importPath": "croco.stereoflow.engine",
        "description": "croco.stereoflow.engine",
        "peekOfCode": "def split_prediction_conf(predictions, with_conf=False):\n    if not with_conf:\n        return predictions, None\n    conf = predictions[:,-1:,:,:]\n    predictions = predictions[:,:-1,:,:]\n    return predictions, conf\ndef train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module, metrics: torch.nn.Module,\n                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n                    device: torch.device, epoch: int, loss_scaler,\n                    log_writer=None, print_freq = 20,",
        "detail": "croco.stereoflow.engine",
        "documentation": {}
    },
    {
        "label": "train_one_epoch",
        "kind": 2,
        "importPath": "croco.stereoflow.engine",
        "description": "croco.stereoflow.engine",
        "peekOfCode": "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module, metrics: torch.nn.Module,\n                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n                    device: torch.device, epoch: int, loss_scaler,\n                    log_writer=None, print_freq = 20,\n                    args=None):\n    model.train(True)\n    metric_logger = misc.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('lr', misc.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    header = 'Epoch: [{}]'.format(epoch)\n    accum_iter = args.accum_iter",
        "detail": "croco.stereoflow.engine",
        "documentation": {}
    },
    {
        "label": "validate_one_epoch",
        "kind": 2,
        "importPath": "croco.stereoflow.engine",
        "description": "croco.stereoflow.engine",
        "peekOfCode": "def validate_one_epoch(model: torch.nn.Module,\n                   criterion: torch.nn.Module,\n                   metrics: torch.nn.Module,\n                   data_loaders: list[Iterable],\n                   device: torch.device,\n                   epoch: int,\n                   log_writer=None,\n                   args=None):\n    model.eval()\n    metric_loggers = []",
        "detail": "croco.stereoflow.engine",
        "documentation": {}
    },
    {
        "label": "tiled_pred",
        "kind": 2,
        "importPath": "croco.stereoflow.engine",
        "description": "croco.stereoflow.engine",
        "peekOfCode": "def tiled_pred(model, criterion, img1, img2, gt,\n               overlap=0.5, bad_crop_thr=0.05,\n               downscale=False, crop=512, ret='loss',\n               conf_mode='conf_expsigmoid_10_5', with_conf=False, \n               return_time=False):\n    # for each image, we are going to run inference on many overlapping patches\n    # then, all predictions will be weighted-averaged\n    if gt is not None:\n        B, C, H, W = gt.shape\n    else:",
        "detail": "croco.stereoflow.engine",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "croco.stereoflow.test",
        "description": "croco.stereoflow.test",
        "peekOfCode": "def get_args_parser():\n    parser = argparse.ArgumentParser('Test CroCo models on stereo/flow', add_help=False)\n    # important argument \n    parser.add_argument('--model', required=True, type=str, help='Path to the model to evaluate')\n    parser.add_argument('--dataset', required=True, type=str, help=\"test dataset (there can be multiple dataset separated by a +)\")\n    # tiling \n    parser.add_argument('--tile_conf_mode', type=str, default='', help='Weights for the tiling aggregation based on confidence (empty means use the formula from the loaded checkpoint')\n    parser.add_argument('--tile_overlap', type=float, default=0.7, help='overlap between tiles')\n    # save (it will automatically go to <model_path>_<dataset_str>/<tile_str>_<save>)\n    parser.add_argument('--save', type=str, nargs='+', default=[], ",
        "detail": "croco.stereoflow.test",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "croco.stereoflow.test",
        "description": "croco.stereoflow.test",
        "peekOfCode": "def main(args):\n    # load the pretrained model and metrics\n    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n    model, metrics, cropsize, with_conf, task, tile_conf_mode = _load_model_and_criterion(args.model, 'metrics' in args.save, device)\n    if args.tile_conf_mode=='': args.tile_conf_mode = tile_conf_mode\n    # load the datasets \n    datasets = (get_test_datasets_stereo if task=='stereo' else get_test_datasets_flow)(args.dataset)\n    dataloaders = [DataLoader(dataset, batch_size=1, shuffle=False, num_workers=args.num_workers, pin_memory=True, drop_last=False) for dataset in datasets]    \n    # run\n    for i,dataloader in enumerate(dataloaders):",
        "detail": "croco.stereoflow.test",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "croco.stereoflow.train",
        "description": "croco.stereoflow.train",
        "peekOfCode": "def get_args_parser():\n    # prepare subparsers \n    parser = argparse.ArgumentParser('Finetuning CroCo models on stereo or flow', add_help=False)\n    subparsers = parser.add_subparsers(title=\"Task (stereo or flow)\", dest=\"task\", required=True)\n    parser_stereo = subparsers.add_parser('stereo', help='Training stereo model')\n    parser_flow = subparsers.add_parser('flow', help='Training flow model')\n    def add_arg(name_or_flags, default=None, default_stereo=None, default_flow=None, **kwargs):\n        if default is not None: assert default_stereo is None and default_flow is None, \"setting default makes default_stereo and default_flow disabled\"\n        parser_stereo.add_argument(name_or_flags, default=default if default is not None else default_stereo, **kwargs)\n        parser_flow.add_argument(name_or_flags, default=default if default is not None else default_flow, **kwargs)",
        "detail": "croco.stereoflow.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "croco.stereoflow.train",
        "description": "croco.stereoflow.train",
        "peekOfCode": "def main(args):\n    misc.init_distributed_mode(args)\n    global_rank = misc.get_rank()\n    num_tasks = misc.get_world_size()\n    assert os.path.isfile(args.pretrained)\n    print(\"output_dir: \"+args.output_dir)\n    os.makedirs(args.output_dir, exist_ok=True)\n    # fix the seed for reproducibility\n    seed = args.seed + misc.get_rank()\n    torch.manual_seed(seed)",
        "detail": "croco.stereoflow.train",
        "documentation": {}
    },
    {
        "label": "SmoothedValue",
        "kind": 6,
        "importPath": "croco.utils.misc",
        "description": "croco.utils.misc",
        "peekOfCode": "class SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0",
        "detail": "croco.utils.misc",
        "documentation": {}
    },
    {
        "label": "MetricLogger",
        "kind": 6,
        "importPath": "croco.utils.misc",
        "description": "croco.utils.misc",
        "peekOfCode": "class MetricLogger(object):\n    def __init__(self, delimiter=\"\\t\"):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if v is None:\n                continue\n            if isinstance(v, torch.Tensor):\n                v = v.item()",
        "detail": "croco.utils.misc",
        "documentation": {}
    },
    {
        "label": "NativeScalerWithGradNormCount",
        "kind": 6,
        "importPath": "croco.utils.misc",
        "description": "croco.utils.misc",
        "peekOfCode": "class NativeScalerWithGradNormCount:\n    state_dict_key = \"amp_scaler\"\n    def __init__(self, enabled=True):\n        self._scaler = torch.cuda.amp.GradScaler(enabled=enabled)\n    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True, model = None): # loss_scaler(loss, optimizer, parameters=model.parameters(), update_grad=(data_iter_step + 1) % accum_iter == 0)\n        # print('scale', self._scaler.get_scale())\n        self._scaler.scale(loss).backward(create_graph=create_graph)\n        # output all names for parameters and its norm:\n        if update_grad:\n            if clip_grad is not None:",
        "detail": "croco.utils.misc",
        "documentation": {}
    },
    {
        "label": "setup_for_distributed",
        "kind": 2,
        "importPath": "croco.utils.misc",
        "description": "croco.utils.misc",
        "peekOfCode": "def setup_for_distributed(is_master):\n    \"\"\"\n    This function disables printing when not in master process\n    \"\"\"\n    builtin_print = builtins.print\n    def print(*args, **kwargs):\n        force = kwargs.pop('force', False)\n        force = force or (get_world_size() > 8)\n        if is_master or force:\n            now = datetime.datetime.now().time()",
        "detail": "croco.utils.misc",
        "documentation": {}
    },
    {
        "label": "is_dist_avail_and_initialized",
        "kind": 2,
        "importPath": "croco.utils.misc",
        "description": "croco.utils.misc",
        "peekOfCode": "def is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\ndef get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()",
        "detail": "croco.utils.misc",
        "documentation": {}
    },
    {
        "label": "get_world_size",
        "kind": 2,
        "importPath": "croco.utils.misc",
        "description": "croco.utils.misc",
        "peekOfCode": "def get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()\ndef get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\ndef is_main_process():\n    return get_rank() == 0",
        "detail": "croco.utils.misc",
        "documentation": {}
    },
    {
        "label": "get_rank",
        "kind": 2,
        "importPath": "croco.utils.misc",
        "description": "croco.utils.misc",
        "peekOfCode": "def get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\ndef is_main_process():\n    return get_rank() == 0\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\ndef init_distributed_mode(args):",
        "detail": "croco.utils.misc",
        "documentation": {}
    },
    {
        "label": "is_main_process",
        "kind": 2,
        "importPath": "croco.utils.misc",
        "description": "croco.utils.misc",
        "peekOfCode": "def is_main_process():\n    return get_rank() == 0\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\ndef init_distributed_mode(args):\n    nodist = args.nodist if hasattr(args,'nodist') else False \n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ and not nodist:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ['WORLD_SIZE'])",
        "detail": "croco.utils.misc",
        "documentation": {}
    },
    {
        "label": "save_on_master",
        "kind": 2,
        "importPath": "croco.utils.misc",
        "description": "croco.utils.misc",
        "peekOfCode": "def save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\ndef init_distributed_mode(args):\n    nodist = args.nodist if hasattr(args,'nodist') else False \n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ and not nodist:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ['WORLD_SIZE'])\n        args.gpu = int(os.environ['LOCAL_RANK'])\n    else:",
        "detail": "croco.utils.misc",
        "documentation": {}
    },
    {
        "label": "init_distributed_mode",
        "kind": 2,
        "importPath": "croco.utils.misc",
        "description": "croco.utils.misc",
        "peekOfCode": "def init_distributed_mode(args):\n    nodist = args.nodist if hasattr(args,'nodist') else False \n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ and not nodist:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ['WORLD_SIZE'])\n        args.gpu = int(os.environ['LOCAL_RANK'])\n    else:\n        print('Not using distributed mode')\n        setup_for_distributed(is_master=True)  # hack\n        args.distributed = False",
        "detail": "croco.utils.misc",
        "documentation": {}
    },
    {
        "label": "get_grad_norm_",
        "kind": 2,
        "importPath": "croco.utils.misc",
        "description": "croco.utils.misc",
        "peekOfCode": "def get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    parameters = [p for p in parameters if p.grad is not None]\n    norm_type = float(norm_type)\n    if len(parameters) == 0:\n        return torch.tensor(0.)\n    device = parameters[0].grad.device\n    if norm_type == inf:\n        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)",
        "detail": "croco.utils.misc",
        "documentation": {}
    },
    {
        "label": "save_model",
        "kind": 2,
        "importPath": "croco.utils.misc",
        "description": "croco.utils.misc",
        "peekOfCode": "def save_model(args, epoch, model_without_ddp, optimizer, loss_scaler, fname=None, best_so_far=None):\n    if fname is None: fname = str(epoch)\n    checkpoint_path = os.path.join(args.output_dir, ('checkpoint-%s.pth' % fname))\n    to_save = {\n        'model': model_without_ddp.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        'scaler': loss_scaler.state_dict(),\n        'args': args,\n        'epoch': epoch,\n    }",
        "detail": "croco.utils.misc",
        "documentation": {}
    },
    {
        "label": "load_model",
        "kind": 2,
        "importPath": "croco.utils.misc",
        "description": "croco.utils.misc",
        "peekOfCode": "def load_model(args, model_without_ddp, optimizer, loss_scaler):\n    args.start_epoch = 0\n    best_so_far = None\n    if args.resume is not None:\n        if args.resume.startswith('https'):\n            checkpoint = torch.hub.load_state_dict_from_url(\n                args.resume, map_location='cpu', check_hash=True)\n        else:\n            checkpoint = torch.load(args.resume, map_location='cpu')\n        print(\"Resume checkpoint %s\" % args.resume)",
        "detail": "croco.utils.misc",
        "documentation": {}
    },
    {
        "label": "all_reduce_mean",
        "kind": 2,
        "importPath": "croco.utils.misc",
        "description": "croco.utils.misc",
        "peekOfCode": "def all_reduce_mean(x):\n    world_size = get_world_size()\n    if world_size > 1:\n        x_reduce = torch.tensor(x).cuda()\n        dist.all_reduce(x_reduce)\n        x_reduce /= world_size\n        return x_reduce.item()\n    else:\n        return x\ndef _replace(text, src, tgt, rm=''):",
        "detail": "croco.utils.misc",
        "documentation": {}
    },
    {
        "label": "filename",
        "kind": 2,
        "importPath": "croco.utils.misc",
        "description": "croco.utils.misc",
        "peekOfCode": "def filename( obj ):\n    \"\"\" transform a python obj or cmd into a proper filename. \n     - \\1 gets replaced by slash '/'\n     - \\2 gets replaced by comma ','\n    \"\"\"\n    if not isinstance(obj, str): \n        obj = repr(obj)\n    obj = str(obj).replace('()','')\n    obj = _replace(obj, '_,(*/\\1\\2','-__x%/,', rm=' )\\'\"')\n    assert all(len(s) < 256 for s in obj.split(os.sep)), 'filename too long (>256 characters):\\n'+obj",
        "detail": "croco.utils.misc",
        "documentation": {}
    },
    {
        "label": "get_parameter_groups",
        "kind": 2,
        "importPath": "croco.utils.misc",
        "description": "croco.utils.misc",
        "peekOfCode": "def get_parameter_groups(model, weight_decay, layer_decay=1.0, skip_list=(), no_lr_scale_list=[]):\n    parameter_group_names = {}\n    parameter_group_vars = {}\n    enc_depth, dec_depth = None, None\n    # prepare layer decay values \n    assert layer_decay==1.0 or 0.<layer_decay<1.\n    if layer_decay<1.:\n        enc_depth = model.enc_depth\n        dec_depth = model.dec_depth if hasattr(model, 'dec_blocks') else 0\n        num_layers = enc_depth+dec_depth",
        "detail": "croco.utils.misc",
        "documentation": {}
    },
    {
        "label": "adjust_learning_rate",
        "kind": 2,
        "importPath": "croco.utils.misc",
        "description": "croco.utils.misc",
        "peekOfCode": "def adjust_learning_rate(optimizer, epoch, args):\n    \"\"\"Decay the learning rate with half-cycle cosine after warmup\"\"\"\n    if epoch < args.warmup_epochs:\n        lr = args.lr * epoch / args.warmup_epochs \n    else:\n        lr = args.min_lr + (args.lr - args.min_lr) * 0.5 * \\\n            (1. + math.cos(math.pi * (epoch - args.warmup_epochs) / (args.epochs - args.warmup_epochs)))\n    for param_group in optimizer.param_groups:\n        if \"lr_scale\" in param_group:\n            param_group[\"lr\"] = lr * param_group[\"lr_scale\"]",
        "detail": "croco.utils.misc",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "croco.demo",
        "description": "croco.demo",
        "peekOfCode": "def main():\n    device = torch.device('cuda:0' if torch.cuda.is_available() and torch.cuda.device_count()>0 else 'cpu')\n    # load 224x224 images and transform them to tensor \n    imagenet_mean = [0.485, 0.456, 0.406]\n    imagenet_mean_tensor = torch.tensor(imagenet_mean).view(1,3,1,1).to(device, non_blocking=True)\n    imagenet_std = [0.229, 0.224, 0.225]\n    imagenet_std_tensor = torch.tensor(imagenet_std).view(1,3,1,1).to(device, non_blocking=True)\n    trfs = Compose([ToTensor(), Normalize(mean=imagenet_mean, std=imagenet_std)])\n    image1 = trfs(Image.open('assets/Chateau1.png').convert('RGB')).to(device, non_blocking=True).unsqueeze(0)\n    image2 = trfs(Image.open('assets/Chateau2.png').convert('RGB')).to(device, non_blocking=True).unsqueeze(0)",
        "detail": "croco.demo",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "croco.pretrain",
        "description": "croco.pretrain",
        "peekOfCode": "def get_args_parser():\n    parser = argparse.ArgumentParser('CroCo pre-training', add_help=False)\n    # model and criterion\n    parser.add_argument('--model', default='CroCoNet()', type=str, help=\"string containing the model to build\")\n    parser.add_argument('--norm_pix_loss', default=1, choices=[0,1], help=\"apply per-patch mean/std normalization before applying the loss\")\n    # dataset \n    parser.add_argument('--dataset', default='habitat_release', type=str, help=\"training set\")\n    parser.add_argument('--transforms', default='crop224+acolor', type=str, help=\"transforms to apply\") # in the paper, we also use some homography and rotation, but find later that they were not useful or even harmful\n    # training \n    parser.add_argument('--seed', default=0, type=int, help=\"Random seed\")",
        "detail": "croco.pretrain",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "croco.pretrain",
        "description": "croco.pretrain",
        "peekOfCode": "def main(args):\n    misc.init_distributed_mode(args)\n    global_rank = misc.get_rank()\n    world_size = misc.get_world_size()\n    print(\"output_dir: \"+args.output_dir)\n    if args.output_dir:\n        Path(args.output_dir).mkdir(parents=True, exist_ok=True)                         \n    # auto resume \n    last_ckpt_fname = os.path.join(args.output_dir, f'checkpoint-last.pth')\n    args.resume = last_ckpt_fname if os.path.isfile(last_ckpt_fname) else None",
        "detail": "croco.pretrain",
        "documentation": {}
    },
    {
        "label": "train_one_epoch",
        "kind": 2,
        "importPath": "croco.pretrain",
        "description": "croco.pretrain",
        "peekOfCode": "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n                    device: torch.device, epoch: int, loss_scaler,\n                    log_writer=None,\n                    args=None):\n    model.train(True)\n    metric_logger = misc.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('lr', misc.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    header = 'Epoch: [{}]'.format(epoch)\n    accum_iter = args.accum_iter",
        "detail": "croco.pretrain",
        "documentation": {}
    },
    {
        "label": "extract_scene_name",
        "kind": 2,
        "importPath": "datasets_preprocess.gather_metadata",
        "description": "datasets_preprocess.gather_metadata",
        "peekOfCode": "def extract_scene_name(x, data_name_):\n    if \"_meta\" == data_name_[-5:]:\n        data_name = data_name_[:-5]\n    else:\n        data_name = data_name_\n    # print('extract', x, data_name)\n    if \"scannetpp\" in x:\n        return 'scannetpp'\n    elif \"scannet\" in x:\n        xx = x.split('/')",
        "detail": "datasets_preprocess.gather_metadata",
        "documentation": {}
    },
    {
        "label": "split_dps",
        "kind": 2,
        "importPath": "datasets_preprocess.gather_metadata",
        "description": "datasets_preprocess.gather_metadata",
        "peekOfCode": "def split_dps(x, dir_name):\n    # makedirs\n    if not os.path.exists(dir_name):\n        os.makedirs(dir_name)\n    data_train = []\n    data_test = []\n    for (d_id, d) in enumerate(x):\n        rgb_path = d['rgb_list'][0]\n        data_name = extract_scene_name(rgb_path, args.data_name)\n        if data_name in train_name_list:",
        "detail": "datasets_preprocess.gather_metadata",
        "documentation": {}
    },
    {
        "label": "tuple_n_general_new",
        "kind": 2,
        "importPath": "datasets_preprocess.gather_metadata",
        "description": "datasets_preprocess.gather_metadata",
        "peekOfCode": "def tuple_n_general_new(dir_name, tgt_name):\n    if os.path.exists(f\"{dir_name}/dps.h5\"):\n        x = h5py.File(f\"{dir_name}/dps.h5\", 'r')\n        json_strs = x['json_strs']\n        for x in json_strs[::1000]:\n            print(json.loads(x)['rgb_list'])\n        input()\n        x = [json.loads(x) for x in json_strs]\n        split_dps(x, dir_name)\n        return",
        "detail": "datasets_preprocess.gather_metadata",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "datasets_preprocess.gather_metadata",
        "description": "datasets_preprocess.gather_metadata",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument(\"--data-name\")\nparser.add_argument(\"--data-dir\")\nparser.add_argument(\"--tgt-dir\")\n# parser.add_argument(\"--node-no\", default=0, type=int)\nargs = parser.parse_args()\nif args.data_dir is None:\n    args.data_dir = f\"/home/zgtang/{args.data_name}\"\nelse:\n    args.data_name = os.path.basename(args.data_dir)",
        "detail": "datasets_preprocess.gather_metadata",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "datasets_preprocess.gather_metadata",
        "description": "datasets_preprocess.gather_metadata",
        "peekOfCode": "args = parser.parse_args()\nif args.data_dir is None:\n    args.data_dir = f\"/home/zgtang/{args.data_name}\"\nelse:\n    args.data_name = os.path.basename(args.data_dir)\nif args.tgt_dir is None:\n    args.tgt_dir = args.data_dir\ndef extract_scene_name(x, data_name_):\n    if \"_meta\" == data_name_[-5:]:\n        data_name = data_name_[:-5]",
        "detail": "datasets_preprocess.gather_metadata",
        "documentation": {}
    },
    {
        "label": "train_name_list",
        "kind": 5,
        "importPath": "datasets_preprocess.gather_metadata",
        "description": "datasets_preprocess.gather_metadata",
        "peekOfCode": "train_name_list = json.load(open(\"./data/train_name_list.json\", 'r'))\ndef split_dps(x, dir_name):\n    # makedirs\n    if not os.path.exists(dir_name):\n        os.makedirs(dir_name)\n    data_train = []\n    data_test = []\n    for (d_id, d) in enumerate(x):\n        rgb_path = d['rgb_list'][0]\n        data_name = extract_scene_name(rgb_path, args.data_name)",
        "detail": "datasets_preprocess.gather_metadata",
        "documentation": {}
    },
    {
        "label": "init_distributed",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "def init_distributed():\n    if not dist.is_initialized():\n        dist.init_process_group(backend='gloo', init_method='env://')\ndef get_rank():\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()\ninit_distributed()\nrank = get_rank()\ncuda_id = int(rank) % 8",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "get_rank",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "def get_rank():\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()\ninit_distributed()\nrank = get_rank()\ncuda_id = int(rank) % 8\ndevice = f\"cuda:{cuda_id}\"\nprint('cuda id', cuda_id)\ndef ok(score_list, mi, ma):",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "ok",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "def ok(score_list, mi, ma):\n    ma_id = np.argmax(score_list)\n    if score_list[ma_id] < mi:\n        return False\n    if score_list[ma_id] > ma:\n        return False\n    score_list_except = np.delete(score_list, ma_id)\n    if len(score_list_except) == 0:\n        return True\n    if np.max(score_list_except) > mi:",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "compare",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "def compare(a, b):\n    a = int(os.path.basename(a)[:-4])\n    b = int(os.path.basename(b)[:-4])\n    return a < b\ndef key(a):\n    return int(os.path.basename(a)[:-4])\ndef min_dis(A, B):\n    A = torch.from_numpy(A).reshape(-1, 3).to(device)\n    B = torch.from_numpy(B).reshape(-1, 3).to(device)\n    from pytorch3d.ops import knn_points",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "key",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "def key(a):\n    return int(os.path.basename(a)[:-4])\ndef min_dis(A, B):\n    A = torch.from_numpy(A).reshape(-1, 3).to(device)\n    B = torch.from_numpy(B).reshape(-1, 3).to(device)\n    from pytorch3d.ops import knn_points\n    dis, _, _ = knn_points(B[None], A[None]) # B querying in A, dis: [1, B.shape[0], 1]\n    return dis[0,:,0]\ndef cover(pc1_, pc2_): # querying pc2 in pc1\n    import numpy as np",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "min_dis",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "def min_dis(A, B):\n    A = torch.from_numpy(A).reshape(-1, 3).to(device)\n    B = torch.from_numpy(B).reshape(-1, 3).to(device)\n    from pytorch3d.ops import knn_points\n    dis, _, _ = knn_points(B[None], A[None]) # B querying in A, dis: [1, B.shape[0], 1]\n    return dis[0,:,0]\ndef cover(pc1_, pc2_): # querying pc2 in pc1\n    import numpy as np\n    pc1 = pc1_.reshape(-1, 3)\n    pc2 = pc2_.reshape(-1, 3)",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "cover",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "def cover(pc1_, pc2_): # querying pc2 in pc1\n    import numpy as np\n    pc1 = pc1_.reshape(-1, 3)\n    pc2 = pc2_.reshape(-1, 3)\n    distances = min_dis(pc1, pc2)\n    thres = 0.015 * nn_shrink_rate\n    return distances[(distances > 0) * (distances < thres)].shape[0] / distances.shape[0]\ndef get_score(pc1, pc2):\n    return (cover(pc1, pc2) + cover(pc2, pc1)) / 2\ndef extract_valid_frames(valid_frames_ss):",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "get_score",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "def get_score(pc1, pc2):\n    return (cover(pc1, pc2) + cover(pc2, pc1)) / 2\ndef extract_valid_frames(valid_frames_ss):\n    valid_frames_name = []\n    for x in valid_frames_ss:\n        x = x.split(' ')\n        if len(x) <= 1:\n            break\n        if int(x[2]) != 0:\n            print('get bad frame', x)",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "extract_valid_frames",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "def extract_valid_frames(valid_frames_ss):\n    valid_frames_name = []\n    for x in valid_frames_ss:\n        x = x.split(' ')\n        if len(x) <= 1:\n            break\n        if int(x[2]) != 0:\n            print('get bad frame', x)\n            # exit(0)\n        valid_frames_name.append(int(x[1]))",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "qvec2rotmat",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "def qvec2rotmat(qvec):\n    return np.array([\n        [1 - 2 * qvec[2]**2 - 2 * qvec[3]**2,\n         2 * qvec[1] * qvec[2] - 2 * qvec[0] * qvec[3],\n         2 * qvec[3] * qvec[1] + 2 * qvec[0] * qvec[2]],\n        [2 * qvec[1] * qvec[2] + 2 * qvec[0] * qvec[3],\n         1 - 2 * qvec[1]**2 - 2 * qvec[3]**2,\n         2 * qvec[2] * qvec[3] - 2 * qvec[0] * qvec[1]],\n        [2 * qvec[3] * qvec[1] - 2 * qvec[0] * qvec[2],\n         2 * qvec[2] * qvec[3] + 2 * qvec[0] * qvec[1],",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "qt2w2c",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "def qt2w2c(q, t) -> np.ndarray:\n    R = qvec2rotmat(q)\n    world2cam = np.eye(4)\n    world2cam[:3, :3] = R\n    world2cam[:3, 3] = t\n    return world2cam\ndef extract_image_txt(lines):\n    c2ws = []\n    frame_names = []\n    for x in lines:",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "extract_image_txt",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "def extract_image_txt(lines):\n    c2ws = []\n    frame_names = []\n    for x in lines:\n        if \".jpg\" in x:\n            x = x.split(' ')\n            q = [float(y) for y in x[1:5]]\n            t = [float(y) for y in x[5:8]]\n            frame = int(x[9].replace('frame_', '').replace('.jpg', ''))\n            c2ws.append(np.linalg.inv(qt2w2c(q, t)))",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "extract_K",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "def extract_K(lines):\n    ss = lines[3].split(' ')\n    K = np.eye(3)\n    K[0,0] = float(ss[4])\n    K[1,1] = float(ss[5])\n    K[0,2] = float(ss[6])\n    K[1,2] = float(ss[7])\n    return K\ndef get_scene_list():\n    if data_type == \"scannet\":",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "get_scene_list",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "def get_scene_list():\n    if data_type == \"scannet\":\n        scan_list = glob.glob(\"./data/scannet/scans/*\")\n        scan_list.sort()\n    elif data_type == \"scannetpp\":\n        scan_list = np.load(g_pathmgr.get_local_path(global_variable.scannetpp_scan_list))\n        scan_list = [os.path.join(global_variable.scannetpp_data_dir + \"/data\", x.split('/')[-1]) for x in scan_list]\n    return scan_list\ndef filter_test(scene_list):\n    scene_list_new = []",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "filter_test",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "def filter_test(scene_list):\n    scene_list_new = []\n    for x in scene_list:\n        scene_name = os.path.basename(x)\n        if scene_name not in train_name_list:\n            scene_list_new.append(x)\n    return scene_list_new\ndef main():\n    scene_list = get_scene_list()\n    if args.split == \"test\":",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "def main():\n    scene_list = get_scene_list()\n    if args.split == \"test\":\n        scene_list = filter_test(scene_list)\n    # exit(0)\n    tuple_done = 0\n    for scene_id, scene_name in enumerate(scene_list[::-1]):\n        if scene_id % args.div != cuda_id + args.node_no * 8:\n            continue\n        print('trying', scene_id, cuda_id, cuda_id + args.node_no * 8, scene_name)",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument(\"--div\", default=1, type=int)\nparser.add_argument(\"--node-no\", default=0, type=int)\nparser.add_argument(\"--hardness\", type=str)\nparser.add_argument(\"--n-v\", type = int)\nparser.add_argument(\"--n-render\", type = int)\nparser.add_argument(\"--data-type\", type = str)\nparser.add_argument(\"--split\", type = str, default = \"all\")\nparser.add_argument(\"--render-overlap\", type = float, default = 0.95)\nparser.add_argument(\"--n-tuple-per-scene\", type = int, default = 1000)",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "args = parser.parse_args()\nprint('args', args)\nnode_no = args.node_no\nimport torch.distributed as dist\ndef init_distributed():\n    if not dist.is_initialized():\n        dist.init_process_group(backend='gloo', init_method='env://')\ndef get_rank():\n    if not dist.is_initialized():\n        return 0",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "node_no",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "node_no = args.node_no\nimport torch.distributed as dist\ndef init_distributed():\n    if not dist.is_initialized():\n        dist.init_process_group(backend='gloo', init_method='env://')\ndef get_rank():\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()\ninit_distributed()",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "rank",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "rank = get_rank()\ncuda_id = int(rank) % 8\ndevice = f\"cuda:{cuda_id}\"\nprint('cuda id', cuda_id)\ndef ok(score_list, mi, ma):\n    ma_id = np.argmax(score_list)\n    if score_list[ma_id] < mi:\n        return False\n    if score_list[ma_id] > ma:\n        return False",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "cuda_id",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "cuda_id = int(rank) % 8\ndevice = f\"cuda:{cuda_id}\"\nprint('cuda id', cuda_id)\ndef ok(score_list, mi, ma):\n    ma_id = np.argmax(score_list)\n    if score_list[ma_id] < mi:\n        return False\n    if score_list[ma_id] > ma:\n        return False\n    score_list_except = np.delete(score_list, ma_id)",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "device = f\"cuda:{cuda_id}\"\nprint('cuda id', cuda_id)\ndef ok(score_list, mi, ma):\n    ma_id = np.argmax(score_list)\n    if score_list[ma_id] < mi:\n        return False\n    if score_list[ma_id] > ma:\n        return False\n    score_list_except = np.delete(score_list, ma_id)\n    if len(score_list_except) == 0:",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "train_name_list_path",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "train_name_list_path = g_pathmgr.get_local_path(global_variable.train_name_list_path)\ntrain_name_list = json.load(open(global_variable.train_name_list_path, 'r'))\nn_v = args.n_v\nn_render = args.n_render\n# n_v = 4\n# n_render = 1\n# data_type = \"scannet\"\ndata_type = args.data_type\nhardness = args.hardness\nsplit = args.split",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "train_name_list",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "train_name_list = json.load(open(global_variable.train_name_list_path, 'r'))\nn_v = args.n_v\nn_render = args.n_render\n# n_v = 4\n# n_render = 1\n# data_type = \"scannet\"\ndata_type = args.data_type\nhardness = args.hardness\nsplit = args.split\nn_tuple_per_scene = args.n_tuple_per_scene",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "n_v",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "n_v = args.n_v\nn_render = args.n_render\n# n_v = 4\n# n_render = 1\n# data_type = \"scannet\"\ndata_type = args.data_type\nhardness = args.hardness\nsplit = args.split\nn_tuple_per_scene = args.n_tuple_per_scene\nn_try = 1000",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "n_render",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "n_render = args.n_render\n# n_v = 4\n# n_render = 1\n# data_type = \"scannet\"\ndata_type = args.data_type\nhardness = args.hardness\nsplit = args.split\nn_tuple_per_scene = args.n_tuple_per_scene\nn_try = 1000\ntarget_n = 1000",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "data_type",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "data_type = args.data_type\nhardness = args.hardness\nsplit = args.split\nn_tuple_per_scene = args.n_tuple_per_scene\nn_try = 1000\ntarget_n = 1000\nn_try_scene = 100000\nrender_range = [\n    [0, 4 * (i + 1)]\n    for i in range(6)",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "hardness",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "hardness = args.hardness\nsplit = args.split\nn_tuple_per_scene = args.n_tuple_per_scene\nn_try = 1000\ntarget_n = 1000\nn_try_scene = 100000\nrender_range = [\n    [0, 4 * (i + 1)]\n    for i in range(6)\n]",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "split",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "split = args.split\nn_tuple_per_scene = args.n_tuple_per_scene\nn_try = 1000\ntarget_n = 1000\nn_try_scene = 100000\nrender_range = [\n    [0, 4 * (i + 1)]\n    for i in range(6)\n]\ncover_hardness = {",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "n_tuple_per_scene",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "n_tuple_per_scene = args.n_tuple_per_scene\nn_try = 1000\ntarget_n = 1000\nn_try_scene = 100000\nrender_range = [\n    [0, 4 * (i + 1)]\n    for i in range(6)\n]\ncover_hardness = {\n    'hard': [0.1, 0.4],",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "n_try",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "n_try = 1000\ntarget_n = 1000\nn_try_scene = 100000\nrender_range = [\n    [0, 4 * (i + 1)]\n    for i in range(6)\n]\ncover_hardness = {\n    'hard': [0.1, 0.4],\n    'easy': [0.3, 0.7],",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "target_n",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "target_n = 1000\nn_try_scene = 100000\nrender_range = [\n    [0, 4 * (i + 1)]\n    for i in range(6)\n]\ncover_hardness = {\n    'hard': [0.1, 0.4],\n    'easy': [0.3, 0.7],\n    'easier': [0.3, 1.0],",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "n_try_scene",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "n_try_scene = 100000\nrender_range = [\n    [0, 4 * (i + 1)]\n    for i in range(6)\n]\ncover_hardness = {\n    'hard': [0.1, 0.4],\n    'easy': [0.3, 0.7],\n    'easier': [0.3, 1.0],\n}",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "render_range",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "render_range = [\n    [0, 4 * (i + 1)]\n    for i in range(6)\n]\ncover_hardness = {\n    'hard': [0.1, 0.4],\n    'easy': [0.3, 0.7],\n    'easier': [0.3, 1.0],\n}\nif \"_\" in hardness:",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "cover_hardness",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "cover_hardness = {\n    'hard': [0.1, 0.4],\n    'easy': [0.3, 0.7],\n    'easier': [0.3, 1.0],\n}\nif \"_\" in hardness:\n    mi, ma = hardness.split('_')\n    cover_hardness[hardness] = [float(mi), float(ma)]\nn_inference = n_v - n_render\n# dataset_name = \"scannet_large_easy_12\"",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "n_inference",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "n_inference = n_v - n_render\n# dataset_name = \"scannet_large_easy_12\"\n# dataset_name = \"scannet_large_easier_12\"\n# dataset_name = \"scannet_large_12\"\n# dataset_name = f\"scannetpp_large_easier_{n_v}\"\n# dataset_name = f\"scannetpp_large_easy_{n_v}\"\ndataset_name = f\"{data_type}_large_{hardness}_{n_v}_{split}\"\nif data_type == \"scannet\":\n    nn_shrink_rate = 3\nelif data_type == \"scannetpp\":",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "dataset_name",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_gen",
        "description": "datasets_preprocess.scannet_traj_gen",
        "peekOfCode": "dataset_name = f\"{data_type}_large_{hardness}_{n_v}_{split}\"\nif data_type == \"scannet\":\n    nn_shrink_rate = 3\nelif data_type == \"scannetpp\":\n    nn_shrink_rate = 9\ndef compare(a, b):\n    a = int(os.path.basename(a)[:-4])\n    b = int(os.path.basename(b)[:-4])\n    return a < b\ndef key(a):",
        "detail": "datasets_preprocess.scannet_traj_gen",
        "documentation": {}
    },
    {
        "label": "init_distributed",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "def init_distributed():\n    if not dist.is_initialized():\n        dist.init_process_group(backend='gloo', init_method='env://')\ndef get_rank():\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()\ninit_distributed()\nrank = get_rank()\ncuda_id = int(rank) % 8",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "get_rank",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "def get_rank():\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()\ninit_distributed()\nrank = get_rank()\ncuda_id = int(rank) % 8\ndevice = f\"cuda:{cuda_id}\"\nprint('cuda id', cuda_id)\nimport PIL",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "compare",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "def compare(a, b):\n    a = int(os.path.basename(a)[:-4])\n    b = int(os.path.basename(b)[:-4])\n    return a < b\ndef key(a):\n    return int(os.path.basename(a)[:-4])\ndef min_dis(A, B):\n    A = torch.from_numpy(A).reshape(-1, 3).to(device)\n    B = torch.from_numpy(B).reshape(-1, 3).to(device)\n    from pytorch3d.ops import knn_points",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "key",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "def key(a):\n    return int(os.path.basename(a)[:-4])\ndef min_dis(A, B):\n    A = torch.from_numpy(A).reshape(-1, 3).to(device)\n    B = torch.from_numpy(B).reshape(-1, 3).to(device)\n    from pytorch3d.ops import knn_points\n    dis, _, _ = knn_points(B[None], A[None]) # B querying in A, dis: [1, B.shape[0], 1]\n    return dis[0,:,0]\ndef cover(pc1_, pc2_): # querying pc2 in pc1\n    import numpy as np",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "min_dis",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "def min_dis(A, B):\n    A = torch.from_numpy(A).reshape(-1, 3).to(device)\n    B = torch.from_numpy(B).reshape(-1, 3).to(device)\n    from pytorch3d.ops import knn_points\n    dis, _, _ = knn_points(B[None], A[None]) # B querying in A, dis: [1, B.shape[0], 1]\n    return dis[0,:,0]\ndef cover(pc1_, pc2_): # querying pc2 in pc1\n    import numpy as np\n    pc1 = pc1_.reshape(-1, 3)\n    pc2 = pc2_.reshape(-1, 3)",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "cover",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "def cover(pc1_, pc2_): # querying pc2 in pc1\n    import numpy as np\n    pc1 = pc1_.reshape(-1, 3)\n    pc2 = pc2_.reshape(-1, 3)\n    distances = min_dis(pc1, pc2)\n    thres = 0.015 * nn_shrink_rate\n    return distances[(distances > 0) * (distances < thres)].shape[0] / distances.shape[0]\ndef get_score(pc1, pc2):\n    return (cover(pc1, pc2) + cover(pc2, pc1)) / 2\ndef extract_valid_frames(valid_frames_ss):",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "get_score",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "def get_score(pc1, pc2):\n    return (cover(pc1, pc2) + cover(pc2, pc1)) / 2\ndef extract_valid_frames(valid_frames_ss):\n    valid_frames_name = []\n    for x in valid_frames_ss:\n        x = x.split(' ')\n        if len(x) <= 1:\n            break\n        if int(x[2]) != 0:\n            print('get bad frame', x)",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "extract_valid_frames",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "def extract_valid_frames(valid_frames_ss):\n    valid_frames_name = []\n    for x in valid_frames_ss:\n        x = x.split(' ')\n        if len(x) <= 1:\n            break\n        if int(x[2]) != 0:\n            print('get bad frame', x)\n            # exit(0)\n        valid_frames_name.append(int(x[1]))",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "qvec2rotmat",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "def qvec2rotmat(qvec):\n    return np.array([\n        [1 - 2 * qvec[2]**2 - 2 * qvec[3]**2,\n         2 * qvec[1] * qvec[2] - 2 * qvec[0] * qvec[3],\n         2 * qvec[3] * qvec[1] + 2 * qvec[0] * qvec[2]],\n        [2 * qvec[1] * qvec[2] + 2 * qvec[0] * qvec[3],\n         1 - 2 * qvec[1]**2 - 2 * qvec[3]**2,\n         2 * qvec[2] * qvec[3] - 2 * qvec[0] * qvec[1]],\n        [2 * qvec[3] * qvec[1] - 2 * qvec[0] * qvec[2],\n         2 * qvec[2] * qvec[3] + 2 * qvec[0] * qvec[1],",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "qt2w2c",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "def qt2w2c(q, t) -> np.ndarray:\n    R = qvec2rotmat(q)\n    world2cam = np.eye(4)\n    world2cam[:3, :3] = R\n    world2cam[:3, 3] = t\n    return world2cam\ndef extract_image_txt(lines):\n    c2ws = []\n    frame_names = []\n    for x in lines:",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "extract_image_txt",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "def extract_image_txt(lines):\n    c2ws = []\n    frame_names = []\n    for x in lines:\n        if \".jpg\" in x:\n            x = x.split(' ')\n            q = [float(y) for y in x[1:5]]\n            t = [float(y) for y in x[5:8]]\n            frame = int(x[9].replace('frame_', '').replace('.jpg', ''))\n            c2ws.append(np.linalg.inv(qt2w2c(q, t)))",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "extract_K",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "def extract_K(lines):\n    ss = lines[3].split(' ')\n    K = np.eye(3)\n    K[0,0] = float(ss[4])\n    K[1,1] = float(ss[5])\n    K[0,2] = float(ss[6])\n    K[1,2] = float(ss[7])\n    return K\ndef get_scene_list():\n    if data_type == \"scannet\":",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "get_scene_list",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "def get_scene_list():\n    if data_type == \"scannet\":\n        scan_list = glob.glob(\"/vol22/zt15/scannet/scans/*\")\n        scan_list.sort()\n        # scan_list = np.load(g_pathmgr.get_local_path(global_variable.scannet_scan_list))\n    elif data_type == \"scannetpp\":\n        scan_list = np.load(g_pathmgr.get_local_path(global_variable.scannetpp_scan_list))\n        scan_list = [os.path.join(global_variable.scannetpp_data_dir + \"/data\", x.split('/')[-1]) for x in scan_list]\n    return scan_list\ndef filter_test(scene_list):",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "filter_test",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "def filter_test(scene_list):\n    scene_list_new = []\n    for x in scene_list:\n        scene_name = os.path.basename(x)\n        if scene_name not in train_name_list:\n            scene_list_new.append(x)\n    return scene_list_new\ndef main():\n    scene_list = get_scene_list()\n    if args.split == \"test\":",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "def main():\n    scene_list = get_scene_list()\n    if args.split == \"test\":\n        scene_list = filter_test(scene_list)\n    # exit(0)\n    tuple_done = 0\n    for scene_id, scene_name in enumerate(scene_list[::1]):\n        if scene_id % args.div != cuda_id + args.node_no * 8:\n            continue\n        print('trying', scene_id, cuda_id, cuda_id + args.node_no * 8, scene_name)",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument(\"--div\", default=1, type=int)\nparser.add_argument(\"--node-no\", default=0, type=int)\nparser.add_argument(\"--hardness\", type=str)\nparser.add_argument(\"--n-v\", type = int)\nparser.add_argument(\"--n-render\", type = int)\nparser.add_argument(\"--data-type\", type = str)\nparser.add_argument(\"--split\", type = str, default = \"all\")\nparser.add_argument(\"--render-overlap\", type = float, default = 0.95)\nparser.add_argument(\"--n-tuple-per-scene\", type = int, default = 1000)",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "args = parser.parse_args()\nprint('args', args)\nnode_no = args.node_no\nimport torch.distributed as dist\ndef init_distributed():\n    if not dist.is_initialized():\n        dist.init_process_group(backend='gloo', init_method='env://')\ndef get_rank():\n    if not dist.is_initialized():\n        return 0",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "node_no",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "node_no = args.node_no\nimport torch.distributed as dist\ndef init_distributed():\n    if not dist.is_initialized():\n        dist.init_process_group(backend='gloo', init_method='env://')\ndef get_rank():\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()\ninit_distributed()",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "rank",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "rank = get_rank()\ncuda_id = int(rank) % 8\ndevice = f\"cuda:{cuda_id}\"\nprint('cuda id', cuda_id)\nimport PIL\nimport numpy as np\nimport torch\nimport glob, sys\nimport json\nimport imageio",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "cuda_id",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "cuda_id = int(rank) % 8\ndevice = f\"cuda:{cuda_id}\"\nprint('cuda id', cuda_id)\nimport PIL\nimport numpy as np\nimport torch\nimport glob, sys\nimport json\nimport imageio\nimport cv2",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "device = f\"cuda:{cuda_id}\"\nprint('cuda id', cuda_id)\nimport PIL\nimport numpy as np\nimport torch\nimport glob, sys\nimport json\nimport imageio\nimport cv2\nfrom dust3r.utils.geometry import depthmap_to_absolute_camera_coordinates",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "train_name_list_path",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "train_name_list_path = g_pathmgr.get_local_path(global_variable.train_name_list_path)\ntrain_name_list = json.load(open(global_variable.train_name_list_path, 'r'))\nn_v = args.n_v\nn_render = args.n_render\n# n_v = 4\n# n_render = 1\n# data_type = \"scannet\"\ndata_type = args.data_type\nhardness = args.hardness\nsplit = args.split",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "train_name_list",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "train_name_list = json.load(open(global_variable.train_name_list_path, 'r'))\nn_v = args.n_v\nn_render = args.n_render\n# n_v = 4\n# n_render = 1\n# data_type = \"scannet\"\ndata_type = args.data_type\nhardness = args.hardness\nsplit = args.split\nn_tuple_per_scene = args.n_tuple_per_scene",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "n_v",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "n_v = args.n_v\nn_render = args.n_render\n# n_v = 4\n# n_render = 1\n# data_type = \"scannet\"\ndata_type = args.data_type\nhardness = args.hardness\nsplit = args.split\nn_tuple_per_scene = args.n_tuple_per_scene\nn_try = 1000",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "n_render",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "n_render = args.n_render\n# n_v = 4\n# n_render = 1\n# data_type = \"scannet\"\ndata_type = args.data_type\nhardness = args.hardness\nsplit = args.split\nn_tuple_per_scene = args.n_tuple_per_scene\nn_try = 1000\ntarget_n = 100",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "data_type",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "data_type = args.data_type\nhardness = args.hardness\nsplit = args.split\nn_tuple_per_scene = args.n_tuple_per_scene\nn_try = 1000\ntarget_n = 100\nn_try_scene = 100000\nrender_range = [\n    [0, 4 * (i + 1)]\n    for i in range(6)",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "hardness",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "hardness = args.hardness\nsplit = args.split\nn_tuple_per_scene = args.n_tuple_per_scene\nn_try = 1000\ntarget_n = 100\nn_try_scene = 100000\nrender_range = [\n    [0, 4 * (i + 1)]\n    for i in range(6)\n]",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "split",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "split = args.split\nn_tuple_per_scene = args.n_tuple_per_scene\nn_try = 1000\ntarget_n = 100\nn_try_scene = 100000\nrender_range = [\n    [0, 4 * (i + 1)]\n    for i in range(6)\n]\ncover_hardness = {",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "n_tuple_per_scene",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "n_tuple_per_scene = args.n_tuple_per_scene\nn_try = 1000\ntarget_n = 100\nn_try_scene = 100000\nrender_range = [\n    [0, 4 * (i + 1)]\n    for i in range(6)\n]\ncover_hardness = {\n    'hard': [0.1, 0.4],",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "n_try",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "n_try = 1000\ntarget_n = 100\nn_try_scene = 100000\nrender_range = [\n    [0, 4 * (i + 1)]\n    for i in range(6)\n]\ncover_hardness = {\n    'hard': [0.1, 0.4],\n    'easy': [0.3, 0.7],",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "target_n",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "target_n = 100\nn_try_scene = 100000\nrender_range = [\n    [0, 4 * (i + 1)]\n    for i in range(6)\n]\ncover_hardness = {\n    'hard': [0.1, 0.4],\n    'easy': [0.3, 0.7],\n    'easier': [0.3, 1.0],",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "n_try_scene",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "n_try_scene = 100000\nrender_range = [\n    [0, 4 * (i + 1)]\n    for i in range(6)\n]\ncover_hardness = {\n    'hard': [0.1, 0.4],\n    'easy': [0.3, 0.7],\n    'easier': [0.3, 1.0],\n}",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "render_range",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "render_range = [\n    [0, 4 * (i + 1)]\n    for i in range(6)\n]\ncover_hardness = {\n    'hard': [0.1, 0.4],\n    'easy': [0.3, 0.7],\n    'easier': [0.3, 1.0],\n}\nif \"_\" in hardness:",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "cover_hardness",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "cover_hardness = {\n    'hard': [0.1, 0.4],\n    'easy': [0.3, 0.7],\n    'easier': [0.3, 1.0],\n}\nif \"_\" in hardness:\n    mi, ma = hardness.split('_')\n    cover_hardness[hardness] = [float(mi), float(ma)]\nn_inference = n_v - n_render\n# dataset_name = \"scannet_large_easy_12\"",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "n_inference",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "n_inference = n_v - n_render\n# dataset_name = \"scannet_large_easy_12\"\n# dataset_name = \"scannet_large_easier_12\"\n# dataset_name = \"scannet_large_12\"\n# dataset_name = f\"scannetpp_large_easier_{n_v}\"\n# dataset_name = f\"scannetpp_large_easy_{n_v}\"\ndataset_name = f\"{data_type}_{hardness}_{n_v}_seq_{split}\"\nif data_type == \"scannet\":\n    nn_shrink_rate = 3\nelif data_type == \"scannetpp\":",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "dataset_name",
        "kind": 5,
        "importPath": "datasets_preprocess.scannet_traj_seq_gen",
        "description": "datasets_preprocess.scannet_traj_seq_gen",
        "peekOfCode": "dataset_name = f\"{data_type}_{hardness}_{n_v}_seq_{split}\"\nif data_type == \"scannet\":\n    nn_shrink_rate = 3\nelif data_type == \"scannetpp\":\n    nn_shrink_rate = 9\ndef compare(a, b):\n    a = int(os.path.basename(a)[:-4])\n    b = int(os.path.basename(b)[:-4])\n    return a < b\ndef key(a):",
        "detail": "datasets_preprocess.scannet_traj_seq_gen",
        "documentation": {}
    },
    {
        "label": "BasePCOptimize",
        "kind": 6,
        "importPath": "dust3r.cloud_opt.base_opt",
        "description": "dust3r.cloud_opt.base_opt",
        "peekOfCode": "class BasePCOptimizer (nn.Module):\n    \"\"\" Optimize a global scene, given a list of pairwise observations.\n    Graph node: images\n    Graph edges: observations = (pred1, pred2)\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        if len(args) == 1 and len(kwargs) == 0:\n            other = deepcopy(args[0])\n            attrs = '''edges is_symmetrized dist n_imgs pred_i pred_j imshapes \n                        min_conf_thr conf_thr conf_i conf_j im_conf",
        "detail": "dust3r.cloud_opt.base_opt",
        "documentation": {}
    },
    {
        "label": "global_alignment_loop",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.base_opt",
        "description": "dust3r.cloud_opt.base_opt",
        "peekOfCode": "def global_alignment_loop(net, lr=0.01, niter=300, schedule='cosine', lr_min=1e-6):\n    params = [p for p in net.parameters() if p.requires_grad]\n    if not params:\n        return net\n    verbose = net.verbose\n    if verbose:\n        print('Global alignement - optimizing for:')\n        print([name for name, value in net.named_parameters() if value.requires_grad])\n    lr_base = lr\n    optimizer = torch.optim.Adam(params, lr=lr, betas=(0.9, 0.9))",
        "detail": "dust3r.cloud_opt.base_opt",
        "documentation": {}
    },
    {
        "label": "global_alignment_iter",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.base_opt",
        "description": "dust3r.cloud_opt.base_opt",
        "peekOfCode": "def global_alignment_iter(net, cur_iter, niter, lr_base, lr_min, optimizer, schedule):\n    t = cur_iter / niter\n    if schedule == 'cosine':\n        lr = cosine_schedule(t, lr_base, lr_min)\n    elif schedule == 'linear':\n        lr = linear_schedule(t, lr_base, lr_min)\n    else:\n        raise ValueError(f'bad lr {schedule=}')\n    adjust_learning_rate_by_lr(optimizer, lr)\n    optimizer.zero_grad()",
        "detail": "dust3r.cloud_opt.base_opt",
        "documentation": {}
    },
    {
        "label": "edge_str",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "peekOfCode": "def edge_str(i, j):\n    return f'{i}_{j}'\ndef i_j_ij(ij):\n    return edge_str(*ij), ij\ndef edge_conf(conf_i, conf_j, edge):\n    return float(conf_i[edge].mean() * conf_j[edge].mean())\ndef compute_edge_scores(edges, conf_i, conf_j):\n    return {(i, j): edge_conf(conf_i, conf_j, e) for e, (i, j) in edges}\ndef NoGradParamDict(x):\n    assert isinstance(x, dict)",
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "i_j_ij",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "peekOfCode": "def i_j_ij(ij):\n    return edge_str(*ij), ij\ndef edge_conf(conf_i, conf_j, edge):\n    return float(conf_i[edge].mean() * conf_j[edge].mean())\ndef compute_edge_scores(edges, conf_i, conf_j):\n    return {(i, j): edge_conf(conf_i, conf_j, e) for e, (i, j) in edges}\ndef NoGradParamDict(x):\n    assert isinstance(x, dict)\n    return nn.ParameterDict(x).requires_grad_(False)\ndef get_imshapes(edges, pred_i, pred_j):",
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "edge_conf",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "peekOfCode": "def edge_conf(conf_i, conf_j, edge):\n    return float(conf_i[edge].mean() * conf_j[edge].mean())\ndef compute_edge_scores(edges, conf_i, conf_j):\n    return {(i, j): edge_conf(conf_i, conf_j, e) for e, (i, j) in edges}\ndef NoGradParamDict(x):\n    assert isinstance(x, dict)\n    return nn.ParameterDict(x).requires_grad_(False)\ndef get_imshapes(edges, pred_i, pred_j):\n    n_imgs = max(max(e) for e in edges) + 1\n    imshapes = [None] * n_imgs",
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "compute_edge_scores",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "peekOfCode": "def compute_edge_scores(edges, conf_i, conf_j):\n    return {(i, j): edge_conf(conf_i, conf_j, e) for e, (i, j) in edges}\ndef NoGradParamDict(x):\n    assert isinstance(x, dict)\n    return nn.ParameterDict(x).requires_grad_(False)\ndef get_imshapes(edges, pred_i, pred_j):\n    n_imgs = max(max(e) for e in edges) + 1\n    imshapes = [None] * n_imgs\n    for e, (i, j) in enumerate(edges):\n        shape_i = tuple(pred_i[e].shape[0:2])",
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "NoGradParamDict",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "peekOfCode": "def NoGradParamDict(x):\n    assert isinstance(x, dict)\n    return nn.ParameterDict(x).requires_grad_(False)\ndef get_imshapes(edges, pred_i, pred_j):\n    n_imgs = max(max(e) for e in edges) + 1\n    imshapes = [None] * n_imgs\n    for e, (i, j) in enumerate(edges):\n        shape_i = tuple(pred_i[e].shape[0:2])\n        shape_j = tuple(pred_j[e].shape[0:2])\n        if imshapes[i]:",
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "get_imshapes",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "peekOfCode": "def get_imshapes(edges, pred_i, pred_j):\n    n_imgs = max(max(e) for e in edges) + 1\n    imshapes = [None] * n_imgs\n    for e, (i, j) in enumerate(edges):\n        shape_i = tuple(pred_i[e].shape[0:2])\n        shape_j = tuple(pred_j[e].shape[0:2])\n        if imshapes[i]:\n            assert imshapes[i] == shape_i, f'incorrect shape for image {i}'\n        if imshapes[j]:\n            assert imshapes[j] == shape_j, f'incorrect shape for image {j}'",
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "get_conf_trf",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "peekOfCode": "def get_conf_trf(mode):\n    if mode == 'log':\n        def conf_trf(x): return x.log()\n    elif mode == 'sqrt':\n        def conf_trf(x): return x.sqrt()\n    elif mode == 'm1':\n        def conf_trf(x): return x-1\n    elif mode in ('id', 'none'):\n        def conf_trf(x): return x\n    else:",
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "l2_dist",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "peekOfCode": "def l2_dist(a, b, weight):\n    return ((a - b).square().sum(dim=-1) * weight)\ndef l1_dist(a, b, weight):\n    return ((a - b).norm(dim=-1) * weight)\nALL_DISTS = dict(l1=l1_dist, l2=l2_dist)\ndef signed_log1p(x):\n    sign = torch.sign(x)\n    return sign * torch.log1p(torch.abs(x))\ndef signed_expm1(x):\n    sign = torch.sign(x)",
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "l1_dist",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "peekOfCode": "def l1_dist(a, b, weight):\n    return ((a - b).norm(dim=-1) * weight)\nALL_DISTS = dict(l1=l1_dist, l2=l2_dist)\ndef signed_log1p(x):\n    sign = torch.sign(x)\n    return sign * torch.log1p(torch.abs(x))\ndef signed_expm1(x):\n    sign = torch.sign(x)\n    return sign * torch.expm1(torch.abs(x))\ndef cosine_schedule(t, lr_start, lr_end):",
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "signed_log1p",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "peekOfCode": "def signed_log1p(x):\n    sign = torch.sign(x)\n    return sign * torch.log1p(torch.abs(x))\ndef signed_expm1(x):\n    sign = torch.sign(x)\n    return sign * torch.expm1(torch.abs(x))\ndef cosine_schedule(t, lr_start, lr_end):\n    assert 0 <= t <= 1\n    return lr_end + (lr_start - lr_end) * (1+np.cos(t * np.pi))/2\ndef linear_schedule(t, lr_start, lr_end):",
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "signed_expm1",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "peekOfCode": "def signed_expm1(x):\n    sign = torch.sign(x)\n    return sign * torch.expm1(torch.abs(x))\ndef cosine_schedule(t, lr_start, lr_end):\n    assert 0 <= t <= 1\n    return lr_end + (lr_start - lr_end) * (1+np.cos(t * np.pi))/2\ndef linear_schedule(t, lr_start, lr_end):\n    assert 0 <= t <= 1\n    return lr_start + (lr_end - lr_start) * t",
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "cosine_schedule",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "peekOfCode": "def cosine_schedule(t, lr_start, lr_end):\n    assert 0 <= t <= 1\n    return lr_end + (lr_start - lr_end) * (1+np.cos(t * np.pi))/2\ndef linear_schedule(t, lr_start, lr_end):\n    assert 0 <= t <= 1\n    return lr_start + (lr_end - lr_start) * t",
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "linear_schedule",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "peekOfCode": "def linear_schedule(t, lr_start, lr_end):\n    assert 0 <= t <= 1\n    return lr_start + (lr_end - lr_start) * t",
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "ALL_DISTS",
        "kind": 5,
        "importPath": "dust3r.cloud_opt.commons",
        "description": "dust3r.cloud_opt.commons",
        "peekOfCode": "ALL_DISTS = dict(l1=l1_dist, l2=l2_dist)\ndef signed_log1p(x):\n    sign = torch.sign(x)\n    return sign * torch.log1p(torch.abs(x))\ndef signed_expm1(x):\n    sign = torch.sign(x)\n    return sign * torch.expm1(torch.abs(x))\ndef cosine_schedule(t, lr_start, lr_end):\n    assert 0 <= t <= 1\n    return lr_end + (lr_start - lr_end) * (1+np.cos(t * np.pi))/2",
        "detail": "dust3r.cloud_opt.commons",
        "documentation": {}
    },
    {
        "label": "init_from_known_poses",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.init_im_poses",
        "description": "dust3r.cloud_opt.init_im_poses",
        "peekOfCode": "def init_from_known_poses(self, niter_PnP=10, min_conf_thr=3):\n    device = self.device\n    # indices of known poses\n    nkp, known_poses_msk, known_poses = get_known_poses(self)\n    assert nkp == self.n_imgs, 'not all poses are known'\n    # get all focals\n    nkf, _, im_focals = get_known_focals(self)\n    assert nkf == self.n_imgs\n    im_pp = self.get_principal_points()\n    best_depthmaps = {}",
        "detail": "dust3r.cloud_opt.init_im_poses",
        "documentation": {}
    },
    {
        "label": "init_minimum_spanning_tree",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.init_im_poses",
        "description": "dust3r.cloud_opt.init_im_poses",
        "peekOfCode": "def init_minimum_spanning_tree(self, **kw):\n    \"\"\" Init all camera poses (image-wise and pairwise poses) given\n        an initial set of pairwise estimations.\n    \"\"\"\n    device = self.device\n    pts3d, _, im_focals, im_poses = minimum_spanning_tree(self.imshapes, self.edges,\n                                                          self.pred_i, self.pred_j, self.conf_i, self.conf_j, self.im_conf, self.min_conf_thr,\n                                                          device, has_im_poses=self.has_im_poses, verbose=self.verbose,\n                                                          **kw)\n    return init_from_pts3d(self, pts3d, im_focals, im_poses)",
        "detail": "dust3r.cloud_opt.init_im_poses",
        "documentation": {}
    },
    {
        "label": "init_from_pts3d",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.init_im_poses",
        "description": "dust3r.cloud_opt.init_im_poses",
        "peekOfCode": "def init_from_pts3d(self, pts3d, im_focals, im_poses):\n    # init poses\n    nkp, known_poses_msk, known_poses = get_known_poses(self)\n    if nkp == 1:\n        raise NotImplementedError(\"Would be simpler to just align everything afterwards on the single known pose\")\n    elif nkp > 1:\n        # global rigid SE3 alignment\n        s, R, T = align_multiple_poses(im_poses[known_poses_msk], known_poses[known_poses_msk])\n        trf = sRT_to_4x4(s, R, T, device=known_poses.device)\n        # rotate everything",
        "detail": "dust3r.cloud_opt.init_im_poses",
        "documentation": {}
    },
    {
        "label": "minimum_spanning_tree",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.init_im_poses",
        "description": "dust3r.cloud_opt.init_im_poses",
        "peekOfCode": "def minimum_spanning_tree(imshapes, edges, pred_i, pred_j, conf_i, conf_j, im_conf, min_conf_thr,\n                          device, has_im_poses=True, niter_PnP=10, verbose=True):\n    n_imgs = len(imshapes)\n    sparse_graph = -dict_to_sparse_graph(compute_edge_scores(map(i_j_ij, edges), conf_i, conf_j))\n    msp = sp.csgraph.minimum_spanning_tree(sparse_graph).tocoo()\n    # temp variable to store 3d points\n    pts3d = [None] * len(imshapes)\n    todo = sorted(zip(-msp.data, msp.row, msp.col))  # sorted edges\n    im_poses = [None] * n_imgs\n    im_focals = [None] * n_imgs",
        "detail": "dust3r.cloud_opt.init_im_poses",
        "documentation": {}
    },
    {
        "label": "dict_to_sparse_graph",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.init_im_poses",
        "description": "dust3r.cloud_opt.init_im_poses",
        "peekOfCode": "def dict_to_sparse_graph(dic):\n    n_imgs = max(max(e) for e in dic) + 1\n    res = sp.dok_matrix((n_imgs, n_imgs))\n    for edge, value in dic.items():\n        res[edge] = value\n    return res\ndef rigid_points_registration(pts1, pts2, conf):\n    R, T, s = roma.rigid_points_registration(\n        pts1.reshape(-1, 3), pts2.reshape(-1, 3), weights=conf.ravel(), compute_scaling=True)\n    return s, R, T  # return un-scaled (R, T)",
        "detail": "dust3r.cloud_opt.init_im_poses",
        "documentation": {}
    },
    {
        "label": "rigid_points_registration",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.init_im_poses",
        "description": "dust3r.cloud_opt.init_im_poses",
        "peekOfCode": "def rigid_points_registration(pts1, pts2, conf):\n    R, T, s = roma.rigid_points_registration(\n        pts1.reshape(-1, 3), pts2.reshape(-1, 3), weights=conf.ravel(), compute_scaling=True)\n    return s, R, T  # return un-scaled (R, T)\ndef sRT_to_4x4(scale, R, T, device):\n    trf = torch.eye(4, device=device)\n    trf[:3, :3] = R * scale\n    trf[:3, 3] = T.ravel()  # doesn't need scaling\n    return trf\ndef estimate_focal(pts3d_i, pp=None):",
        "detail": "dust3r.cloud_opt.init_im_poses",
        "documentation": {}
    },
    {
        "label": "sRT_to_4x4",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.init_im_poses",
        "description": "dust3r.cloud_opt.init_im_poses",
        "peekOfCode": "def sRT_to_4x4(scale, R, T, device):\n    trf = torch.eye(4, device=device)\n    trf[:3, :3] = R * scale\n    trf[:3, 3] = T.ravel()  # doesn't need scaling\n    return trf\ndef estimate_focal(pts3d_i, pp=None):\n    if pp is None:\n        H, W, THREE = pts3d_i.shape\n        assert THREE == 3\n        pp = torch.tensor((W/2, H/2), device=pts3d_i.device)",
        "detail": "dust3r.cloud_opt.init_im_poses",
        "documentation": {}
    },
    {
        "label": "estimate_focal",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.init_im_poses",
        "description": "dust3r.cloud_opt.init_im_poses",
        "peekOfCode": "def estimate_focal(pts3d_i, pp=None):\n    if pp is None:\n        H, W, THREE = pts3d_i.shape\n        assert THREE == 3\n        pp = torch.tensor((W/2, H/2), device=pts3d_i.device)\n    focal = estimate_focal_knowing_depth(pts3d_i.unsqueeze(0), pp.unsqueeze(0), focal_mode='weiszfeld').ravel()\n    return float(focal)\n@cache\ndef pixel_grid(H, W):\n    return np.mgrid[:W, :H].T.astype(np.float32)",
        "detail": "dust3r.cloud_opt.init_im_poses",
        "documentation": {}
    },
    {
        "label": "pixel_grid",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.init_im_poses",
        "description": "dust3r.cloud_opt.init_im_poses",
        "peekOfCode": "def pixel_grid(H, W):\n    return np.mgrid[:W, :H].T.astype(np.float32)\ndef fast_pnp(pts3d, focal, msk, device, pp=None, niter_PnP=10):\n    # extract camera poses and focals with RANSAC-PnP\n    if msk.sum() < 4:\n        return None  # we need at least 4 points for PnP\n    pts3d, msk = map(to_numpy, (pts3d, msk))\n    H, W, THREE = pts3d.shape\n    assert THREE == 3\n    pixels = pixel_grid(H, W)",
        "detail": "dust3r.cloud_opt.init_im_poses",
        "documentation": {}
    },
    {
        "label": "fast_pnp",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.init_im_poses",
        "description": "dust3r.cloud_opt.init_im_poses",
        "peekOfCode": "def fast_pnp(pts3d, focal, msk, device, pp=None, niter_PnP=10):\n    # extract camera poses and focals with RANSAC-PnP\n    if msk.sum() < 4:\n        return None  # we need at least 4 points for PnP\n    pts3d, msk = map(to_numpy, (pts3d, msk))\n    H, W, THREE = pts3d.shape\n    assert THREE == 3\n    pixels = pixel_grid(H, W)\n    if focal is None:\n        S = max(W, H)",
        "detail": "dust3r.cloud_opt.init_im_poses",
        "documentation": {}
    },
    {
        "label": "get_known_poses",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.init_im_poses",
        "description": "dust3r.cloud_opt.init_im_poses",
        "peekOfCode": "def get_known_poses(self):\n    if self.has_im_poses:\n        known_poses_msk = torch.tensor([not (p.requires_grad) for p in self.im_poses])\n        known_poses = self.get_im_poses()\n        return known_poses_msk.sum(), known_poses_msk, known_poses\n    else:\n        return 0, None, None\ndef get_known_focals(self):\n    if self.has_im_poses:\n        known_focal_msk = self.get_known_focal_mask()",
        "detail": "dust3r.cloud_opt.init_im_poses",
        "documentation": {}
    },
    {
        "label": "get_known_focals",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.init_im_poses",
        "description": "dust3r.cloud_opt.init_im_poses",
        "peekOfCode": "def get_known_focals(self):\n    if self.has_im_poses:\n        known_focal_msk = self.get_known_focal_mask()\n        known_focals = self.get_focals()\n        return known_focal_msk.sum(), known_focal_msk, known_focals\n    else:\n        return 0, None, None\ndef align_multiple_poses(src_poses, target_poses):\n    N = len(src_poses)\n    assert src_poses.shape == target_poses.shape == (N, 4, 4)",
        "detail": "dust3r.cloud_opt.init_im_poses",
        "documentation": {}
    },
    {
        "label": "align_multiple_poses",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.init_im_poses",
        "description": "dust3r.cloud_opt.init_im_poses",
        "peekOfCode": "def align_multiple_poses(src_poses, target_poses):\n    N = len(src_poses)\n    assert src_poses.shape == target_poses.shape == (N, 4, 4)\n    def center_and_z(poses):\n        eps = get_med_dist_between_poses(poses) / 100\n        return torch.cat((poses[:, :3, 3], poses[:, :3, 3] + eps*poses[:, :3, 2]))\n    R, T, s = roma.rigid_points_registration(center_and_z(src_poses), center_and_z(target_poses), compute_scaling=True)\n    return s, R, T",
        "detail": "dust3r.cloud_opt.init_im_poses",
        "documentation": {}
    },
    {
        "label": "ModularPointCloudOptimize",
        "kind": 6,
        "importPath": "dust3r.cloud_opt.modular_optimizer",
        "description": "dust3r.cloud_opt.modular_optimizer",
        "peekOfCode": "class ModularPointCloudOptimizer (BasePCOptimizer):\n    \"\"\" Optimize a global scene, given a list of pairwise observations.\n    Unlike PointCloudOptimizer, you can fix parts of the optimization process (partial poses/intrinsics)\n    Graph node: images\n    Graph edges: observations = (pred1, pred2)\n    \"\"\"\n    def __init__(self, *args, optimize_pp=False, fx_and_fy=False, focal_brake=20, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.has_im_poses = True  # by definition of this class\n        self.focal_brake = focal_brake",
        "detail": "dust3r.cloud_opt.modular_optimizer",
        "documentation": {}
    },
    {
        "label": "PointCloudOptimizer",
        "kind": 6,
        "importPath": "dust3r.cloud_opt.optimizer",
        "description": "dust3r.cloud_opt.optimizer",
        "peekOfCode": "class PointCloudOptimizer(BasePCOptimizer):\n    \"\"\" Optimize a global scene, given a list of pairwise observations.\n    Graph node: images\n    Graph edges: observations = (pred1, pred2)\n    \"\"\"\n    def __init__(self, *args, optimize_pp=False, focal_break=20, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.has_im_poses = True  # by definition of this class\n        self.focal_break = focal_break\n        # adding thing to optimize",
        "detail": "dust3r.cloud_opt.optimizer",
        "documentation": {}
    },
    {
        "label": "ParameterStack",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.optimizer",
        "description": "dust3r.cloud_opt.optimizer",
        "peekOfCode": "def ParameterStack(params, keys=None, is_param=None, fill=0):\n    if keys is not None:\n        params = [params[k] for k in keys]\n    if fill > 0:\n        params = [_ravel_hw(p, fill) for p in params]\n    requires_grad = params[0].requires_grad\n    assert all(p.requires_grad == requires_grad for p in params)\n    params = torch.stack(list(params)).float().detach()\n    if is_param or requires_grad:\n        params = nn.Parameter(params)",
        "detail": "dust3r.cloud_opt.optimizer",
        "documentation": {}
    },
    {
        "label": "acceptable_focal_range",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.optimizer",
        "description": "dust3r.cloud_opt.optimizer",
        "peekOfCode": "def acceptable_focal_range(H, W, minf=0.5, maxf=3.5):\n    focal_base = max(H, W) / (2 * np.tan(np.deg2rad(60) / 2))  # size / 1.1547005383792515\n    return minf*focal_base, maxf*focal_base\ndef apply_mask(img, msk):\n    img = img.copy()\n    img[msk] = 0\n    return img",
        "detail": "dust3r.cloud_opt.optimizer",
        "documentation": {}
    },
    {
        "label": "apply_mask",
        "kind": 2,
        "importPath": "dust3r.cloud_opt.optimizer",
        "description": "dust3r.cloud_opt.optimizer",
        "peekOfCode": "def apply_mask(img, msk):\n    img = img.copy()\n    img[msk] = 0\n    return img",
        "detail": "dust3r.cloud_opt.optimizer",
        "documentation": {}
    },
    {
        "label": "PairViewe",
        "kind": 6,
        "importPath": "dust3r.cloud_opt.pair_viewer",
        "description": "dust3r.cloud_opt.pair_viewer",
        "peekOfCode": "class PairViewer (BasePCOptimizer):\n    \"\"\"\n    This a Dummy Optimizer.\n    To use only when the goal is to visualize the results for a pair of images (with is_symmetrized)\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert self.is_symmetrized and self.n_edges == 2\n        self.has_im_poses = True\n        # compute all parameters directly from raw input",
        "detail": "dust3r.cloud_opt.pair_viewer",
        "documentation": {}
    },
    {
        "label": "BaseStereoViewDatase",
        "kind": 6,
        "importPath": "dust3r.datasets.base.base_stereo_view_dataset",
        "description": "dust3r.datasets.base.base_stereo_view_dataset",
        "peekOfCode": "class BaseStereoViewDataset (EasyDataset):\n    \"\"\" Define all basic options.\n    Usage:\n        class MyDataset (BaseStereoViewDataset):\n            def _get_views(self, idx, rng):\n                # overload here\n                views = []\n                views.append(dict(img=, ...))\n                return views\n    \"\"\"",
        "detail": "dust3r.datasets.base.base_stereo_view_dataset",
        "documentation": {}
    },
    {
        "label": "is_good_type",
        "kind": 2,
        "importPath": "dust3r.datasets.base.base_stereo_view_dataset",
        "description": "dust3r.datasets.base.base_stereo_view_dataset",
        "peekOfCode": "def is_good_type(key, v):\n    \"\"\" returns (is_good, err_msg) \n    \"\"\"\n    if isinstance(v, (str, int, tuple)):\n        return True, None\n    if v.dtype is torch.float32:\n        return True, None\n    if v.dtype not in (np.float32, bool, np.int32, np.int64, np.uint8):\n        return False, f\"bad {v.dtype=}\"\n    return True, None",
        "detail": "dust3r.datasets.base.base_stereo_view_dataset",
        "documentation": {}
    },
    {
        "label": "view_name",
        "kind": 2,
        "importPath": "dust3r.datasets.base.base_stereo_view_dataset",
        "description": "dust3r.datasets.base.base_stereo_view_dataset",
        "peekOfCode": "def view_name(view, batch_index=None):\n    def sel(x): return x[batch_index] if batch_index not in (None, slice(None)) else x\n    db = sel(view['dataset'])\n    label = sel(view['label'])\n    instance = sel(view['instance'])\n    return f\"{db}/{label}/{instance}\"\ndef transpose_to_landscape(view):\n    height, width = view['true_shape']\n    if width < height:\n        # rectify portrait to landscape",
        "detail": "dust3r.datasets.base.base_stereo_view_dataset",
        "documentation": {}
    },
    {
        "label": "transpose_to_landscape",
        "kind": 2,
        "importPath": "dust3r.datasets.base.base_stereo_view_dataset",
        "description": "dust3r.datasets.base.base_stereo_view_dataset",
        "peekOfCode": "def transpose_to_landscape(view):\n    height, width = view['true_shape']\n    if width < height:\n        # rectify portrait to landscape\n        assert view['img'].shape == (3, height, width)\n        view['img'] = view['img'].swapaxes(1, 2)\n        assert view['valid_mask'].shape == (height, width)\n        view['valid_mask'] = view['valid_mask'].swapaxes(0, 1)\n        assert view['depthmap'].shape == (height, width)\n        view['depthmap'] = view['depthmap'].swapaxes(0, 1)",
        "detail": "dust3r.datasets.base.base_stereo_view_dataset",
        "documentation": {}
    },
    {
        "label": "BatchedRandomSampler",
        "kind": 6,
        "importPath": "dust3r.datasets.base.batched_sampler",
        "description": "dust3r.datasets.base.batched_sampler",
        "peekOfCode": "class BatchedRandomSampler:\n    \"\"\" Random sampling under a constraint: each sample in the batch has the same feature, \n    which is chosen randomly from a known pool of 'features' for each batch.\n    For instance, the 'feature' could be the image aspect-ratio.\n    The index returned is a tuple (sample_idx, feat_idx).\n    This sampler ensures that each series of `batch_size` indices has the same `feat_idx`.\n    \"\"\"\n    def __init__(self, dataset, batch_size, pool_size, world_size=1, rank=0, drop_last=True):\n        self.batch_size = batch_size\n        self.pool_size = pool_size",
        "detail": "dust3r.datasets.base.batched_sampler",
        "documentation": {}
    },
    {
        "label": "round_by",
        "kind": 2,
        "importPath": "dust3r.datasets.base.batched_sampler",
        "description": "dust3r.datasets.base.batched_sampler",
        "peekOfCode": "def round_by(total, multiple, up=False):\n    if up:\n        total = total + multiple-1\n    return (total//multiple) * multiple",
        "detail": "dust3r.datasets.base.batched_sampler",
        "documentation": {}
    },
    {
        "label": "EasyDataset",
        "kind": 6,
        "importPath": "dust3r.datasets.base.easy_dataset",
        "description": "dust3r.datasets.base.easy_dataset",
        "peekOfCode": "class EasyDataset:\n    \"\"\" a dataset that you can easily resize and combine.\n    Examples:\n    ---------\n        2 * dataset ==> duplicate each element 2x\n        10 @ dataset ==> set the size to 10 (random sampling, duplicates if necessary)\n        dataset1 + dataset2 ==> concatenate datasets\n    \"\"\"\n    def __add__(self, other):\n        return CatDataset([self, other])",
        "detail": "dust3r.datasets.base.easy_dataset",
        "documentation": {}
    },
    {
        "label": "MulDatase",
        "kind": 6,
        "importPath": "dust3r.datasets.base.easy_dataset",
        "description": "dust3r.datasets.base.easy_dataset",
        "peekOfCode": "class MulDataset (EasyDataset):\n    \"\"\" Artifically augmenting the size of a dataset.\n    \"\"\"\n    multiplicator: int\n    def __init__(self, multiplicator, dataset):\n        assert isinstance(multiplicator, int) and multiplicator > 0\n        self.multiplicator = multiplicator\n        self.dataset = dataset\n    def __len__(self):\n        return self.multiplicator * len(self.dataset)",
        "detail": "dust3r.datasets.base.easy_dataset",
        "documentation": {}
    },
    {
        "label": "ResizedDatase",
        "kind": 6,
        "importPath": "dust3r.datasets.base.easy_dataset",
        "description": "dust3r.datasets.base.easy_dataset",
        "peekOfCode": "class ResizedDataset (EasyDataset):\n    \"\"\" Artifically changing the size of a dataset.\n    \"\"\"\n    new_size: int\n    def __init__(self, new_size, dataset):\n        assert isinstance(new_size, int) and new_size > 0\n        self.new_size = new_size\n        self.dataset = dataset\n        self.split = self.dataset.split\n        self.save_results = self.dataset.save_results",
        "detail": "dust3r.datasets.base.easy_dataset",
        "documentation": {}
    },
    {
        "label": "CatDatase",
        "kind": 6,
        "importPath": "dust3r.datasets.base.easy_dataset",
        "description": "dust3r.datasets.base.easy_dataset",
        "peekOfCode": "class CatDataset (EasyDataset):\n    \"\"\" Concatenation of several datasets \n    \"\"\"\n    def __init__(self, datasets):\n        for dataset in datasets:\n            assert isinstance(dataset, EasyDataset)\n        self.datasets = datasets\n        self._cum_sizes = np.cumsum([len(dataset) for dataset in datasets])\n    def __len__(self):\n        return self._cum_sizes[-1]",
        "detail": "dust3r.datasets.base.easy_dataset",
        "documentation": {}
    },
    {
        "label": "ImageList",
        "kind": 6,
        "importPath": "dust3r.datasets.utils.cropping",
        "description": "dust3r.datasets.utils.cropping",
        "peekOfCode": "class ImageList:\n    \"\"\" Convenience class to aply the same operation to a whole set of images.\n    \"\"\"\n    def __init__(self, images):\n        if not isinstance(images, (tuple, list, set)):\n            images = [images]\n        self.images = []\n        for image in images:\n            if not isinstance(image, PIL.Image.Image):\n                image = PIL.Image.fromarray(image)",
        "detail": "dust3r.datasets.utils.cropping",
        "documentation": {}
    },
    {
        "label": "rescale_image_depthmap",
        "kind": 2,
        "importPath": "dust3r.datasets.utils.cropping",
        "description": "dust3r.datasets.utils.cropping",
        "peekOfCode": "def rescale_image_depthmap(image, depthmap, camera_intrinsics, output_resolution):\n    \"\"\" Jointly rescale a (image, depthmap) \n        so that (out_width, out_height) >= output_res\n    \"\"\"\n    image = ImageList(image)\n    input_resolution = np.array(image.size)  # (W,H)\n    output_resolution = np.array(output_resolution)\n    if depthmap is not None:\n        # can also use this with masks instead of depthmaps\n        assert tuple(depthmap.shape[:2]) == image.size[::-1]",
        "detail": "dust3r.datasets.utils.cropping",
        "documentation": {}
    },
    {
        "label": "camera_matrix_of_crop",
        "kind": 2,
        "importPath": "dust3r.datasets.utils.cropping",
        "description": "dust3r.datasets.utils.cropping",
        "peekOfCode": "def camera_matrix_of_crop(input_camera_matrix, input_resolution, output_resolution, scaling=1, offset_factor=0.5, offset=None):\n    # Margins to offset the origin\n    margins = np.asarray(input_resolution) * scaling - output_resolution\n    assert np.all(margins >= 0.0)\n    if offset is None:\n        offset = offset_factor * margins\n    # Generate new camera parameters\n    output_camera_matrix_colmap = opencv_to_colmap_intrinsics(input_camera_matrix)\n    output_camera_matrix_colmap[:2, :] *= scaling\n    output_camera_matrix_colmap[:2, 2] -= offset",
        "detail": "dust3r.datasets.utils.cropping",
        "documentation": {}
    },
    {
        "label": "crop_image_depthmap",
        "kind": 2,
        "importPath": "dust3r.datasets.utils.cropping",
        "description": "dust3r.datasets.utils.cropping",
        "peekOfCode": "def crop_image_depthmap(image, depthmap, camera_intrinsics, crop_bbox):\n    \"\"\"\n    Return a crop of the input view.\n    \"\"\"\n    image = ImageList(image)\n    l, t, r, b = crop_bbox\n    image = image.crop((l, t, r, b))\n    depthmap = depthmap[t:b, l:r]\n    camera_intrinsics = camera_intrinsics.copy()\n    camera_intrinsics[0, 2] -= l",
        "detail": "dust3r.datasets.utils.cropping",
        "documentation": {}
    },
    {
        "label": "bbox_from_intrinsics_in_out",
        "kind": 2,
        "importPath": "dust3r.datasets.utils.cropping",
        "description": "dust3r.datasets.utils.cropping",
        "peekOfCode": "def bbox_from_intrinsics_in_out(input_camera_matrix, output_camera_matrix, output_resolution):\n    out_width, out_height = output_resolution\n    l, t = np.int32(np.round(input_camera_matrix[:2, 2] - output_camera_matrix[:2, 2]))\n    crop_bbox = (l, t, l+out_width, t+out_height)\n    return crop_bbox",
        "detail": "dust3r.datasets.utils.cropping",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"]",
        "kind": 5,
        "importPath": "dust3r.datasets.utils.cropping",
        "description": "dust3r.datasets.utils.cropping",
        "peekOfCode": "os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"] = \"1\"\nimport cv2  # noqa\nimport numpy as np  # noqa\nfrom dust3r.utils.geometry import colmap_to_opencv_intrinsics, opencv_to_colmap_intrinsics  # noqa\ntry:\n    lanczos = PIL.Image.Resampling.LANCZOS\nexcept AttributeError:\n    lanczos = PIL.Image.LANCZOS\nclass ImageList:\n    \"\"\" Convenience class to aply the same operation to a whole set of images.",
        "detail": "dust3r.datasets.utils.cropping",
        "documentation": {}
    },
    {
        "label": "ColorJitter",
        "kind": 5,
        "importPath": "dust3r.datasets.utils.transforms",
        "description": "dust3r.datasets.utils.transforms",
        "peekOfCode": "ColorJitter = tvf.Compose([tvf.ColorJitter(0.5, 0.5, 0.5, 0.1), ImgNorm])",
        "detail": "dust3r.datasets.utils.transforms",
        "documentation": {}
    },
    {
        "label": "MVDataset",
        "kind": 6,
        "importPath": "dust3r.datasets.mvdataset",
        "description": "dust3r.datasets.mvdataset",
        "peekOfCode": "class MVDataset(BaseStereoViewDataset):\n    def __init__(self, mask_bg=True, from_tar = False, random_order = False, random_render_order = False, debug = False, *args, ROOT, n_test=1000, num_render_views = 0, n_vis_test = 8, n_vis_train = 8, split_thres = 0.9, render_start = None, tb_name = None, ref_all = False, n_ref = 1, random_nv_nr = None, dps_name = 'dps.h5', n_all = None, single_id = None, reverse = False, **kwargs):\n        self.ROOT = ROOT\n        self.num_render_views = num_render_views\n        self.from_tar = from_tar\n        self.random_order = random_order\n        self.random_render_order = random_render_order\n        super().__init__(*args, **kwargs) # self.num_views, split set inside\n        if \"test\" in dps_name:\n            self.test = True",
        "detail": "dust3r.datasets.mvdataset",
        "documentation": {}
    },
    {
        "label": "DPTOutputAdapter_fix",
        "kind": 6,
        "importPath": "dust3r.heads.dpt_head",
        "description": "dust3r.heads.dpt_head",
        "peekOfCode": "class DPTOutputAdapter_fix(DPTOutputAdapter):\n    \"\"\"\n    Adapt croco's DPTOutputAdapter implementation for dust3r:\n    remove duplicated weigths, and fix forward for dust3r\n    \"\"\"\n    def init(self, dim_tokens_enc=768):\n        super().init(dim_tokens_enc)\n        # these are duplicated weights\n        del self.act_1_postprocess\n        del self.act_2_postprocess",
        "detail": "dust3r.heads.dpt_head",
        "documentation": {}
    },
    {
        "label": "PixelwiseTaskWithDPT",
        "kind": 6,
        "importPath": "dust3r.heads.dpt_head",
        "description": "dust3r.heads.dpt_head",
        "peekOfCode": "class PixelwiseTaskWithDPT(nn.Module):\n    \"\"\" DPT module for dust3r, can return 3D points + confidence for all pixels\"\"\"\n    def __init__(self, *, n_cls_token=0, hooks_idx=None, dim_tokens=None,\n                 output_width_ratio=1, num_channels=1, postprocess=None, depth_mode=None, conf_mode=None, **kwargs):\n        super(PixelwiseTaskWithDPT, self).__init__()\n        self.return_all_layers = True  # backbone needs to return all layers\n        self.postprocess = postprocess\n        self.depth_mode = depth_mode\n        self.conf_mode = conf_mode\n        assert n_cls_token == 0, \"Not implemented\"",
        "detail": "dust3r.heads.dpt_head",
        "documentation": {}
    },
    {
        "label": "create_dpt_head",
        "kind": 2,
        "importPath": "dust3r.heads.dpt_head",
        "description": "dust3r.heads.dpt_head",
        "peekOfCode": "def create_dpt_head(net, has_conf=False):\n    \"\"\"\n    return PixelwiseTaskWithDPT for given net params\n    \"\"\"\n    assert net.dec_depth > 9\n    l2 = net.dec_depth\n    feature_dim = 256\n    last_dim = feature_dim//2\n    out_nchan = 3\n    ed = net.enc_embed_dim",
        "detail": "dust3r.heads.dpt_head",
        "documentation": {}
    },
    {
        "label": "DownSampling",
        "kind": 6,
        "importPath": "dust3r.heads.linear_head",
        "description": "dust3r.heads.linear_head",
        "peekOfCode": "class DownSampling(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=kernel_size//2)\n        self.norm = nn.BatchNorm2d(out_channels)\n        self.A = nn.SELU()\n    def forward(self, x):\n        return self.A(self.norm(self.conv(x)))\nclass MiddleBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, activation = False):",
        "detail": "dust3r.heads.linear_head",
        "documentation": {}
    },
    {
        "label": "MiddleBlock",
        "kind": 6,
        "importPath": "dust3r.heads.linear_head",
        "description": "dust3r.heads.linear_head",
        "peekOfCode": "class MiddleBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, activation = False):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=kernel_size//2)\n        self.norm = nn.BatchNorm2d(out_channels)\n        self.A = nn.SELU()\n        self.activation = activation\n    def forward(self, x):\n        if self.activation:\n            return self.A(self.norm(self.conv(x)))",
        "detail": "dust3r.heads.linear_head",
        "documentation": {}
    },
    {
        "label": "LinearPts3d",
        "kind": 6,
        "importPath": "dust3r.heads.linear_head",
        "description": "dust3r.heads.linear_head",
        "peekOfCode": "class LinearPts3d(nn.Module):\n    \"\"\" \n    Linear head for dust3r\n    Each token outputs: - 16x16 3D points (+ confidence)\n    \"\"\"\n    def __init__(self, net, has_conf=False, skip = False):\n        super().__init__()\n        self.patch_size = net.patch_embed.patch_size[0]\n        self.depth_mode = net.depth_mode\n        self.conf_mode = net.conf_mode",
        "detail": "dust3r.heads.linear_head",
        "documentation": {}
    },
    {
        "label": "DownSamplings",
        "kind": 6,
        "importPath": "dust3r.heads.linear_head",
        "description": "dust3r.heads.linear_head",
        "peekOfCode": "class DownSamplings(nn.Module):\n    def __init__(self, in_channels, out_channels, num_layers=2):\n        super().__init__()\n        self.layers = nn.ModuleList()\n        for i in range(num_layers):\n            in_channels_i = in_channels if i == 0 else out_channels // (2 ** (num_layers - i))\n            out_channels_i = out_channels // (2 ** (num_layers - 1 - i))\n            self.layers.append(DownSampling(in_channels_i, out_channels_i))\n    def forward(self, x):\n        for layer in self.layers:",
        "detail": "dust3r.heads.linear_head",
        "documentation": {}
    },
    {
        "label": "GSHead",
        "kind": 6,
        "importPath": "dust3r.heads.linear_head",
        "description": "dust3r.heads.linear_head",
        "peekOfCode": "class GSHead(nn.Module):\n    def __init__(\n        self,\n        net,\n        mlp_dim=768, # 256 original\n        mlp_depth=1, # 1 means one hidden layer, 2-mlp\n        # scale_range=(0.0001, 0.02),\n        cp=True,\n        radius=0.5, # original 0.5 too\n        norm_layer=nn.LayerNorm,",
        "detail": "dust3r.heads.linear_head",
        "documentation": {}
    },
    {
        "label": "zero_module",
        "kind": 2,
        "importPath": "dust3r.heads.linear_head",
        "description": "dust3r.heads.linear_head",
        "peekOfCode": "def zero_module(module):\n    for p in module.parameters():\n        nn.init.zeros_(p)\n    return module\ndef build_pytorch_mlp(input_dim, hidden_dim, output_dim, depth=10, bias=False):\n    mlp = []\n    mlp.append(nn.Linear(input_dim, hidden_dim, bias=bias))\n    mlp.append(nn.ReLU())\n    for _ in range(depth - 1):\n        mlp.append(nn.Linear(hidden_dim, hidden_dim, bias=bias))",
        "detail": "dust3r.heads.linear_head",
        "documentation": {}
    },
    {
        "label": "build_pytorch_mlp",
        "kind": 2,
        "importPath": "dust3r.heads.linear_head",
        "description": "dust3r.heads.linear_head",
        "peekOfCode": "def build_pytorch_mlp(input_dim, hidden_dim, output_dim, depth=10, bias=False):\n    mlp = []\n    mlp.append(nn.Linear(input_dim, hidden_dim, bias=bias))\n    mlp.append(nn.ReLU())\n    for _ in range(depth - 1):\n        mlp.append(nn.Linear(hidden_dim, hidden_dim, bias=bias))\n        mlp.append(nn.ReLU())\n    mlp.append(nn.Linear(hidden_dim, output_dim, bias=bias))\n    mlp = nn.Sequential(*mlp)\n    return mlp",
        "detail": "dust3r.heads.linear_head",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "kind": 2,
        "importPath": "dust3r.heads.linear_head",
        "description": "dust3r.heads.linear_head",
        "peekOfCode": "def trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):\n    # type: (Tensor, float, float, float, float) -> Tensor\n    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\nclass DownSamplings(nn.Module):\n    def __init__(self, in_channels, out_channels, num_layers=2):\n        super().__init__()\n        self.layers = nn.ModuleList()\n        for i in range(num_layers):\n            in_channels_i = in_channels if i == 0 else out_channels // (2 ** (num_layers - i))\n            out_channels_i = out_channels // (2 ** (num_layers - 1 - i))",
        "detail": "dust3r.heads.linear_head",
        "documentation": {}
    },
    {
        "label": "postprocess",
        "kind": 2,
        "importPath": "dust3r.heads.postprocess",
        "description": "dust3r.heads.postprocess",
        "peekOfCode": "def postprocess(out, depth_mode, conf_mode):\n    \"\"\"\n    extract 3D points/confidence from prediction head output\n    \"\"\"\n    fmap = out.permute(0, 2, 3, 1)  # B,H,W,3\n    res = dict(pts3d=reg_dense_depth(fmap[:, :, :, 0:3], mode=depth_mode))\n    if conf_mode is not None:\n        res['conf'] = reg_dense_conf(fmap[:, :, :, 3], mode=conf_mode)\n    return res\ndef reg_dense_depth(xyz, mode):",
        "detail": "dust3r.heads.postprocess",
        "documentation": {}
    },
    {
        "label": "reg_dense_depth",
        "kind": 2,
        "importPath": "dust3r.heads.postprocess",
        "description": "dust3r.heads.postprocess",
        "peekOfCode": "def reg_dense_depth(xyz, mode):\n    \"\"\"\n    extract 3D points from prediction head output\n    \"\"\"\n    mode, vmin, vmax = mode\n    no_bounds = (vmin == -float('inf')) and (vmax == float('inf'))\n    assert no_bounds\n    if mode == 'linear':\n        if no_bounds:\n            return xyz  # [-inf, +inf]",
        "detail": "dust3r.heads.postprocess",
        "documentation": {}
    },
    {
        "label": "reg_dense_conf",
        "kind": 2,
        "importPath": "dust3r.heads.postprocess",
        "description": "dust3r.heads.postprocess",
        "peekOfCode": "def reg_dense_conf(x, mode):\n    \"\"\"\n    extract confidence from prediction head output\n    \"\"\"\n    mode, vmin, vmax = mode\n    if mode == 'exp':\n        return vmin + x.exp().clip(max=vmax-vmin)\n    if mode == 'sigmoid':\n        return (vmax - vmin) * torch.sigmoid(x) + vmin\n    raise ValueError(f'bad {mode=}')",
        "detail": "dust3r.heads.postprocess",
        "documentation": {}
    },
    {
        "label": "todevice",
        "kind": 2,
        "importPath": "dust3r.utils.device",
        "description": "dust3r.utils.device",
        "peekOfCode": "def todevice(batch, device, callback=None, non_blocking=False):\n    ''' Transfer some variables to another device (i.e. GPU, CPU:torch, CPU:numpy).\n    batch: list, tuple, dict of tensors or other things\n    device: pytorch device or 'numpy'\n    callback: function that would be called on every sub-elements.\n    '''\n    if callback:\n        batch = callback(batch)\n    if isinstance(batch, dict):\n        return {k: todevice(v, device) for k, v in batch.items()}",
        "detail": "dust3r.utils.device",
        "documentation": {}
    },
    {
        "label": "to_numpy",
        "kind": 2,
        "importPath": "dust3r.utils.device",
        "description": "dust3r.utils.device",
        "peekOfCode": "def to_numpy(x): return todevice(x, 'numpy')\ndef to_cpu(x): return todevice(x, 'cpu')\ndef to_cuda(x): return todevice(x, 'cuda')\ndef collate_with_cat(whatever, lists=False):\n    if isinstance(whatever, dict):\n        return {k: collate_with_cat(vals, lists=lists) for k, vals in whatever.items()}\n    elif isinstance(whatever, (tuple, list)):\n        if len(whatever) == 0:\n            return whatever\n        elem = whatever[0]",
        "detail": "dust3r.utils.device",
        "documentation": {}
    },
    {
        "label": "to_cpu",
        "kind": 2,
        "importPath": "dust3r.utils.device",
        "description": "dust3r.utils.device",
        "peekOfCode": "def to_cpu(x): return todevice(x, 'cpu')\ndef to_cuda(x): return todevice(x, 'cuda')\ndef collate_with_cat(whatever, lists=False):\n    if isinstance(whatever, dict):\n        return {k: collate_with_cat(vals, lists=lists) for k, vals in whatever.items()}\n    elif isinstance(whatever, (tuple, list)):\n        if len(whatever) == 0:\n            return whatever\n        elem = whatever[0]\n        T = type(whatever)",
        "detail": "dust3r.utils.device",
        "documentation": {}
    },
    {
        "label": "to_cuda",
        "kind": 2,
        "importPath": "dust3r.utils.device",
        "description": "dust3r.utils.device",
        "peekOfCode": "def to_cuda(x): return todevice(x, 'cuda')\ndef collate_with_cat(whatever, lists=False):\n    if isinstance(whatever, dict):\n        return {k: collate_with_cat(vals, lists=lists) for k, vals in whatever.items()}\n    elif isinstance(whatever, (tuple, list)):\n        if len(whatever) == 0:\n            return whatever\n        elem = whatever[0]\n        T = type(whatever)\n        if elem is None:",
        "detail": "dust3r.utils.device",
        "documentation": {}
    },
    {
        "label": "collate_with_cat",
        "kind": 2,
        "importPath": "dust3r.utils.device",
        "description": "dust3r.utils.device",
        "peekOfCode": "def collate_with_cat(whatever, lists=False):\n    if isinstance(whatever, dict):\n        return {k: collate_with_cat(vals, lists=lists) for k, vals in whatever.items()}\n    elif isinstance(whatever, (tuple, list)):\n        if len(whatever) == 0:\n            return whatever\n        elem = whatever[0]\n        T = type(whatever)\n        if elem is None:\n            return None",
        "detail": "dust3r.utils.device",
        "documentation": {}
    },
    {
        "label": "listify",
        "kind": 2,
        "importPath": "dust3r.utils.device",
        "description": "dust3r.utils.device",
        "peekOfCode": "def listify(elems):\n    return [x for e in elems for x in e]",
        "detail": "dust3r.utils.device",
        "documentation": {}
    },
    {
        "label": "to_device",
        "kind": 5,
        "importPath": "dust3r.utils.device",
        "description": "dust3r.utils.device",
        "peekOfCode": "to_device = todevice  # alias\ndef to_numpy(x): return todevice(x, 'numpy')\ndef to_cpu(x): return todevice(x, 'cpu')\ndef to_cuda(x): return todevice(x, 'cuda')\ndef collate_with_cat(whatever, lists=False):\n    if isinstance(whatever, dict):\n        return {k: collate_with_cat(vals, lists=lists) for k, vals in whatever.items()}\n    elif isinstance(whatever, (tuple, list)):\n        if len(whatever) == 0:\n            return whatever",
        "detail": "dust3r.utils.device",
        "documentation": {}
    },
    {
        "label": "xy_grid",
        "kind": 2,
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "peekOfCode": "def xy_grid(W, H, device=None, origin=(0, 0), unsqueeze=None, cat_dim=-1, homogeneous=False, **arange_kw):\n    \"\"\" Output a (H,W,2) array of int32 \n        with output[j,i,0] = i + origin[0]\n             output[j,i,1] = j + origin[1]\n    \"\"\"\n    if device is None:\n        # numpy\n        arange, meshgrid, stack, ones = np.arange, np.meshgrid, np.stack, np.ones\n    else:\n        # torch",
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "geotrf",
        "kind": 2,
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "peekOfCode": "def geotrf(Trf, pts, ncol=None, norm=False):\n    \"\"\" Apply a geometric transformation to a list of 3-D points.\n    H: 3x3 or 4x4 projection matrix (typically a Homography)\n    p: numpy/torch/tuple of coordinates. Shape must be (...,2) or (...,3)\n    ncol: int. number of columns of the result (2 or 3)\n    norm: float. if != 0, the resut is projected on the z=norm plane.\n    Returns an array of projected 2d points.\n    \"\"\"\n    assert Trf.ndim >= 2\n    if isinstance(Trf, np.ndarray):",
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "inv",
        "kind": 2,
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "peekOfCode": "def inv(mat):\n    \"\"\" Invert a torch or numpy matrix\n    \"\"\"\n    if isinstance(mat, torch.Tensor):\n        return torch.linalg.inv(mat)\n    if isinstance(mat, np.ndarray):\n        return np.linalg.inv(mat)\n    raise ValueError(f'bad matrix type = {type(mat)}')\ndef depthmap_to_pts3d(depth, pseudo_focal, pp=None, **_):\n    \"\"\"",
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "depthmap_to_pts3d",
        "kind": 2,
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "peekOfCode": "def depthmap_to_pts3d(depth, pseudo_focal, pp=None, **_):\n    \"\"\"\n    Args:\n        - depthmap (BxHxW array):\n        - pseudo_focal: [B,H,W] ; [B,2,H,W] or [B,1,H,W]\n    Returns:\n        pointmap of absolute coordinates (BxHxWx3 array)\n    \"\"\"\n    if len(depth.shape) == 4:\n        B, H, W, n = depth.shape",
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "depthmap_to_camera_coordinates",
        "kind": 2,
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "peekOfCode": "def depthmap_to_camera_coordinates(depthmap, camera_intrinsics, pseudo_focal=None):\n    \"\"\"\n    Args:\n        - depthmap (HxW array):\n        - camera_intrinsics: a 3x3 matrix\n    Returns:\n        pointmap of absolute coordinates (HxWx3 array), and a mask specifying valid pixels.\n    \"\"\"\n    camera_intrinsics = np.float32(camera_intrinsics)\n    H, W = depthmap.shape",
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "depthmap_to_absolute_camera_coordinates",
        "kind": 2,
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "peekOfCode": "def depthmap_to_absolute_camera_coordinates(depthmap, camera_intrinsics, camera_pose, **kw): # camera_pose: c2w\n    \"\"\"\n    Args:\n        - depthmap (HxW array):\n        - camera_intrinsics: a 3x3 matrix\n        - camera_pose: a 4x3 or 4x4 cam2world matrix\n    Returns:\n        pointmap of absolute coordinates (HxWx3 array), and a mask specifying valid pixels.\"\"\"\n    X_cam, valid_mask = depthmap_to_camera_coordinates(depthmap, camera_intrinsics)\n    # R_cam2world = np.float32(camera_params[\"R_cam2world\"])",
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "colmap_to_opencv_intrinsics",
        "kind": 2,
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "peekOfCode": "def colmap_to_opencv_intrinsics(K):\n    \"\"\"\n    Modify camera intrinsics to follow a different convention.\n    Coordinates of the center of the top-left pixels are by default:\n    - (0.5, 0.5) in Colmap\n    - (0,0) in OpenCV\n    \"\"\"\n    K = K.copy()\n    K[0, 2] -= 0.5\n    K[1, 2] -= 0.5",
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "opencv_to_colmap_intrinsics",
        "kind": 2,
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "peekOfCode": "def opencv_to_colmap_intrinsics(K):\n    \"\"\"\n    Modify camera intrinsics to follow a different convention.\n    Coordinates of the center of the top-left pixels are by default:\n    - (0.5, 0.5) in Colmap\n    - (0,0) in OpenCV\n    \"\"\"\n    K = K.copy()\n    K[0, 2] += 0.5\n    K[1, 2] += 0.5",
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "normalize_pointcloud",
        "kind": 2,
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "peekOfCode": "def normalize_pointcloud(pts1, pts2, norm_mode='avg_dis', valid1=None, valid2=None):\n    \"\"\" renorm pointmaps pts1, pts2 with norm_mode\n    \"\"\"\n    assert pts1.ndim >= 3 and pts1.shape[-1] == 3\n    assert pts2 is None or (pts2.ndim >= 3 and pts2.shape[-1] == 3)\n    norm_mode, dis_mode = norm_mode.split('_')\n    if norm_mode == 'avg':\n        # gather all points together (joint normalization)\n        nan_pts1, nnz1 = invalid_to_zeros(pts1, valid1, ndim=3)\n        nan_pts2, nnz2 = invalid_to_zeros(pts2, valid2, ndim=3) if pts2 is not None else (None, 0)",
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "normalize_pointclouds",
        "kind": 2,
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "peekOfCode": "def normalize_pointclouds(pts1, pts2s, norm_mode='avg_dis', valid1=None, valid2s=None, return_norm_factor = False):\n    \"\"\" renorm pointmaps pts1, pts2 with norm_mode\n    \"\"\"\n    assert pts1.ndim >= 3 and pts1.shape[-1] == 3\n    assert pts2s[0] is None or (pts2s[0].ndim >= 3 and pts2s[0].shape[-1] == 3)\n    norm_mode, dis_mode = norm_mode.split('_')\n    if norm_mode == 'avg':\n        # gather all points together (joint normalization)\n        nan_pts1, nnz1 = invalid_to_zeros(pts1, valid1, ndim=3)\n        nan_pts2_nnz2s = [invalid_to_zeros(pts2, valid2, ndim=3) if pts2 is not None else (None, 0) for (pts2, valid2) in zip(pts2s, valid2s)]",
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "get_joint_pointcloud_depth",
        "kind": 2,
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "peekOfCode": "def get_joint_pointcloud_depth(z1, z2, valid_mask1, valid_mask2=None, quantile=0.5):\n    # set invalid points to NaN\n    _z1 = invalid_to_nans(z1, valid_mask1).reshape(len(z1), -1)\n    _z2 = invalid_to_nans(z2, valid_mask2).reshape(len(z2), -1) if z2 is not None else None\n    _z = torch.cat((_z1, _z2), dim=-1) if z2 is not None else _z1\n    # compute median depth overall (ignoring nans)\n    if quantile == 0.5:\n        shift_z = torch.nanmedian(_z, dim=-1).values\n    else:\n        shift_z = torch.nanquantile(_z, quantile, dim=-1)",
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "get_joint_pointcloud_depths",
        "kind": 2,
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "peekOfCode": "def get_joint_pointcloud_depths(z1, z2s, valid_mask1, valid_mask2s=None, quantile=0.5):\n    # set invalid points to NaN\n    _z1 = invalid_to_nans(z1, valid_mask1).reshape(len(z1), -1)\n    _z2s = [invalid_to_nans(z2, valid_mask2).reshape(len(z2), -1) if z2 is not None else None for (z2, valid_mask2) in zip(z2s, valid_mask2s)]\n    _z = torch.cat((_z1, *_z2s), dim=-1) if z2s is not None else _z1\n    # compute median depth overall (ignoring nans)\n    if quantile == 0.5:\n        shift_z = torch.nanmedian(_z, dim=-1).values\n    else:\n        shift_z = torch.nanquantile(_z, quantile, dim=-1)",
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "get_joint_pointcloud_center_scale",
        "kind": 2,
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "peekOfCode": "def get_joint_pointcloud_center_scale(pts1, pts2, valid_mask1=None, valid_mask2=None, z_only=False, center=True):\n    # set invalid points to NaN\n    _pts1 = invalid_to_nans(pts1, valid_mask1).reshape(len(pts1), -1, 3)\n    _pts2 = invalid_to_nans(pts2, valid_mask2).reshape(len(pts2), -1, 3) if pts2 is not None else None\n    _pts = torch.cat((_pts1, _pts2), dim=1) if pts2 is not None else _pts1\n    # compute median center\n    _center = torch.nanmedian(_pts, dim=1, keepdim=True).values  # (B,1,3)\n    if z_only:\n        _center[..., :2] = 0  # do not center X and Y\n    # compute median norm",
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "get_joint_pointcloud_center_scales",
        "kind": 2,
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "peekOfCode": "def get_joint_pointcloud_center_scales(pts1, pts2s, valid_mask1=None, valid_mask2s=None, z_only=False, center=True):\n    # set invalid points to NaN\n    _pts1 = invalid_to_nans(pts1, valid_mask1).reshape(len(pts1), -1, 3)\n    _pts2s = [invalid_to_nans(pts2, valid_mask2).reshape(len(pts2), -1, 3) if pts2 is not None else None for (pts2, valid_mask2) in zip(pts2s, valid_mask2s)]\n    _pts = torch.cat((_pts1, *_pts2s), dim=1) if pts2s is not None else _pts1\n    # compute median center\n    _center = torch.nanmedian(_pts, dim=1, keepdim=True).values  # (B,1,3)\n    if z_only:\n        _center[..., :2] = 0  # do not center X and Y\n    # compute median norm",
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "find_reciprocal_matches",
        "kind": 2,
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "peekOfCode": "def find_reciprocal_matches(P1, P2):\n    \"\"\"\n    returns 3 values:\n    1 - reciprocal_in_P2: a boolean array of size P2.shape[0], a \"True\" value indicates a match\n    2 - nn2_in_P1: a int array of size P2.shape[0], it contains the indexes of the closest points in P1\n    3 - reciprocal_in_P2.sum(): the number of matches\n    \"\"\"\n    tree1 = KDTree(P1)\n    tree2 = KDTree(P2)\n    _, nn1_in_P2 = tree2.query(P1, workers=8)",
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "get_med_dist_between_poses",
        "kind": 2,
        "importPath": "dust3r.utils.geometry",
        "description": "dust3r.utils.geometry",
        "peekOfCode": "def get_med_dist_between_poses(poses):\n    from scipy.spatial.distance import pdist\n    return np.median(pdist([to_numpy(p[:3, 3]) for p in poses]))",
        "detail": "dust3r.utils.geometry",
        "documentation": {}
    },
    {
        "label": "imread_cv2",
        "kind": 2,
        "importPath": "dust3r.utils.image",
        "description": "dust3r.utils.image",
        "peekOfCode": "def imread_cv2(path, options=cv2.IMREAD_COLOR):\n    \"\"\" Open an image or a depthmap with opencv-python.\n    \"\"\"\n    if path.endswith(('.exr', 'EXR')):\n        options = cv2.IMREAD_ANYDEPTH\n    img = cv2.imread(path, options)\n    if img is None:\n        raise IOError(f'Could not load image={path} with {options=}')\n    if img.ndim == 3:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)",
        "detail": "dust3r.utils.image",
        "documentation": {}
    },
    {
        "label": "rgb",
        "kind": 2,
        "importPath": "dust3r.utils.image",
        "description": "dust3r.utils.image",
        "peekOfCode": "def rgb(ftensor, true_shape=None):\n    if isinstance(ftensor, list):\n        return [rgb(x, true_shape=true_shape) for x in ftensor]\n    if isinstance(ftensor, torch.Tensor):\n        ftensor = ftensor.detach().cpu().numpy()  # H,W,3\n    if ftensor.ndim == 3 and ftensor.shape[0] == 3:\n        ftensor = ftensor.transpose(1, 2, 0)\n    elif ftensor.ndim == 4 and ftensor.shape[1] == 3:\n        ftensor = ftensor.transpose(0, 2, 3, 1)\n    if true_shape is not None:",
        "detail": "dust3r.utils.image",
        "documentation": {}
    },
    {
        "label": "load_images",
        "kind": 2,
        "importPath": "dust3r.utils.image",
        "description": "dust3r.utils.image",
        "peekOfCode": "def load_images(folder_or_list, size, square_ok=False, verbose=True, n_frame = 10):\n    \"\"\" open and convert all images in a list or folder to proper input format for DUSt3R\n    \"\"\"\n    if isinstance(folder_or_list, str):\n        if verbose:\n            print(f'>> Loading images from {folder_or_list}')\n        root, folder_content = folder_or_list, sorted(os.listdir(folder_or_list))\n    elif isinstance(folder_or_list, list):\n        if verbose:\n            print(f'>> Loading a list of {len(folder_or_list)} images')",
        "detail": "dust3r.utils.image",
        "documentation": {}
    },
    {
        "label": "os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"]",
        "kind": 5,
        "importPath": "dust3r.utils.image",
        "description": "dust3r.utils.image",
        "peekOfCode": "os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"] = \"1\"\nimport cv2  # noqa\ntry:\n    from pillow_heif import register_heif_opener  # noqa\n    register_heif_opener()\n    heif_support_enabled = True\nexcept ImportError:\n    heif_support_enabled = False\nImgNorm = tvf.Compose([tvf.ToTensor(), tvf.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\ndef imread_cv2(path, options=cv2.IMREAD_COLOR):",
        "detail": "dust3r.utils.image",
        "documentation": {}
    },
    {
        "label": "ImgNorm",
        "kind": 5,
        "importPath": "dust3r.utils.image",
        "description": "dust3r.utils.image",
        "peekOfCode": "ImgNorm = tvf.Compose([tvf.ToTensor(), tvf.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\ndef imread_cv2(path, options=cv2.IMREAD_COLOR):\n    \"\"\" Open an image or a depthmap with opencv-python.\n    \"\"\"\n    if path.endswith(('.exr', 'EXR')):\n        options = cv2.IMREAD_ANYDEPTH\n    img = cv2.imread(path, options)\n    if img is None:\n        raise IOError(f'Could not load image={path} with {options=}')\n    if img.ndim == 3:",
        "detail": "dust3r.utils.image",
        "documentation": {}
    },
    {
        "label": "fill_default_args",
        "kind": 2,
        "importPath": "dust3r.utils.misc",
        "description": "dust3r.utils.misc",
        "peekOfCode": "def fill_default_args(kwargs, func):\n    import inspect  # a bit hacky but it works reliably\n    signature = inspect.signature(func)\n    for k, v in signature.parameters.items():\n        if v.default is inspect.Parameter.empty:\n            continue\n        kwargs.setdefault(k, v.default)\n    return kwargs\ndef freeze_all_params(modules):\n    for module in modules:",
        "detail": "dust3r.utils.misc",
        "documentation": {}
    },
    {
        "label": "freeze_all_params",
        "kind": 2,
        "importPath": "dust3r.utils.misc",
        "description": "dust3r.utils.misc",
        "peekOfCode": "def freeze_all_params(modules):\n    for module in modules:\n        try:\n            for n, param in module.named_parameters():\n                param.requires_grad = False\n        except AttributeError:\n            # module is directly a parameter\n            module.requires_grad = False\ndef is_symmetrized(gt1, gt2):\n    x = gt1['instance']",
        "detail": "dust3r.utils.misc",
        "documentation": {}
    },
    {
        "label": "is_symmetrized",
        "kind": 2,
        "importPath": "dust3r.utils.misc",
        "description": "dust3r.utils.misc",
        "peekOfCode": "def is_symmetrized(gt1, gt2):\n    x = gt1['instance']\n    y = gt2['instance']\n    if len(x) == len(y) and len(x) == 1:\n        return False  # special case of batchsize 1\n    ok = True\n    for i in range(0, len(x), 2):\n        ok = ok and (x[i] == y[i+1]) and (x[i+1] == y[i])\n    return ok\ndef flip(tensor):",
        "detail": "dust3r.utils.misc",
        "documentation": {}
    },
    {
        "label": "flip",
        "kind": 2,
        "importPath": "dust3r.utils.misc",
        "description": "dust3r.utils.misc",
        "peekOfCode": "def flip(tensor):\n    \"\"\" flip so that tensor[0::2] <=> tensor[1::2] \"\"\"\n    return torch.stack((tensor[1::2], tensor[0::2]), dim=1).flatten(0, 1)\ndef interleave(tensor1, tensor2):\n    res1 = torch.stack((tensor1, tensor2), dim=1).flatten(0, 1)\n    res2 = torch.stack((tensor2, tensor1), dim=1).flatten(0, 1)\n    return res1, res2\ndef transpose_to_landscape(head, activate=True):\n    \"\"\" Predict in the correct aspect-ratio,\n        then transpose the result in landscape ",
        "detail": "dust3r.utils.misc",
        "documentation": {}
    },
    {
        "label": "interleave",
        "kind": 2,
        "importPath": "dust3r.utils.misc",
        "description": "dust3r.utils.misc",
        "peekOfCode": "def interleave(tensor1, tensor2):\n    res1 = torch.stack((tensor1, tensor2), dim=1).flatten(0, 1)\n    res2 = torch.stack((tensor2, tensor1), dim=1).flatten(0, 1)\n    return res1, res2\ndef transpose_to_landscape(head, activate=True):\n    \"\"\" Predict in the correct aspect-ratio,\n        then transpose the result in landscape \n        and stack everything back together.\n    \"\"\"\n    def wrapper_no(decout, true_shape):",
        "detail": "dust3r.utils.misc",
        "documentation": {}
    },
    {
        "label": "transpose_to_landscape",
        "kind": 2,
        "importPath": "dust3r.utils.misc",
        "description": "dust3r.utils.misc",
        "peekOfCode": "def transpose_to_landscape(head, activate=True):\n    \"\"\" Predict in the correct aspect-ratio,\n        then transpose the result in landscape \n        and stack everything back together.\n    \"\"\"\n    def wrapper_no(decout, true_shape):\n        B = len(true_shape)\n        assert true_shape[0:1].allclose(true_shape), 'true_shape must be all identical'\n        H, W = true_shape[0].cpu().tolist()\n        res = head(decout, (H, W))",
        "detail": "dust3r.utils.misc",
        "documentation": {}
    },
    {
        "label": "transposed",
        "kind": 2,
        "importPath": "dust3r.utils.misc",
        "description": "dust3r.utils.misc",
        "peekOfCode": "def transposed(dic):\n    return {k: v.swapaxes(1, 2) for k, v in dic.items()}\ndef invalid_to_nans(arr, valid_mask, ndim=999):\n    if valid_mask is not None:\n        arr = arr.clone()\n        arr[~valid_mask] = float('nan')\n    if arr.ndim > ndim:\n        arr = arr.flatten(-2 - (arr.ndim - ndim), -2)\n    return arr\ndef invalid_to_zeros(arr, valid_mask, ndim=999):",
        "detail": "dust3r.utils.misc",
        "documentation": {}
    },
    {
        "label": "invalid_to_nans",
        "kind": 2,
        "importPath": "dust3r.utils.misc",
        "description": "dust3r.utils.misc",
        "peekOfCode": "def invalid_to_nans(arr, valid_mask, ndim=999):\n    if valid_mask is not None:\n        arr = arr.clone()\n        arr[~valid_mask] = float('nan')\n    if arr.ndim > ndim:\n        arr = arr.flatten(-2 - (arr.ndim - ndim), -2)\n    return arr\ndef invalid_to_zeros(arr, valid_mask, ndim=999):\n    if valid_mask is not None:\n        arr = arr.clone()",
        "detail": "dust3r.utils.misc",
        "documentation": {}
    },
    {
        "label": "invalid_to_zeros",
        "kind": 2,
        "importPath": "dust3r.utils.misc",
        "description": "dust3r.utils.misc",
        "peekOfCode": "def invalid_to_zeros(arr, valid_mask, ndim=999):\n    if valid_mask is not None:\n        arr = arr.clone()\n        arr[~valid_mask] = 0\n        nnz = valid_mask.view(len(valid_mask), -1).sum(1)\n        # print('valid_mask not none', nnz)\n    else:\n        nnz = arr.numel() // len(arr) if len(arr) else 0  # number of point per image\n    if arr.ndim > ndim:\n        arr = arr.flatten(-2 - (arr.ndim - ndim), -2)",
        "detail": "dust3r.utils.misc",
        "documentation": {}
    },
    {
        "label": "HERE_PATH",
        "kind": 5,
        "importPath": "dust3r.utils.path_to_croco",
        "description": "dust3r.utils.path_to_croco",
        "peekOfCode": "HERE_PATH = path.normpath(path.dirname(__file__))\nCROCO_REPO_PATH = path.normpath(path.join(HERE_PATH, '../../croco'))\nCROCO_MODELS_PATH = path.join(CROCO_REPO_PATH, 'models')\n# check the presence of models directory in repo to be sure its cloned\nif path.isdir(CROCO_MODELS_PATH):\n    # workaround for sibling import\n    sys.path.insert(0, CROCO_REPO_PATH)\nelse:\n    raise ImportError(f\"croco is not initialized, could not find: {CROCO_MODELS_PATH}.\\n \"\n                      \"Did you forget to run 'git submodule update --init --recursive' ?\")",
        "detail": "dust3r.utils.path_to_croco",
        "documentation": {}
    },
    {
        "label": "CROCO_REPO_PATH",
        "kind": 5,
        "importPath": "dust3r.utils.path_to_croco",
        "description": "dust3r.utils.path_to_croco",
        "peekOfCode": "CROCO_REPO_PATH = path.normpath(path.join(HERE_PATH, '../../croco'))\nCROCO_MODELS_PATH = path.join(CROCO_REPO_PATH, 'models')\n# check the presence of models directory in repo to be sure its cloned\nif path.isdir(CROCO_MODELS_PATH):\n    # workaround for sibling import\n    sys.path.insert(0, CROCO_REPO_PATH)\nelse:\n    raise ImportError(f\"croco is not initialized, could not find: {CROCO_MODELS_PATH}.\\n \"\n                      \"Did you forget to run 'git submodule update --init --recursive' ?\")",
        "detail": "dust3r.utils.path_to_croco",
        "documentation": {}
    },
    {
        "label": "CROCO_MODELS_PATH",
        "kind": 5,
        "importPath": "dust3r.utils.path_to_croco",
        "description": "dust3r.utils.path_to_croco",
        "peekOfCode": "CROCO_MODELS_PATH = path.join(CROCO_REPO_PATH, 'models')\n# check the presence of models directory in repo to be sure its cloned\nif path.isdir(CROCO_MODELS_PATH):\n    # workaround for sibling import\n    sys.path.insert(0, CROCO_REPO_PATH)\nelse:\n    raise ImportError(f\"croco is not initialized, could not find: {CROCO_MODELS_PATH}.\\n \"\n                      \"Did you forget to run 'git submodule update --init --recursive' ?\")",
        "detail": "dust3r.utils.path_to_croco",
        "documentation": {}
    },
    {
        "label": "global_variable",
        "kind": 6,
        "importPath": "dust3r.dummy_io",
        "description": "dust3r.dummy_io",
        "peekOfCode": "class global_variable:\n    train_name_list_path = \"./data/train_name_list.json\"\n    metadata_dir = \"./metadata\"\ndef get_local_path(x):\n    return x\n2\ndef set_device(x):\n    return\nclass g_pathmgr:\n    open=open",
        "detail": "dust3r.dummy_io",
        "documentation": {}
    },
    {
        "label": "g_pathmgr",
        "kind": 6,
        "importPath": "dust3r.dummy_io",
        "description": "dust3r.dummy_io",
        "peekOfCode": "class g_pathmgr:\n    open=open\n    get_local_path=get_local_path\n    exists=os.path.exists\n    @classmethod\n    def mkdirs(cls, value):\n        return os.makedirs(value, exist_ok=True)\n    @classmethod\n    def isfile(cls, value):\n        return os.path.isfile(value)",
        "detail": "dust3r.dummy_io",
        "documentation": {}
    },
    {
        "label": "get_local_path",
        "kind": 2,
        "importPath": "dust3r.dummy_io",
        "description": "dust3r.dummy_io",
        "peekOfCode": "def get_local_path(x):\n    return x\n2\ndef set_device(x):\n    return\nclass g_pathmgr:\n    open=open\n    get_local_path=get_local_path\n    exists=os.path.exists\n    @classmethod",
        "detail": "dust3r.dummy_io",
        "documentation": {}
    },
    {
        "label": "set_device",
        "kind": 2,
        "importPath": "dust3r.dummy_io",
        "description": "dust3r.dummy_io",
        "peekOfCode": "def set_device(x):\n    return\nclass g_pathmgr:\n    open=open\n    get_local_path=get_local_path\n    exists=os.path.exists\n    @classmethod\n    def mkdirs(cls, value):\n        return os.makedirs(value, exist_ok=True)\n    @classmethod",
        "detail": "dust3r.dummy_io",
        "documentation": {}
    },
    {
        "label": "get_log_dir_warp",
        "kind": 2,
        "importPath": "dust3r.dummy_io",
        "description": "dust3r.dummy_io",
        "peekOfCode": "def get_log_dir_warp(output_dir):\n    return output_dir\ndef change_to_sr(file_lists):\n    return file_lists\ndef save_image_manifold(img, tgt):\n    import imageio\n    imageio.imwrite(tgt, img)",
        "detail": "dust3r.dummy_io",
        "documentation": {}
    },
    {
        "label": "change_to_sr",
        "kind": 2,
        "importPath": "dust3r.dummy_io",
        "description": "dust3r.dummy_io",
        "peekOfCode": "def change_to_sr(file_lists):\n    return file_lists\ndef save_image_manifold(img, tgt):\n    import imageio\n    imageio.imwrite(tgt, img)",
        "detail": "dust3r.dummy_io",
        "documentation": {}
    },
    {
        "label": "save_image_manifold",
        "kind": 2,
        "importPath": "dust3r.dummy_io",
        "description": "dust3r.dummy_io",
        "peekOfCode": "def save_image_manifold(img, tgt):\n    import imageio\n    imageio.imwrite(tgt, img)",
        "detail": "dust3r.dummy_io",
        "documentation": {}
    },
    {
        "label": "GaussianRenderer",
        "kind": 6,
        "importPath": "dust3r.gs",
        "description": "dust3r.gs",
        "peekOfCode": "class GaussianRenderer(nn.Module):\n    def __init__(self, im_height = 224, im_width = 224, znear=0.01, zfar=100.0):\n        super().__init__()\n        self.im_height = int(im_height)\n        self.im_width = int(im_width)\n        self.znear = znear\n        self.zfar = zfar\n        self.register_buffer(\"bg_color\", torch.ones((1, 3), dtype=torch.float32))\n    def set_view_info(self, height=0, width=0, znear=0.01, zfar=100.0):\n        self.im_height = int(height)",
        "detail": "dust3r.gs",
        "documentation": {}
    },
    {
        "label": "gs_render",
        "kind": 2,
        "importPath": "dust3r.gs",
        "description": "dust3r.gs",
        "peekOfCode": "def gs_render(gts, preds, dp_id_gt, dp_id_pred, c2w_canonical, normalize = False, rot = True, gt_img = False, gt_pcd = False):\n    # gt1, gt2s, pred1, pred2s = gts[0], gts[1:], preds[0], preds[1:]\n    # gt_pts1, gt_pts2s, pr_pts1, pr_pts2s, c2ws = torch.load('/home/zgtang/others.pt')\n    # c2ws = torch.stack([c2w[dp_id] for c2w in c2ws], 0).cuda()\n    # c2ws = torch.stack([gt1['camera_pose'][dp_id]] + [gt2['camera_pose'][dp_id] for gt2 in gt2s], 0).cuda() # single: [4,4]\n    intrinsics = torch.stack([gt['camera_intrinsics'][dp_id_gt][:3,:3] for gt in gts]).cuda() # 3,3\n    rot_gs = torch.stack([pred['rotation'][dp_id_pred] for pred in preds], 0).reshape(-1, 4)\n    scale_gs = torch.stack([pred['scale'][dp_id_pred] for pred in preds], 0).reshape(-1, 3)\n    opacity_gs = torch.stack([pred['opacity'][dp_id_pred] for pred in preds], 0).reshape(-1)\n    if gt_pcd:",
        "detail": "dust3r.gs",
        "documentation": {}
    },
    {
        "label": "make_pairs",
        "kind": 2,
        "importPath": "dust3r.image_pairs",
        "description": "dust3r.image_pairs",
        "peekOfCode": "def make_pairs(imgs, scene_graph='complete', prefilter=None, symmetrize=True):\n    pairs = []\n    if scene_graph == 'complete':  # complete graph\n        for i in range(len(imgs)):\n            for j in range(i):\n                pairs.append((imgs[i], imgs[j]))\n    elif scene_graph.startswith('swin'):\n        winsize = int(scene_graph.split('-')[1]) if '-' in scene_graph else 3\n        pairsid = set()\n        for i in range(len(imgs)):",
        "detail": "dust3r.image_pairs",
        "documentation": {}
    },
    {
        "label": "sel",
        "kind": 2,
        "importPath": "dust3r.image_pairs",
        "description": "dust3r.image_pairs",
        "peekOfCode": "def sel(x, kept):\n    if isinstance(x, dict):\n        return {k: sel(v, kept) for k, v in x.items()}\n    if isinstance(x, (torch.Tensor, np.ndarray)):\n        return x[kept]\n    if isinstance(x, (tuple, list)):\n        return type(x)([x[k] for k in kept])\ndef _filter_edges_seq(edges, seq_dis_thr, cyclic=False):\n    # number of images\n    n = max(max(e) for e in edges)+1",
        "detail": "dust3r.image_pairs",
        "documentation": {}
    },
    {
        "label": "filter_pairs_seq",
        "kind": 2,
        "importPath": "dust3r.image_pairs",
        "description": "dust3r.image_pairs",
        "peekOfCode": "def filter_pairs_seq(pairs, seq_dis_thr, cyclic=False):\n    edges = [(img1['idx'], img2['idx']) for img1, img2 in pairs]\n    kept = _filter_edges_seq(edges, seq_dis_thr, cyclic=cyclic)\n    return [pairs[i] for i in kept]\ndef filter_edges_seq(view1, view2, pred1, pred2, seq_dis_thr, cyclic=False):\n    edges = [(int(i), int(j)) for i, j in zip(view1['idx'], view2['idx'])]\n    kept = _filter_edges_seq(edges, seq_dis_thr, cyclic=cyclic)\n    print(f'>> Filtering edges more than {seq_dis_thr} frames apart: kept {len(kept)}/{len(edges)} edges')\n    return sel(view1, kept), sel(view2, kept), sel(pred1, kept), sel(pred2, kept)",
        "detail": "dust3r.image_pairs",
        "documentation": {}
    },
    {
        "label": "filter_edges_seq",
        "kind": 2,
        "importPath": "dust3r.image_pairs",
        "description": "dust3r.image_pairs",
        "peekOfCode": "def filter_edges_seq(view1, view2, pred1, pred2, seq_dis_thr, cyclic=False):\n    edges = [(int(i), int(j)) for i, j in zip(view1['idx'], view2['idx'])]\n    kept = _filter_edges_seq(edges, seq_dis_thr, cyclic=cyclic)\n    print(f'>> Filtering edges more than {seq_dis_thr} frames apart: kept {len(kept)}/{len(edges)} edges')\n    return sel(view1, kept), sel(view2, kept), sel(pred1, kept), sel(pred2, kept)",
        "detail": "dust3r.image_pairs",
        "documentation": {}
    },
    {
        "label": "make_batch_symmetric",
        "kind": 2,
        "importPath": "dust3r.inference",
        "description": "dust3r.inference",
        "peekOfCode": "def make_batch_symmetric(batch):\n    view1, view2 = batch\n    view1, view2 = (_interleave_imgs(view1, view2), _interleave_imgs(view2, view1))\n    return view1, view2\ndef loss_of_one_batch_mv(batch, model, criterion, device, symmetrize_batch=False, use_amp=False, ret=None, log = True):\n    views = batch\n    view1, view2s = views[0], views[1:]\n    for view in batch:\n        for name in 'img pts3d valid_mask camera_pose camera_intrinsics F_matrix corres'.split():  # pseudo_focal\n            if name not in view:",
        "detail": "dust3r.inference",
        "documentation": {}
    },
    {
        "label": "loss_of_one_batch_mv",
        "kind": 2,
        "importPath": "dust3r.inference",
        "description": "dust3r.inference",
        "peekOfCode": "def loss_of_one_batch_mv(batch, model, criterion, device, symmetrize_batch=False, use_amp=False, ret=None, log = True):\n    views = batch\n    view1, view2s = views[0], views[1:]\n    for view in batch:\n        for name in 'img pts3d valid_mask camera_pose camera_intrinsics F_matrix corres'.split():  # pseudo_focal\n            if name not in view:\n                continue\n            view[name] = view[name].to(device, non_blocking=True)\n    with torch.cuda.amp.autocast(enabled=bool(use_amp)):\n        t = time.time()",
        "detail": "dust3r.inference",
        "documentation": {}
    },
    {
        "label": "loss_of_one_batch",
        "kind": 2,
        "importPath": "dust3r.inference",
        "description": "dust3r.inference",
        "peekOfCode": "def loss_of_one_batch(batch, model, criterion, device, symmetrize_batch=False, use_amp=False, ret=None, log = True):\n    if criterion is not None and criterion.mv:\n        return loss_of_one_batch_mv(batch, model, criterion, device, False, use_amp, ret, log)\n    view1, view2 = batch\n    for view in batch:\n        for name in 'img pts3d valid_mask camera_pose camera_intrinsics F_matrix corres'.split():  # pseudo_focal\n            if name not in view:\n                continue\n            view[name] = view[name].to(device, non_blocking=True)\n    if symmetrize_batch:",
        "detail": "dust3r.inference",
        "documentation": {}
    },
    {
        "label": "inference_mv",
        "kind": 2,
        "importPath": "dust3r.inference",
        "description": "dust3r.inference",
        "peekOfCode": "def inference_mv(batch, model, device, verbose=True):\n    if verbose:\n        print(f'>> Inference with model on {len(batch)} images')\n    result = []\n    res = loss_of_one_batch_mv(batch, model, None, device, log = True)\n    result.append(to_cpu(res))\n    result = collate_with_cat(result, lists=False)\n    return result\n@torch.no_grad()\ndef inference(pairs, model, device, batch_size=8, verbose=True):",
        "detail": "dust3r.inference",
        "documentation": {}
    },
    {
        "label": "inference",
        "kind": 2,
        "importPath": "dust3r.inference",
        "description": "dust3r.inference",
        "peekOfCode": "def inference(pairs, model, device, batch_size=8, verbose=True):\n    if verbose:\n        print(f'>> Inference with model on {len(pairs)} image pairs')\n    result = []\n    # first, check if all images have the same size\n    multiple_shapes = not (check_if_same_size(pairs))\n    if multiple_shapes:  # force bs=1\n        batch_size = 1\n    for i in tqdm.trange(0, len(pairs), batch_size, disable=not verbose):\n        res = loss_of_one_batch(collate_with_cat(pairs[i:i+batch_size]), model, None, device)",
        "detail": "dust3r.inference",
        "documentation": {}
    },
    {
        "label": "check_if_same_size",
        "kind": 2,
        "importPath": "dust3r.inference",
        "description": "dust3r.inference",
        "peekOfCode": "def check_if_same_size(pairs):\n    shapes1 = [img1['img'].shape[-2:] for img1, img2 in pairs]\n    shapes2 = [img2['img'].shape[-2:] for img1, img2 in pairs]\n    return all(shapes1[0] == s for s in shapes1) and all(shapes2[0] == s for s in shapes2)\ndef get_pred_pts3d(gt, pred, use_pose=False):\n    if 'depth' in pred and 'pseudo_focal' in pred:\n        try:\n            pp = gt['camera_intrinsics'][..., :2, 2]\n        except KeyError:\n            pp = None",
        "detail": "dust3r.inference",
        "documentation": {}
    },
    {
        "label": "get_pred_pts3d",
        "kind": 2,
        "importPath": "dust3r.inference",
        "description": "dust3r.inference",
        "peekOfCode": "def get_pred_pts3d(gt, pred, use_pose=False):\n    if 'depth' in pred and 'pseudo_focal' in pred:\n        try:\n            pp = gt['camera_intrinsics'][..., :2, 2]\n        except KeyError:\n            pp = None\n        pts3d = depthmap_to_pts3d(**pred, pp=pp)\n    elif 'pts3d' in pred:\n        # pts3d from my camera\n        pts3d = pred['pts3d']",
        "detail": "dust3r.inference",
        "documentation": {}
    },
    {
        "label": "find_opt_scaling",
        "kind": 2,
        "importPath": "dust3r.inference",
        "description": "dust3r.inference",
        "peekOfCode": "def find_opt_scaling(gt_pts1, gt_pts2, pr_pts1, pr_pts2=None, fit_mode='weiszfeld_stop_grad', valid1=None, valid2=None):\n    assert gt_pts1.ndim == pr_pts1.ndim == 4\n    assert gt_pts1.shape == pr_pts1.shape\n    if gt_pts2 is not None:\n        assert gt_pts2.ndim == pr_pts2.ndim == 4\n        assert gt_pts2.shape == pr_pts2.shape\n    # concat the pointcloud\n    nan_gt_pts1 = invalid_to_nans(gt_pts1, valid1).flatten(1, 2)\n    nan_gt_pts2 = invalid_to_nans(gt_pts2, valid2).flatten(1, 2) if gt_pts2 is not None else None\n    pr_pts1 = invalid_to_nans(pr_pts1, valid1).flatten(1, 2)",
        "detail": "dust3r.inference",
        "documentation": {}
    },
    {
        "label": "LLos",
        "kind": 6,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "class LLoss (nn.Module):\n    \"\"\" L-norm loss\n    \"\"\"\n    def __init__(self, reduction='mean'):\n        super().__init__()\n        self.reduction = reduction\n    def forward(self, a, b, mask = None, reduction=None):\n        if mask is not None:\n            dist = self.distance(a, b)\n            assert reduction == \"mean_bs\"",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "L21Los",
        "kind": 6,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "class L21Loss (LLoss):\n    \"\"\" Euclidean distance between 3d points  \"\"\"\n    def distance(self, a, b):\n        return torch.norm(a - b, dim=-1)  # normalized L2 distance\nL21 = L21Loss()\nclass Criterion (nn.Module):\n    def __init__(self, criterion=None):\n        super().__init__()\n        assert isinstance(criterion, LLoss), f'{criterion} is not a proper criterion!'+bb()\n        self.criterion = copy(criterion)",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "Criterio",
        "kind": 6,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "class Criterion (nn.Module):\n    def __init__(self, criterion=None):\n        super().__init__()\n        assert isinstance(criterion, LLoss), f'{criterion} is not a proper criterion!'+bb()\n        self.criterion = copy(criterion)\n    def get_name(self):\n        return f'{type(self).__name__}({self.criterion})'\n    def with_reduction(self, mode):\n        res = loss = deepcopy(self)\n        while loss is not None:",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "MultiLos",
        "kind": 6,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "class MultiLoss (nn.Module):\n    \"\"\" Easily combinable losses (also keep track of individual loss values):\n        loss = MyLoss1() + 0.1*MyLoss2()\n    Usage:\n        Inherit from this class and override get_name() and compute_loss()\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self._alpha = 1\n        self._loss2 = None",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "Regr3",
        "kind": 6,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "class Regr3D (Criterion, MultiLoss):\n    \"\"\" Ensure that all 3D points are correct.\n        Asymmetric loss: view1 is supposed to be the anchor.\n        P1 = RT1 @ D1\n        P2 = RT2 @ D2\n        loss1 = (I @ pred_D1) - (RT1^-1 @ RT1 @ D1)\n        loss2 = (RT21 @ pred_D2) - (RT1^-1 @ P2)\n              = (RT21 @ pred_D2) - (RT1^-1 @ RT2 @ D2)\n    \"\"\"\n    def __init__(self, criterion, norm_mode='avg_dis', gt_scale=False, mv = False, rot_invariant = False, dummy = False):",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "ConfLos",
        "kind": 6,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "class ConfLoss (MultiLoss):\n    \"\"\" Weighted regression by learned confidence.\n        Assuming the input pixel_loss is a pixel-level regression loss.\n    Principle:\n        high-confidence means high conf = 0.1 ==> conf_loss = x / 10 + alpha*log(10)\n        low  confidence means low  conf = 10  ==> conf_loss = x * 10 - alpha*log(10) \n        alpha: hyperparameter\n    \"\"\"\n    def __init__(self, pixel_loss, alpha=1):\n        super().__init__()",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "Regr3D_ShiftIn",
        "kind": 6,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "class Regr3D_ShiftInv (Regr3D): # first this\n    \"\"\" Same than Regr3D but invariant to depth shift.\n    \"\"\"\n    def get_all_pts3d(self, gt1, gt2, pred1, pred2):\n        if self.mv:\n            return self.get_all_pts3ds(gt1, gt2, pred1, pred2)\n        # compute unnormalized points\n        gt_pts1, gt_pts2, pred_pts1, pred_pts2, mask1, mask2, monitoring = \\\n            super().get_all_pts3d(gt1, gt2, pred1, pred2)\n        # compute median depth",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "Regr3D_ShiftAllIn",
        "kind": 6,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "class Regr3D_ShiftAllInv (Regr3D): # first this\n    \"\"\" Same than Regr3D but invariant to shift of xyz (center to original)\n    \"\"\"\n    def get_all_pts3ds(self, gt1, gt2s, pred1, pred2s, **kw):\n        # compute unnormalized points\n        gt_pts1, gt_pts2s, pred_pts1, pred_pts2s, mask1, mask2s, monitoring = \\\n            super().get_all_pts3ds(gt1, gt2s, pred1, pred2s)\n        for coor_id in range(3):\n            # compute median depth\n            gt_z1, gt_z2s = gt_pts1[..., coor_id], [gt_pts2[..., coor_id] for gt_pts2 in gt_pts2s]",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "Regr3D_ScaleIn",
        "kind": 6,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "class Regr3D_ScaleInv (Regr3D): # then this\n    \"\"\" Same than Regr3D but invariant to depth shift.\n        if gt_scale == True: enforce the prediction to take the same scale than GT\n    \"\"\"\n    def get_all_pts3d(self, gt1, gt2, pred1, pred2):\n        # compute depth-normalized points\n        gt_pts1, gt_pts2, pred_pts1, pred_pts2, mask1, mask2, monitoring = super().get_all_pts3d(gt1, gt2, pred1, pred2)\n        # measure scene scale\n        _, gt_scale = get_joint_pointcloud_center_scale(gt_pts1, gt_pts2, mask1, mask2)\n        _, pred_scale = get_joint_pointcloud_center_scale(pred_pts1, pred_pts2, mask1, mask2)",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "Regr3D_ScaleShiftIn",
        "kind": 6,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "class Regr3D_ScaleShiftInv (Regr3D_ScaleInv, Regr3D_ShiftInv):\n    # calls Regr3D_ShiftInv first, then Regr3D_ScaleInv\n    pass\nclass Regr3D_ScaleShiftAllInv (Regr3D_ScaleInv, Regr3D_ShiftAllInv):\n    # calls Regr3D_ShiftAllInv first, then Regr3D_ScaleInv\n    pass\nclass CalcMetrics(): # note that this is not differentiable\n    def __init__(self, random_crop_size = None, resize = None):\n        from torchmetrics.functional import structural_similarity_index_measure\n        from torchmetrics.image import PeakSignalNoiseRatio",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "Regr3D_ScaleShiftAllIn",
        "kind": 6,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "class Regr3D_ScaleShiftAllInv (Regr3D_ScaleInv, Regr3D_ShiftAllInv):\n    # calls Regr3D_ShiftAllInv first, then Regr3D_ScaleInv\n    pass\nclass CalcMetrics(): # note that this is not differentiable\n    def __init__(self, random_crop_size = None, resize = None):\n        from torchmetrics.functional import structural_similarity_index_measure\n        from torchmetrics.image import PeakSignalNoiseRatio\n        from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n        self.psnr = PeakSignalNoiseRatio(data_range=1.0)\n        self.ssim = structural_similarity_index_measure",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "CalcMetrics",
        "kind": 6,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "class CalcMetrics(): # note that this is not differentiable\n    def __init__(self, random_crop_size = None, resize = None):\n        from torchmetrics.functional import structural_similarity_index_measure\n        from torchmetrics.image import PeakSignalNoiseRatio\n        from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n        self.psnr = PeakSignalNoiseRatio(data_range=1.0)\n        self.ssim = structural_similarity_index_measure\n        self.lpips = LearnedPerceptualImagePatchSimilarity()\n        from torchvision import transforms\n        self.random_crop = transforms.RandomCrop(random_crop_size) if random_crop_size is not None else None",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "GSRenderLos",
        "kind": 6,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "class GSRenderLoss (Criterion, MultiLoss):\n    def __init__(self, criterion, norm_mode='avg_dis', gt_scale=False, mv = False, render_included = False, scale_scaled = True, use_gt_pcd = False, lpips_coeff = 0., rgb_coeff = 1.0, copy_rgb_coeff = 10.0, use_img_rgb = False, cam_relocation = False, local_loss_coeff = 0., lap_loss_coeff = 0.): # criterion = L21, the others are as default\n        super().__init__(criterion)\n        self.norm_mode = norm_mode\n        self.gt_scale = gt_scale\n        self.mv = mv\n        self.compute_loss = None\n        if mv:\n            self.compute_loss = self.compute_loss_mv\n        self.render_included = render_included",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "batched_all_pairs",
        "kind": 2,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "def batched_all_pairs(B, N):\n    # B, N = se3.shape[:2]\n    i1_, i2_ = torch.combinations(torch.arange(N), 2, with_replacement=False).unbind(-1)\n    i1, i2 = [(i[None] + torch.arange(B)[:, None] * N).reshape(-1) for i in [i1_, i2_]]\n    return i1, i2\ndef closed_form_inverse(se3):\n    \"\"\"\n    Computes the inverse of each 4x4 SE3 matrix in the batch.\n    Args:\n    - se3 (Tensor): Nx4x4 tensor of SE3 matrices.",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "closed_form_inverse",
        "kind": 2,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "def closed_form_inverse(se3):\n    \"\"\"\n    Computes the inverse of each 4x4 SE3 matrix in the batch.\n    Args:\n    - se3 (Tensor): Nx4x4 tensor of SE3 matrices.\n    Returns:\n    - Tensor: Nx4x4 tensor of inverted SE3 matrices.\n    \"\"\"\n    R = se3[:, :3, :3]\n    T = se3[:, 3:, :3]",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "rotation_angle",
        "kind": 2,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "def rotation_angle(rot_gt, rot_pred, batch_size=None):\n    # rot_gt, rot_pred (B, 3, 3)\n    try:\n        rel_angle_cos = so3_relative_angle(rot_gt, rot_pred, eps=1e-1)\n    except:\n        R_diff = rot_gt @ rot_pred.transpose(-1, -2)\n        trace_R_diff = R_diff[:,0,0] + R_diff[:,1,1] + R_diff[:,2,2]\n        cos = (trace_R_diff - 1) / 2\n        cos = torch.clamp(cos, -1 + 1e-3, 1 - 1e-3)\n        rel_angle_cos = torch.acos(cos)",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "compare_translation_by_angle",
        "kind": 2,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "def compare_translation_by_angle(t_gt, t, eps=1e-15, default_err=1e6):\n    \"\"\"Normalize the translation vectors and compute the angle between them.\"\"\"\n    t_norm = torch.norm(t, dim=1, keepdim=True)\n    t = t / (t_norm + eps)\n    t_gt_norm = torch.norm(t_gt, dim=1, keepdim=True)\n    t_gt = t_gt / (t_gt_norm + eps)\n    loss_t = torch.clamp_min(1.0 - torch.sum(t * t_gt, dim=1) ** 2, eps)\n    err_t = torch.acos(torch.sqrt(1 - loss_t))\n    err_t[torch.isnan(err_t) | torch.isinf(err_t)] = default_err\n    return err_t",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "translation_angle",
        "kind": 2,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "def translation_angle(tvec_gt, tvec_pred, batch_size=None):\n    # tvec_gt, tvec_pred (B, 3,)\n    rel_tangle_deg = compare_translation_by_angle(tvec_gt, tvec_pred)\n    rel_tangle_deg = rel_tangle_deg * 180.0 / np.pi\n    if batch_size is not None:\n        rel_tangle_deg = rel_tangle_deg.reshape(batch_size, -1)\n    return rel_tangle_deg\ndef camera_to_rel_deg(pred_se3, gt_se3, device, batch_size): # w2c\n    \"\"\"\n    Calculate relative rotation and translation angles between predicted and ground truth cameras.",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "camera_to_rel_deg",
        "kind": 2,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "def camera_to_rel_deg(pred_se3, gt_se3, device, batch_size): # w2c\n    \"\"\"\n    Calculate relative rotation and translation angles between predicted and ground truth cameras.\n    Args:\n    - pred_cameras: Predicted camera.\n    - gt_cameras: Ground truth camera.\n    - accelerator: The device for moving tensors to GPU or others.\n    - batch_size: Number of data samples in one batch.\n    Returns:\n    - rel_rotation_angle_deg, rel_translation_angle_deg: Relative rotation and translation angles in degrees.",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "estimate_focal_knowing_depth",
        "kind": 2,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "def estimate_focal_knowing_depth(pts3d, valid_mask, min_focal=0., max_focal=np.inf):\n    \"\"\" Reprojection method, for when the absolute depth is known:\n        1) estimate the camera focal using a robust estimator\n        2) reproject points onto true rays, minimizing a certain error\n    \"\"\"\n    B, H, W, THREE = pts3d.shape # valid_mask: [1, H, W], bs = 1\n    assert THREE == 3\n    # centered pixel grid\n    pp = torch.tensor([[W/2, H/2]], dtype=torch.float32, device=pts3d.device)\n    pixels = xy_grid(W, H, device=pts3d.device).view(1, -1, 2) - pp.view(-1, 1, 2)  # B,HW,2",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "recursive_concat_collate",
        "kind": 2,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "def recursive_concat_collate(batch):\n    if isinstance(batch[0], torch.Tensor):\n        return torch.cat(batch, dim=0)\n    elif isinstance(batch[0], dict):\n        return {key: recursive_concat_collate([d[key] for d in batch]) for key in batch[0]}\n    elif isinstance(batch[0], list):\n        return [recursive_concat_collate([d[i] for d in batch]) for i in range(len(batch[0]))]\n    else:\n        return batch\ndef recursive_repeat_interleave_collate(data, dim = 0, rp = 1):",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "recursive_repeat_interleave_collate",
        "kind": 2,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "def recursive_repeat_interleave_collate(data, dim = 0, rp = 1):\n    if torch.is_tensor(data):\n        return data.repeat_interleave(rp, dim=dim)\n    elif isinstance(data, dict):\n        return {key: recursive_repeat_interleave_collate(value, dim, rp) for key, value in data.items()}\n    elif isinstance(data, list):\n        return [recursive_repeat_interleave_collate(element, dim, rp) for element in data]\n    elif isinstance(data, tuple):\n        return tuple(recursive_repeat_interleave_collate(element, dim, rp) for element in data)\n    else:",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "combine_dict",
        "kind": 2,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "def combine_dict(dicts, make_list = False):\n    if make_list:\n        dict_all = {k:[] for k in dicts[0].keys()}\n        for dict_i in dicts:\n            for k in dict_i.keys():\n                dict_all[k].append(dict_i[k])\n        return dict_all\n    else:\n        dict_all = deepcopy(dicts[0])\n        for dict_i in dicts[1:]:",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "Sum",
        "kind": 2,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "def Sum(*losses_and_masks):\n    loss, mask = losses_and_masks[0]\n    if loss.ndim > 0:\n        # we are actually returning the loss for every pixels\n        return losses_and_masks\n    else:\n        # we are returning the global loss\n        for loss2, mask2 in losses_and_masks[1:]:\n            # if loss2 is list\n            if isinstance(loss2, list): # for MV use",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "extend_gts",
        "kind": 2,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "def extend_gts(gts, n_ref, bs):\n        gts = recursive_repeat_interleave_collate(gts, 0, n_ref)\n        for data_id in range(bs):\n            for ref_id in range(1, n_ref):\n                for k in gts[0].keys():\n                    recursive_swap(gts[0][k], gts[ref_id][k], data_id * n_ref + ref_id)\n        return gts\ndef swap(a, b):\n    if type(a) == torch.Tensor:\n        return b.clone(), a.clone()",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "swap",
        "kind": 2,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "def swap(a, b):\n    if type(a) == torch.Tensor:\n        return b.clone(), a.clone()\n    else:\n        raise NotImplementedError\ndef swap_ref(a, b):\n    if type(a) == torch.Tensor:\n        temp = a.clone()\n        a[:] = b.clone()\n        b[:] = temp",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "swap_ref",
        "kind": 2,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "def swap_ref(a, b):\n    if type(a) == torch.Tensor:\n        temp = a.clone()\n        a[:] = b.clone()\n        b[:] = temp\n    else:\n        raise NotImplementedError\ndef recursive_swap(a, b, pos):\n    if torch.is_tensor(a):\n        a[pos], b[pos] = swap(a[pos], b[pos])",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "recursive_swap",
        "kind": 2,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "def recursive_swap(a, b, pos):\n    if torch.is_tensor(a):\n        a[pos], b[pos] = swap(a[pos], b[pos])\n    elif isinstance(a, dict):\n        for key in a.keys():\n            recursive_swap(a[key], b[key], pos)\n    elif isinstance(a, list):\n        for i in range(len(a)):\n            recursive_swap(a[i], b[i], pos)\n    elif isinstance(a, tuple):",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "calculate_RRA_RTA",
        "kind": 2,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "def calculate_RRA_RTA(c2w_pred1, c2w_pred2, c2w_gt1, c2w_gt2, eps = 1e-15): # [bs, 3, 3], [bs, 3, 3], [bs, 3, 1], [bs, 3, 1]\n    \"\"\"\n    Return:\n        RRA: [bs,]\n        RTA: [bs,]\n    \"\"\"\n    # R1, R2, R1_gt, R2_gt, t1, t2, t1_gt, t2_gt\n    R1 = c2w_pred1[:, :3, :3]\n    R2 = c2w_pred2[:, :3, :3]\n    R1_gt = c2w_gt1[:, :3, :3]",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "calibrate_camera_pnpransac",
        "kind": 2,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "def calibrate_camera_pnpransac(pointclouds, img_points, masks, intrinsics):\n    \"\"\"\n    Input:\n        pointclouds: (bs, N, 3) \n        img_points: (bs, N, 2) \n    Return:\n        rotations: (bs, 3, 3) \n        translations: (bs, 3, 1) \n        c2ws: (bs, 4, 4) \n    \"\"\"",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "umeyama_alignment",
        "kind": 2,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "def umeyama_alignment(P1, P2, mask_): # [bs, N, 3], [bs, N, 3], [bs, N] all are torch.Tensor\n    \"\"\"\n    Return:\n    R: (bs, 3, 3)\n    sigma: (bs, )\n    t: (bs, 3)\n    \"\"\"\n    from pytorch3d import ops\n    ya = ops.corresponding_points_alignment\n    R, T, s = ya(P1, P2, weights = mask_.float(), estimate_scale = True)",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "chamfer_distance",
        "kind": 2,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "def chamfer_distance(pts1, pts2, mask): # [bs, N, 3], [bs, N, 3], [bs, N]\n    bs = pts1.shape[0]\n    cd = []\n    for i in range(bs):\n        disAB = knn_points(pts1[i:i+1][mask[i:i+1]][None], pts2[i:i+1][mask[i:i+1]][None])[0].mean()\n        disBA = knn_points(pts2[i:i+1][mask[i:i+1]][None], pts1[i:i+1][mask[i:i+1]][None])[0].mean()\n        cd.append(disAB + disBA)\n    cd = torch.stack(cd, 0)\n    return cd\ndef rotationInvMSE(pts3d_normalized, gts3d_normalized, mask_all):",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "rotationInvMSE",
        "kind": 2,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "def rotationInvMSE(pts3d_normalized, gts3d_normalized, mask_all):\n    R, sigma, t = umeyama_alignment(pts3d_normalized, gts3d_normalized, mask_all)\n    pts3d_normalized_rot = (sigma[:,None,None] * (R @ pts3d_normalized.transpose(-1, -2)).transpose(-1, -2)) + t[:, None] # [bs, h*w, 3]\n    local_loss = (pts3d_normalized_rot - gts3d_normalized).norm(dim = -1)[mask_all].mean()\nclass LLoss (nn.Module):\n    \"\"\" L-norm loss\n    \"\"\"\n    def __init__(self, reduction='mean'):\n        super().__init__()\n        self.reduction = reduction",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "L21",
        "kind": 5,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "L21 = L21Loss()\nclass Criterion (nn.Module):\n    def __init__(self, criterion=None):\n        super().__init__()\n        assert isinstance(criterion, LLoss), f'{criterion} is not a proper criterion!'+bb()\n        self.criterion = copy(criterion)\n    def get_name(self):\n        return f'{type(self).__name__}({self.criterion})'\n    def with_reduction(self, mode):\n        res = loss = deepcopy(self)",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "calc_metrics",
        "kind": 5,
        "importPath": "dust3r.losses",
        "description": "dust3r.losses",
        "peekOfCode": "calc_metrics = CalcMetrics(resize = (224, 224))\nclass GSRenderLoss (Criterion, MultiLoss):\n    def __init__(self, criterion, norm_mode='avg_dis', gt_scale=False, mv = False, render_included = False, scale_scaled = True, use_gt_pcd = False, lpips_coeff = 0., rgb_coeff = 1.0, copy_rgb_coeff = 10.0, use_img_rgb = False, cam_relocation = False, local_loss_coeff = 0., lap_loss_coeff = 0.): # criterion = L21, the others are as default\n        super().__init__(criterion)\n        self.norm_mode = norm_mode\n        self.gt_scale = gt_scale\n        self.mv = mv\n        self.compute_loss = None\n        if mv:\n            self.compute_loss = self.compute_loss_mv",
        "detail": "dust3r.losses",
        "documentation": {}
    },
    {
        "label": "AsymmetricCroCo3DStere",
        "kind": 6,
        "importPath": "dust3r.model",
        "description": "dust3r.model",
        "peekOfCode": "class AsymmetricCroCo3DStereo (\n    CroCoNet,\n    huggingface_hub.PyTorchModelHubMixin,\n    library_name=\"dust3r\",\n    repo_url=\"https://github.com/naver/dust3r\",\n    tags=[\"image-to-3d\"],\n):\n    \"\"\" Two siamese encoders, followed by two decoders.\n    The goal is to output 3d points directly, both images in view1's frame\n    (hence the asymmetry).   ",
        "detail": "dust3r.model",
        "documentation": {}
    },
    {
        "label": "AsymmetricCroCo3DStereoMultiVie",
        "kind": 6,
        "importPath": "dust3r.model",
        "description": "dust3r.model",
        "peekOfCode": "class AsymmetricCroCo3DStereoMultiView (\n    CroCoNet,\n    huggingface_hub.PyTorchModelHubMixin,\n    library_name=\"dust3r\",\n    repo_url=\"https://github.com/naver/dust3r\",\n    tags=[\"image-to-3d\"],\n):\n    \"\"\" Two siamese encoders, followed by two decoders.\n    The goal is to output 3d points directly, both images in view1's frame\n    (hence the asymmetry).",
        "detail": "dust3r.model",
        "documentation": {}
    },
    {
        "label": "load_model",
        "kind": 2,
        "importPath": "dust3r.model",
        "description": "dust3r.model",
        "peekOfCode": "def load_model(model_path, device, verbose=True):\n    if verbose:\n        print('... loading model from', model_path)\n    ckpt = torch.load(model_path, map_location='cpu')\n    args = ckpt['args'].model.replace(\"ManyAR_PatchEmbed\", \"PatchEmbedDust3R\")\n    if 'landscape_only' not in args:\n        args = args[:-1] + ', landscape_only=False)'\n    else:\n        args = args.replace(\" \", \"\").replace('landscape_only=True', 'landscape_only=False')\n    assert \"landscape_only=False\" in args",
        "detail": "dust3r.model",
        "documentation": {}
    },
    {
        "label": "except_i",
        "kind": 2,
        "importPath": "dust3r.model",
        "description": "dust3r.model",
        "peekOfCode": "def except_i(a, i):\n    if i == 0:\n        return a[1:]\n    elif i == len(a) - 1:\n        return a[:-1]\n    if type(a) == list:\n        return a[:i] + a[i+1:]\n    return torch.cat([a[:i], a[i+1:]], dim=0)\nclass AsymmetricCroCo3DStereoMultiView (\n    CroCoNet,",
        "detail": "dust3r.model",
        "documentation": {}
    },
    {
        "label": "inf",
        "kind": 5,
        "importPath": "dust3r.model",
        "description": "dust3r.model",
        "peekOfCode": "inf = float('inf')\nhf_version_number = huggingface_hub.__version__\nassert version.parse(hf_version_number) >= version.parse(\"0.22.0\"), \"Outdated huggingface_hub version, please reinstall requirements.txt\"\ndef load_model(model_path, device, verbose=True):\n    if verbose:\n        print('... loading model from', model_path)\n    ckpt = torch.load(model_path, map_location='cpu')\n    args = ckpt['args'].model.replace(\"ManyAR_PatchEmbed\", \"PatchEmbedDust3R\")\n    if 'landscape_only' not in args:\n        args = args[:-1] + ', landscape_only=False)'",
        "detail": "dust3r.model",
        "documentation": {}
    },
    {
        "label": "hf_version_number",
        "kind": 5,
        "importPath": "dust3r.model",
        "description": "dust3r.model",
        "peekOfCode": "hf_version_number = huggingface_hub.__version__\nassert version.parse(hf_version_number) >= version.parse(\"0.22.0\"), \"Outdated huggingface_hub version, please reinstall requirements.txt\"\ndef load_model(model_path, device, verbose=True):\n    if verbose:\n        print('... loading model from', model_path)\n    ckpt = torch.load(model_path, map_location='cpu')\n    args = ckpt['args'].model.replace(\"ManyAR_PatchEmbed\", \"PatchEmbedDust3R\")\n    if 'landscape_only' not in args:\n        args = args[:-1] + ', landscape_only=False)'\n    else:",
        "detail": "dust3r.model",
        "documentation": {}
    },
    {
        "label": "adjust_learning_rate_by_lr",
        "kind": 2,
        "importPath": "dust3r.optim_factory",
        "description": "dust3r.optim_factory",
        "peekOfCode": "def adjust_learning_rate_by_lr(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        if \"lr_scale\" in param_group:\n            param_group[\"lr\"] = lr * param_group[\"lr_scale\"]\n        else:\n            param_group[\"lr\"] = lr",
        "detail": "dust3r.optim_factory",
        "documentation": {}
    },
    {
        "label": "PatchEmbedDust3R",
        "kind": 6,
        "importPath": "dust3r.patch_embed",
        "description": "dust3r.patch_embed",
        "peekOfCode": "class PatchEmbedDust3R(PatchEmbed):\n    def forward(self, x, **kw):\n        B, C, H, W = x.shape\n        assert H % self.patch_size[0] == 0, f\"Input image height ({H}) is not a multiple of patch size ({self.patch_size[0]}).\"\n        assert W % self.patch_size[1] == 0, f\"Input image width ({W}) is not a multiple of patch size ({self.patch_size[1]}).\"\n        x = self.proj(x)\n        pos = self.position_getter(B, x.size(2), x.size(3), x.device)\n        if self.flatten:\n            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC\n        x = self.norm(x)",
        "detail": "dust3r.patch_embed",
        "documentation": {}
    },
    {
        "label": "ManyAR_PatchEmbe",
        "kind": 6,
        "importPath": "dust3r.patch_embed",
        "description": "dust3r.patch_embed",
        "peekOfCode": "class ManyAR_PatchEmbed (PatchEmbed):\n    \"\"\" Handle images with non-square aspect ratio.\n        All images in the same batch have the same aspect ratio.\n        true_shape = [(height, width) ...] indicates the actual shape of each image.\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True):\n        self.embed_dim = embed_dim\n        super().__init__(img_size, patch_size, in_chans, embed_dim, norm_layer, flatten)\n    def forward(self, img, true_shape):\n        B, C, H, W = img.shape",
        "detail": "dust3r.patch_embed",
        "documentation": {}
    },
    {
        "label": "get_patch_embed",
        "kind": 2,
        "importPath": "dust3r.patch_embed",
        "description": "dust3r.patch_embed",
        "peekOfCode": "def get_patch_embed(patch_embed_cls, img_size, patch_size, enc_embed_dim):\n    assert patch_embed_cls in ['PatchEmbedDust3R', 'ManyAR_PatchEmbed']\n    patch_embed = eval(patch_embed_cls)(img_size, patch_size, 3, enc_embed_dim)\n    return patch_embed\nclass PatchEmbedDust3R(PatchEmbed):\n    def forward(self, x, **kw):\n        B, C, H, W = x.shape\n        assert H % self.patch_size[0] == 0, f\"Input image height ({H}) is not a multiple of patch size ({self.patch_size[0]}).\"\n        assert W % self.patch_size[1] == 0, f\"Input image width ({W}) is not a multiple of patch size ({self.patch_size[1]}).\"\n        x = self.proj(x)",
        "detail": "dust3r.patch_embed",
        "documentation": {}
    },
    {
        "label": "spiral_cam_gen",
        "kind": 2,
        "importPath": "dust3r.pcd_render",
        "description": "dust3r.pcd_render",
        "peekOfCode": "def spiral_cam_gen(device, n = 36, dis = 2.7):\n    c2ws = []\n    for i in range(0, 360, 360 // n):\n        R, T = look_at_view_transform(dis, 10, i)\n        c2w = torch.eye(4).to(device)\n        c2w[:3,:3] = R.to(device)\n        c2w[:3,3] = T.to(device).squeeze()\n        c2ws.append(c2w)\n    return torch.stack(c2ws)\ndef pcd_render(pcd, rgb, tgt = None, normalize = False, rot = True, mask = None, debug = False):",
        "detail": "dust3r.pcd_render",
        "documentation": {}
    },
    {
        "label": "pcd_render",
        "kind": 2,
        "importPath": "dust3r.pcd_render",
        "description": "dust3r.pcd_render",
        "peekOfCode": "def pcd_render(pcd, rgb, tgt = None, normalize = False, rot = True, mask = None, debug = False):\n    pcd = pcd.reshape(-1, 3)\n    rgb = rgb.reshape(-1, 3)\n    # print('pcd_render', tgt)\n    # torch.save([pcd, rgb], f\"/home/zgtang/manifold_things/temp/pcd_rgb2.pt\")\n    # pcd[:,1] = pcd[:,1] * -1\n    device = pcd.device\n    if torch.min(rgb) < -0.5:\n        rgb = (rgb + 1) / 2.0\n    if normalize: ",
        "detail": "dust3r.pcd_render",
        "documentation": {}
    },
    {
        "label": "save_video_combined",
        "kind": 2,
        "importPath": "dust3r.pcd_render",
        "description": "dust3r.pcd_render",
        "peekOfCode": "def save_video_combined(images_list, tgt):\n    images = []\n    for i in range(len(images_list[0])):\n        img_row = []\n        for images_list_j in images_list:\n            img_row.append(images_list_j[i])\n        img_row = np.concatenate(img_row, axis = 1) # [h, w (cat here), 3]\n        images.append(img_row)\n    file_name = f\"output_{time.time()}.mp4\"\n    output_video = f'/tmp/{file_name}'",
        "detail": "dust3r.pcd_render",
        "documentation": {}
    },
    {
        "label": "estimate_focal_knowing_depth",
        "kind": 2,
        "importPath": "dust3r.post_process",
        "description": "dust3r.post_process",
        "peekOfCode": "def estimate_focal_knowing_depth(pts3d, pp, focal_mode='median', min_focal=0., max_focal=np.inf):\n    \"\"\" Reprojection method, for when the absolute depth is known:\n        1) estimate the camera focal using a robust estimator\n        2) reproject points onto true rays, minimizing a certain error\n    \"\"\"\n    B, H, W, THREE = pts3d.shape\n    assert THREE == 3\n    # centered pixel grid\n    pixels = xy_grid(W, H, device=pts3d.device).view(1, -1, 2) - pp.view(-1, 1, 2)  # B,HW,2\n    pts3d = pts3d.flatten(1, 2)  # (B, HW, 3)",
        "detail": "dust3r.post_process",
        "documentation": {}
    },
    {
        "label": "SceneViz",
        "kind": 6,
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "peekOfCode": "class SceneViz:\n    def __init__(self):\n        self.scene = trimesh.Scene()\n    def add_pointcloud(self, pts3d, color, mask=None):\n        pts3d = to_numpy(pts3d)\n        mask = to_numpy(mask)\n        if mask is None:\n            mask = [slice(None)] * len(pts3d)\n        pts = np.concatenate([p[m] for p, m in zip(pts3d, mask)])\n        pct = trimesh.PointCloud(pts.reshape(-1, 3))",
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "cat_3d",
        "kind": 2,
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "peekOfCode": "def cat_3d(vecs):\n    if isinstance(vecs, (np.ndarray, torch.Tensor)):\n        vecs = [vecs]\n    return np.concatenate([p.reshape(-1, 3) for p in to_numpy(vecs)])\ndef show_raw_pointcloud(pts3d, colors, point_size=2):\n    scene = trimesh.Scene()\n    pct = trimesh.PointCloud(cat_3d(pts3d), colors=cat_3d(colors))\n    scene.add_geometry(pct)\n    scene.show(line_settings={'point_size': point_size})\ndef pts3d_to_trimesh(img, pts3d, valid=None):",
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "show_raw_pointcloud",
        "kind": 2,
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "peekOfCode": "def show_raw_pointcloud(pts3d, colors, point_size=2):\n    scene = trimesh.Scene()\n    pct = trimesh.PointCloud(cat_3d(pts3d), colors=cat_3d(colors))\n    scene.add_geometry(pct)\n    scene.show(line_settings={'point_size': point_size})\ndef pts3d_to_trimesh(img, pts3d, valid=None):\n    H, W, THREE = img.shape\n    assert THREE == 3\n    assert img.shape == pts3d.shape\n    vertices = pts3d.reshape(-1, 3)",
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "pts3d_to_trimesh",
        "kind": 2,
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "peekOfCode": "def pts3d_to_trimesh(img, pts3d, valid=None):\n    H, W, THREE = img.shape\n    assert THREE == 3\n    assert img.shape == pts3d.shape\n    vertices = pts3d.reshape(-1, 3)\n    # make squares: each pixel == 2 triangles\n    idx = np.arange(len(vertices)).reshape(H, W)\n    idx1 = idx[:-1, :-1].ravel()  # top-left corner\n    idx2 = idx[:-1, +1:].ravel()  # right-left corner\n    idx3 = idx[+1:, :-1].ravel()  # bottom-left corner",
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "cat_meshes",
        "kind": 2,
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "peekOfCode": "def cat_meshes(meshes):\n    vertices, faces, colors = zip(*[(m['vertices'], m['faces'], m['face_colors']) for m in meshes])\n    n_vertices = np.cumsum([0]+[len(v) for v in vertices])\n    for i in range(len(faces)):\n        faces[i][:] += n_vertices[i]\n    vertices = np.concatenate(vertices)\n    colors = np.concatenate(colors)\n    faces = np.concatenate(faces)\n    return dict(vertices=vertices, face_colors=colors, faces=faces)\ndef show_duster_pairs(view1, view2, pred1, pred2):",
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "show_duster_pairs",
        "kind": 2,
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "peekOfCode": "def show_duster_pairs(view1, view2, pred1, pred2):\n    import matplotlib.pyplot as pl\n    pl.ion()\n    for e in range(len(view1['instance'])):\n        i = view1['idx'][e]\n        j = view2['idx'][e]\n        img1 = rgb(view1['img'][e])\n        img2 = rgb(view2['img'][e])\n        conf1 = pred1['conf'][e].squeeze()\n        conf2 = pred2['conf'][e].squeeze()",
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "auto_cam_size",
        "kind": 2,
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "peekOfCode": "def auto_cam_size(im_poses):\n    return 0.1 * get_med_dist_between_poses(im_poses)\nclass SceneViz:\n    def __init__(self):\n        self.scene = trimesh.Scene()\n    def add_pointcloud(self, pts3d, color, mask=None):\n        pts3d = to_numpy(pts3d)\n        mask = to_numpy(mask)\n        if mask is None:\n            mask = [slice(None)] * len(pts3d)",
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "show_raw_pointcloud_with_cams",
        "kind": 2,
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "peekOfCode": "def show_raw_pointcloud_with_cams(imgs, pts3d, mask, focals, cams2world,\n                                  point_size=2, cam_size=0.05, cam_color=None):\n    \"\"\" Visualization of a pointcloud with cameras\n        imgs = (N, H, W, 3) or N-size list of [(H,W,3), ...]\n        pts3d = (N, H, W, 3) or N-size list of [(H,W,3), ...]\n        focals = (N,) or N-size list of [focal, ...]\n        cams2world = (N,4,4) or N-size list of [(4,4), ...]\n    \"\"\"\n    assert len(pts3d) == len(mask) <= len(imgs) <= len(cams2world) == len(focals)\n    pts3d = to_numpy(pts3d)",
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "add_scene_cam",
        "kind": 2,
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "peekOfCode": "def add_scene_cam(scene, pose_c2w, edge_color, image=None, focal=None, imsize=None, screen_width=0.03):\n    if image is not None:\n        H, W, THREE = image.shape\n        assert THREE == 3\n        if image.dtype != np.uint8:\n            image = np.uint8(255*image)\n    elif imsize is not None:\n        W, H = imsize\n    elif focal is not None:\n        H = W = focal / 1.1",
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "cat",
        "kind": 2,
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "peekOfCode": "def cat(a, b):\n    return np.concatenate((a.reshape(-1, 3), b.reshape(-1, 3)))\nOPENGL = np.array([[1, 0, 0, 0],\n                   [0, -1, 0, 0],\n                   [0, 0, -1, 0],\n                   [0, 0, 0, 1]])\nCAM_COLORS = [(255, 0, 0), (0, 0, 255), (0, 255, 0), (255, 0, 255), (255, 204, 0), (0, 204, 204),\n              (128, 255, 255), (255, 128, 255), (255, 255, 128), (0, 0, 0), (128, 128, 128)]\ndef uint8(colors):\n    if not isinstance(colors, np.ndarray):",
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "uint8",
        "kind": 2,
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "peekOfCode": "def uint8(colors):\n    if not isinstance(colors, np.ndarray):\n        colors = np.array(colors)\n    if np.issubdtype(colors.dtype, np.floating):\n        colors *= 255\n    assert 0 <= colors.min() and colors.max() < 256\n    return np.uint8(colors)\ndef segment_sky(image):\n    import cv2\n    from scipy import ndimage",
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "segment_sky",
        "kind": 2,
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "peekOfCode": "def segment_sky(image):\n    import cv2\n    from scipy import ndimage\n    # Convert to HSV\n    image = to_numpy(image)\n    if np.issubdtype(image.dtype, np.floating):\n        image = np.uint8(255*image.clip(min=0, max=1))\n    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    # Define range for blue color and create mask\n    lower_blue = np.array([0, 0, 100])",
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "OPENGL",
        "kind": 5,
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "peekOfCode": "OPENGL = np.array([[1, 0, 0, 0],\n                   [0, -1, 0, 0],\n                   [0, 0, -1, 0],\n                   [0, 0, 0, 1]])\nCAM_COLORS = [(255, 0, 0), (0, 0, 255), (0, 255, 0), (255, 0, 255), (255, 204, 0), (0, 204, 204),\n              (128, 255, 255), (255, 128, 255), (255, 255, 128), (0, 0, 0), (128, 128, 128)]\ndef uint8(colors):\n    if not isinstance(colors, np.ndarray):\n        colors = np.array(colors)\n    if np.issubdtype(colors.dtype, np.floating):",
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "CAM_COLORS",
        "kind": 5,
        "importPath": "dust3r.viz",
        "description": "dust3r.viz",
        "peekOfCode": "CAM_COLORS = [(255, 0, 0), (0, 0, 255), (0, 255, 0), (255, 0, 255), (255, 204, 0), (0, 204, 204),\n              (128, 255, 255), (255, 128, 255), (255, 255, 128), (0, 0, 0), (128, 128, 128)]\ndef uint8(colors):\n    if not isinstance(colors, np.ndarray):\n        colors = np.array(colors)\n    if np.issubdtype(colors.dtype, np.floating):\n        colors *= 255\n    assert 0 <= colors.min() and colors.max() < 256\n    return np.uint8(colors)\ndef segment_sky(image):",
        "detail": "dust3r.viz",
        "documentation": {}
    },
    {
        "label": "CSlamChunk",
        "kind": 6,
        "importPath": "create_chunks",
        "description": "create_chunks",
        "peekOfCode": "class CSlamChunk:\n    def __init__(self, scene_dir, index):\n        self.identity = ''.join(random.choices(string.ascii_lowercase + string.digits, k=4))\n        self.scene_dir = scene_dir + fio.sep + self.identity\n        fio.ensure_dir(self.scene_dir)\n        self.__ply_path = fio.createPath(fio.sep, [scene_dir], \"model.ply\")\n        self.__glb_path = fio.createPath(fio.sep, [scene_dir], \"model.glb\")\n        self.global_index = index\n        self.parent_index = -1\n        self.tags = ['pts3d', 'msk', 'focals', 'cams2world', 'intrinsics', 'images', '3d_models']",
        "detail": "create_chunks",
        "documentation": {}
    },
    {
        "label": "CSlamPredictor",
        "kind": 6,
        "importPath": "create_chunks",
        "description": "create_chunks",
        "peekOfCode": "class CSlamPredictor:\n    # Constructor to initialize variables\n    def __init__(self, model, device):\n        self.model = model\n        self.device = device\n        self.scenes = []\n        self.scenes_info = []\n    def combine_scenes(self):\n        global_scene_chunk = None\n        last_frame = None",
        "detail": "create_chunks",
        "documentation": {}
    },
    {
        "label": "select_random_chunk",
        "kind": 2,
        "importPath": "create_chunks",
        "description": "create_chunks",
        "peekOfCode": "def select_random_chunk(my_list, min_size=12, max_size=24, step=3, start_index=None):\n    if step < 1:\n        step = 1\n    selection_size = random.randint(min_size, max_size)\n    max_start_index = len(my_list) - (selection_size - 1) * step\n    if max_start_index <= 0:\n        max_start_index = len(my_list) - 1\n    if start_index is None:\n        start_index = random.randint(0, max_start_index)\n    random_indices = list(range(start_index, start_index + selection_size * step, step))",
        "detail": "create_chunks",
        "documentation": {}
    },
    {
        "label": "os.environ[\"meta_internal\"]",
        "kind": 5,
        "importPath": "create_chunks",
        "description": "create_chunks",
        "peekOfCode": "os.environ[\"meta_internal\"] = \"False\"\nimport copy\nfrom copy import deepcopy\nimport numpy as np\nimport torch\nimport trimesh\nimport open3d as o3d\nfrom scipy.spatial.transform import Rotation\nfrom dust3r.inference import inference_mv\nfrom dust3r.losses import calibrate_camera_pnpransac, estimate_focal_knowing_depth",
        "detail": "create_chunks",
        "documentation": {}
    },
    {
        "label": "torch.backends.cuda.matmul.allow_tf32",
        "kind": 5,
        "importPath": "create_chunks",
        "description": "create_chunks",
        "peekOfCode": "torch.backends.cuda.matmul.allow_tf32 = True  # for gpu >= Ampere and pytorch >= 1.12\nbatch_size = 1\ninf = np.inf\ndef select_random_chunk(my_list, min_size=12, max_size=24, step=3, start_index=None):\n    if step < 1:\n        step = 1\n    selection_size = random.randint(min_size, max_size)\n    max_start_index = len(my_list) - (selection_size - 1) * step\n    if max_start_index <= 0:\n        max_start_index = len(my_list) - 1",
        "detail": "create_chunks",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "create_chunks",
        "description": "create_chunks",
        "peekOfCode": "batch_size = 1\ninf = np.inf\ndef select_random_chunk(my_list, min_size=12, max_size=24, step=3, start_index=None):\n    if step < 1:\n        step = 1\n    selection_size = random.randint(min_size, max_size)\n    max_start_index = len(my_list) - (selection_size - 1) * step\n    if max_start_index <= 0:\n        max_start_index = len(my_list) - 1\n    if start_index is None:",
        "detail": "create_chunks",
        "documentation": {}
    },
    {
        "label": "inf",
        "kind": 5,
        "importPath": "create_chunks",
        "description": "create_chunks",
        "peekOfCode": "inf = np.inf\ndef select_random_chunk(my_list, min_size=12, max_size=24, step=3, start_index=None):\n    if step < 1:\n        step = 1\n    selection_size = random.randint(min_size, max_size)\n    max_start_index = len(my_list) - (selection_size - 1) * step\n    if max_start_index <= 0:\n        max_start_index = len(my_list) - 1\n    if start_index is None:\n        start_index = random.randint(0, max_start_index)",
        "detail": "create_chunks",
        "documentation": {}
    },
    {
        "label": "CSlamChunk",
        "kind": 6,
        "importPath": "create_mesh",
        "description": "create_mesh",
        "peekOfCode": "class CSlamChunk:\n    def __init__(self, scene_dir, index):\n        self.identity = ''.join(random.choices(string.ascii_lowercase + string.digits, k=4))\n        self.scene_dir = scene_dir \n        fio.ensure_dir(self.scene_dir)\n        self.__ply_path = fio.createPath(fio.sep, [scene_dir], \"model.ply\")\n        self.__glb_path = fio.createPath(fio.sep, [scene_dir], \"model.glb\")\n        self.global_index = index\n        self.parent_index = -1\n        self.tags = ['pts3d', 'msk', 'focals', 'cams2world', 'intrinsics', 'images', '3d_models']",
        "detail": "create_mesh",
        "documentation": {}
    },
    {
        "label": "CSlamPredictor",
        "kind": 6,
        "importPath": "create_mesh",
        "description": "create_mesh",
        "peekOfCode": "class CSlamPredictor:\n    # Constructor to initialize variables\n    def __init__(self, model, device):\n        self.model = model\n        self.device = device\n        self.scenes = []\n        self.scenes_info = []\n    def combine_scenes(self):\n        global_scene_chunk = None\n        last_frame = None",
        "detail": "create_mesh",
        "documentation": {}
    },
    {
        "label": "select_random_chunk",
        "kind": 2,
        "importPath": "create_mesh",
        "description": "create_mesh",
        "peekOfCode": "def select_random_chunk(my_list, min_size=12, max_size=24, step=1, start_index=None):\n    if step < 1:\n        step = 1\n    selection_size = random.randint(min_size, max_size)\n    max_start_index = len(my_list) - (selection_size - 1) * step\n    if max_start_index <= 0:\n        max_start_index = len(my_list) - 1\n    if start_index is None:\n        start_index = random.randint(0, max_start_index)\n    random_indices = list(range(start_index, start_index + selection_size * step, step))",
        "detail": "create_mesh",
        "documentation": {}
    },
    {
        "label": "extract_number_foldername",
        "kind": 2,
        "importPath": "create_mesh",
        "description": "create_mesh",
        "peekOfCode": "def extract_number_foldername(folder_path):\n    # Extract the folder name (assuming it's the last part of the path)\n    folder_name = folder_path.split(\"/\")[-1]\n    # Use regex to find the numeric part of the folder name\n    match = re.search(r'\\d+', folder_name)\n    return int(match.group()) if match else float('inf')  # Return infinity if no number is found\ndef extract_number_filename(file_path):\n    # Use regular expression to find the numeric part\n    match = re.search(r'\\d+', file_path)\n    return int(match.group()) if match else 0",
        "detail": "create_mesh",
        "documentation": {}
    },
    {
        "label": "extract_number_filename",
        "kind": 2,
        "importPath": "create_mesh",
        "description": "create_mesh",
        "peekOfCode": "def extract_number_filename(file_path):\n    # Use regular expression to find the numeric part\n    match = re.search(r'\\d+', file_path)\n    return int(match.group()) if match else 0\nclass CSlamChunk:\n    def __init__(self, scene_dir, index):\n        self.identity = ''.join(random.choices(string.ascii_lowercase + string.digits, k=4))\n        self.scene_dir = scene_dir \n        fio.ensure_dir(self.scene_dir)\n        self.__ply_path = fio.createPath(fio.sep, [scene_dir], \"model.ply\")",
        "detail": "create_mesh",
        "documentation": {}
    },
    {
        "label": "os.environ[\"meta_internal\"]",
        "kind": 5,
        "importPath": "create_mesh",
        "description": "create_mesh",
        "peekOfCode": "os.environ[\"meta_internal\"] = \"False\"\nimport copy\nfrom copy import deepcopy\nimport numpy as np\nimport re\nimport torch\nimport trimesh\nimport open3d as o3d\nfrom scipy.spatial.transform import Rotation\nfrom dust3r.inference import inference_mv",
        "detail": "create_mesh",
        "documentation": {}
    },
    {
        "label": "torch.backends.cuda.matmul.allow_tf32",
        "kind": 5,
        "importPath": "create_mesh",
        "description": "create_mesh",
        "peekOfCode": "torch.backends.cuda.matmul.allow_tf32 = True  # for gpu >= Ampere and pytorch >= 1.12\nbatch_size = 1\ninf = np.inf\ndef select_random_chunk(my_list, min_size=12, max_size=24, step=1, start_index=None):\n    if step < 1:\n        step = 1\n    selection_size = random.randint(min_size, max_size)\n    max_start_index = len(my_list) - (selection_size - 1) * step\n    if max_start_index <= 0:\n        max_start_index = len(my_list) - 1",
        "detail": "create_mesh",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "create_mesh",
        "description": "create_mesh",
        "peekOfCode": "batch_size = 1\ninf = np.inf\ndef select_random_chunk(my_list, min_size=12, max_size=24, step=1, start_index=None):\n    if step < 1:\n        step = 1\n    selection_size = random.randint(min_size, max_size)\n    max_start_index = len(my_list) - (selection_size - 1) * step\n    if max_start_index <= 0:\n        max_start_index = len(my_list) - 1\n    if start_index is None:",
        "detail": "create_mesh",
        "documentation": {}
    },
    {
        "label": "inf",
        "kind": 5,
        "importPath": "create_mesh",
        "description": "create_mesh",
        "peekOfCode": "inf = np.inf\ndef select_random_chunk(my_list, min_size=12, max_size=24, step=1, start_index=None):\n    if step < 1:\n        step = 1\n    selection_size = random.randint(min_size, max_size)\n    max_start_index = len(my_list) - (selection_size - 1) * step\n    if max_start_index <= 0:\n        max_start_index = len(my_list) - 1\n    if start_index is None:\n        start_index = random.randint(0, max_start_index)",
        "detail": "create_mesh",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "demo",
        "description": "demo",
        "peekOfCode": "def get_args_parser():\n    parser = argparse.ArgumentParser()\n    parser_url = parser.add_mutually_exclusive_group()\n    parser_url.add_argument(\"--local_network\", action='store_true', default=False,\n                            help=\"make app accessible on local network: address will be set to 0.0.0.0\")\n    parser_url.add_argument(\"--server_name\", type=str, default=None, help=\"server url, default is 127.0.0.1\")\n    parser.add_argument(\"--image_size\", type=int, default=224, help=\"image size (note, we do not train and test on other resolutions yet, this should not be changed)\")\n    parser.add_argument(\"--server_port\", type=int, help=\"will start gradio app on this port (if available).\",\n                        default=7860)\n    parser_weights = parser.add_mutually_exclusive_group(required=True)",
        "detail": "demo",
        "documentation": {}
    },
    {
        "label": "get_3D_model_from_scene",
        "kind": 2,
        "importPath": "demo",
        "description": "demo",
        "peekOfCode": "def get_3D_model_from_scene(outdir, silent, output, min_conf_thr=3, as_pointcloud=False, transparent_cams=False, cam_size=0.05, only_model=False):\n    \"\"\"\n    extract 3D_model (glb file) from a reconstructed scene\n    \"\"\"\n    with torch.no_grad():\n        _, h, w = output['pred1']['rgb'].shape[0:3] # [1, H, W, 3]\n        rgbimg = [output['pred1']['rgb'][0]] + [x['rgb'][0] for x in output['pred2s']]\n        for i in range(len(rgbimg)):\n            rgbimg[i] = (rgbimg[i] + 1) / 2\n        pts3d = [output['pred1']['pts3d'][0]] + [x['pts3d_in_other_view'][0] for x in output['pred2s']]",
        "detail": "demo",
        "documentation": {}
    },
    {
        "label": "get_reconstructed_scene",
        "kind": 2,
        "importPath": "demo",
        "description": "demo",
        "peekOfCode": "def get_reconstructed_scene(outdir, model, device, silent, image_size, filelist, min_conf_thr,\n                            as_pointcloud, transparent_cams, cam_size, n_frame):\n    \"\"\"\n    from a list of images, run dust3r inference, global aligner.\n    then run get_3D_model_from_scene\n    \"\"\"\n    imgs = load_images(filelist, size=image_size, verbose=not silent, n_frame = n_frame)\n    if len(imgs) == 1:\n        imgs = [imgs[0], copy.deepcopy(imgs[0])]\n        imgs[1]['idx'] = 1",
        "detail": "demo",
        "documentation": {}
    },
    {
        "label": "set_scenegraph_options",
        "kind": 2,
        "importPath": "demo",
        "description": "demo",
        "peekOfCode": "def set_scenegraph_options(inputfiles, winsize, refid, scenegraph_type):\n    num_files = len(inputfiles) if inputfiles is not None else 1\n    max_winsize = max(1, math.ceil((num_files-1)/2))\n    if scenegraph_type == \"swin\":\n        winsize = gradio.Slider(label=\"Scene Graph: Window Size\", value=max_winsize,\n                                minimum=1, maximum=max_winsize, step=1, visible=True)\n        refid = gradio.Slider(label=\"Scene Graph: Id\", value=0, minimum=0,\n                              maximum=num_files-1, step=1, visible=False)\n    elif scenegraph_type == \"oneref\":\n        winsize = gradio.Slider(label=\"Scene Graph: Window Size\", value=max_winsize,",
        "detail": "demo",
        "documentation": {}
    },
    {
        "label": "main_demo",
        "kind": 2,
        "importPath": "demo",
        "description": "demo",
        "peekOfCode": "def main_demo(tmpdirname, model, device, image_size, server_name, server_port, silent=False):\n    recon_fun = functools.partial(get_reconstructed_scene, tmpdirname, model, device, silent, image_size)\n    model_from_scene_fun = functools.partial(get_3D_model_from_scene, tmpdirname, silent, only_model = True)\n    with gradio.Blocks(css=\"\"\".gradio-container {margin: 0 !important; min-width: 100%};\"\"\", title=\"MV-DUSt3R+ Demo\", theme=\"default\") as demo:\n        # scene state is save so that you can change conf_thr, cam_size... without rerunning the inference\n        scene = gradio.State(None)\n        gradio.HTML('<h2 style=\"text-align: center;\">MV-DUSt3R+ Demo</h2>')\n        with gradio.Column():\n            inputfiles = gradio.File(file_count=\"multiple\")\n            run_btn = gradio.Button(\"Run\")",
        "detail": "demo",
        "documentation": {}
    },
    {
        "label": "inf",
        "kind": 5,
        "importPath": "demo",
        "description": "demo",
        "peekOfCode": "inf = np.inf\nimport sys\nsys.path.insert(0, os.path.join(os.path.dirname(__file__)))\ntry:\n    from meta_internal.io import *\n    os.environ[\"meta_internal\"] = \"True\"\nexcept:\n    from dust3r.dummy_io import *\n    os.environ[\"meta_internal\"] = \"False\"\nimport matplotlib.pyplot as pl",
        "detail": "demo",
        "documentation": {}
    },
    {
        "label": "torch.backends.cuda.matmul.allow_tf32",
        "kind": 5,
        "importPath": "demo",
        "description": "demo",
        "peekOfCode": "torch.backends.cuda.matmul.allow_tf32 = True  # for gpu >= Ampere and pytorch >= 1.12\nbatch_size = 1\ndef get_args_parser():\n    parser = argparse.ArgumentParser()\n    parser_url = parser.add_mutually_exclusive_group()\n    parser_url.add_argument(\"--local_network\", action='store_true', default=False,\n                            help=\"make app accessible on local network: address will be set to 0.0.0.0\")\n    parser_url.add_argument(\"--server_name\", type=str, default=None, help=\"server url, default is 127.0.0.1\")\n    parser.add_argument(\"--image_size\", type=int, default=224, help=\"image size (note, we do not train and test on other resolutions yet, this should not be changed)\")\n    parser.add_argument(\"--server_port\", type=int, help=\"will start gradio app on this port (if available).\",",
        "detail": "demo",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "demo",
        "description": "demo",
        "peekOfCode": "batch_size = 1\ndef get_args_parser():\n    parser = argparse.ArgumentParser()\n    parser_url = parser.add_mutually_exclusive_group()\n    parser_url.add_argument(\"--local_network\", action='store_true', default=False,\n                            help=\"make app accessible on local network: address will be set to 0.0.0.0\")\n    parser_url.add_argument(\"--server_name\", type=str, default=None, help=\"server url, default is 127.0.0.1\")\n    parser.add_argument(\"--image_size\", type=int, default=224, help=\"image size (note, we do not train and test on other resolutions yet, this should not be changed)\")\n    parser.add_argument(\"--server_port\", type=int, help=\"will start gradio app on this port (if available).\",\n                        default=7860)",
        "detail": "demo",
        "documentation": {}
    },
    {
        "label": "loss_of_one_batch_go_mv",
        "kind": 2,
        "importPath": "inference_global_optimization",
        "description": "inference_global_optimization",
        "peekOfCode": "def loss_of_one_batch_go_mv(batch, model, criterion, device, symmetrize_batch=False, use_amp=False, ret=None):\n    views = batch\n    view1, view2s = views[0], views[1:]\n    for view in batch:\n        for name in 'img pts3d valid_mask camera_pose camera_intrinsics F_matrix corres'.split():  # pseudo_focal\n            if name not in view:\n                continue\n            view[name] = view[name].to(device, non_blocking=True)\n    t1s, t2s = [], []\n    with torch.cuda.amp.autocast(enabled=bool(use_amp)):",
        "detail": "inference_global_optimization",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "inference_global_optimization",
        "description": "inference_global_optimization",
        "peekOfCode": "def get_args_parser():\n    parser = argparse.ArgumentParser()\n    parser_url = parser.add_mutually_exclusive_group()\n    parser_url.add_argument(\"--local_network\", action='store_true', default=False,\n                            help=\"make app accessible on local network: address will be set to 0.0.0.0\")\n    parser_url.add_argument(\"--server_name\", type=str, default=None, help=\"server url, default is 127.0.0.1\")\n    parser.add_argument(\"--image_size\", type=int, default=512, choices=[512, 224], help=\"image size\")\n    parser.add_argument(\"--server_port\", type=int, help=(\"will start gradio app on this port (if available). \"\n                                                         \"If None, will search for an available port starting at 7860.\"),\n                        default=None)",
        "detail": "inference_global_optimization",
        "documentation": {}
    },
    {
        "label": "get_3D_model_from_scene",
        "kind": 2,
        "importPath": "inference_global_optimization",
        "description": "inference_global_optimization",
        "peekOfCode": "def get_3D_model_from_scene(outdir, silent, scene, min_conf_thr=3, as_pointcloud=False, mask_sky=False,\n                            clean_depth=False, transparent_cams=False, cam_size=0.05):\n    \"\"\"\n    extract 3D_model (glb file) from a reconstructed scene\n    \"\"\"\n    if scene is None:\n        return None\n    # post processes\n    if clean_depth:\n        scene = scene.clean_pointcloud()",
        "detail": "inference_global_optimization",
        "documentation": {}
    },
    {
        "label": "get_reconstructed_scene",
        "kind": 2,
        "importPath": "inference_global_optimization",
        "description": "inference_global_optimization",
        "peekOfCode": "def get_reconstructed_scene(outdir, model, device, silent, image_size, filelist):\n    \"\"\"\n    from a list of images, run dust3r inference, global aligner.\n    then run get_3D_model_from_scene\n    \"\"\"\n    schedule = \"linear\"\n    niter = 300\n    min_conf_thr = 3\n    as_pointcloud = True\n    mask_sky = False",
        "detail": "inference_global_optimization",
        "documentation": {}
    },
    {
        "label": "Rt",
        "kind": 2,
        "importPath": "inference_global_optimization",
        "description": "inference_global_optimization",
        "peekOfCode": "def Rt(M, p):\n    return M[:3,3] + p @ M[:3,:3].T\ndef inference_global_optimization(model, device, silent, img_tensors, first_view_c2w): # (model), cuda, False, 512, [...,...]\n    \"\"\"\n    from a list of images, run dust3r inference, global aligner.\n    then run get_3D_model_from_scene\n    \"\"\"\n    schedule = \"linear\"\n    niter = 300\n    min_conf_thr = 3",
        "detail": "inference_global_optimization",
        "documentation": {}
    },
    {
        "label": "inference_global_optimization",
        "kind": 2,
        "importPath": "inference_global_optimization",
        "description": "inference_global_optimization",
        "peekOfCode": "def inference_global_optimization(model, device, silent, img_tensors, first_view_c2w): # (model), cuda, False, 512, [...,...]\n    \"\"\"\n    from a list of images, run dust3r inference, global aligner.\n    then run get_3D_model_from_scene\n    \"\"\"\n    schedule = \"linear\"\n    niter = 300\n    min_conf_thr = 3\n    as_pointcloud = True\n    mask_sky = False",
        "detail": "inference_global_optimization",
        "documentation": {}
    },
    {
        "label": "set_scenegraph_options",
        "kind": 2,
        "importPath": "inference_global_optimization",
        "description": "inference_global_optimization",
        "peekOfCode": "def set_scenegraph_options(inputfiles, winsize, refid, scenegraph_type):\n    num_files = len(inputfiles) if inputfiles is not None else 1\n    max_winsize = max(1, math.ceil((num_files-1)/2))\n    if scenegraph_type == \"swin\":\n        winsize = gradio.Slider(label=\"Scene Graph: Window Size\", value=max_winsize,\n                                minimum=1, maximum=max_winsize, step=1, visible=True)\n        refid = gradio.Slider(label=\"Scene Graph: Id\", value=0, minimum=0,\n                              maximum=num_files-1, step=1, visible=False)\n    elif scenegraph_type == \"oneref\":\n        winsize = gradio.Slider(label=\"Scene Graph: Window Size\", value=max_winsize,",
        "detail": "inference_global_optimization",
        "documentation": {}
    },
    {
        "label": "main_demo",
        "kind": 2,
        "importPath": "inference_global_optimization",
        "description": "inference_global_optimization",
        "peekOfCode": "def main_demo(tmpdirname, model, device, image_size, server_name, server_port, silent=False):\n    recon_fun = functools.partial(get_reconstructed_scene, tmpdirname, model, device, silent, image_size)\n    recon_fun([\"/home/zgtang/manifold_things/sample_img/vis_0_0.png\", \"/home/zgtang/manifold_things/sample_img/vis_0_1.png\", \"/home/zgtang/manifold_things/sample_img/vis_0_0.png\", \"/home/zgtang/manifold_things/sample_img/vis_0_1.png\"])\n    model_from_scene_fun = functools.partial(get_3D_model_from_scene, tmpdirname, silent)\n    recon_fun(inputfiles, schedule, niter, min_conf_thr, as_pointcloud,\n                mask_sky, clean_depth, transparent_cams, cam_size,\n                scenegraph_type, winsize, refid)\nif __name__ == '__main__':\n    parser = get_args_parser()\n    args = parser.parse_args()",
        "detail": "inference_global_optimization",
        "documentation": {}
    },
    {
        "label": "torch.backends.cuda.matmul.allow_tf32",
        "kind": 5,
        "importPath": "inference_global_optimization",
        "description": "inference_global_optimization",
        "peekOfCode": "torch.backends.cuda.matmul.allow_tf32 = True  # for gpu >= Ampere and pytorch >= 1.12\nbatch_size = 1\nfrom dust3r.pcd_render import pcd_render\ndef loss_of_one_batch_go_mv(batch, model, criterion, device, symmetrize_batch=False, use_amp=False, ret=None):\n    views = batch\n    view1, view2s = views[0], views[1:]\n    for view in batch:\n        for name in 'img pts3d valid_mask camera_pose camera_intrinsics F_matrix corres'.split():  # pseudo_focal\n            if name not in view:\n                continue",
        "detail": "inference_global_optimization",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "inference_global_optimization",
        "description": "inference_global_optimization",
        "peekOfCode": "batch_size = 1\nfrom dust3r.pcd_render import pcd_render\ndef loss_of_one_batch_go_mv(batch, model, criterion, device, symmetrize_batch=False, use_amp=False, ret=None):\n    views = batch\n    view1, view2s = views[0], views[1:]\n    for view in batch:\n        for name in 'img pts3d valid_mask camera_pose camera_intrinsics F_matrix corres'.split():  # pseudo_focal\n            if name not in view:\n                continue\n            view[name] = view[name].to(device, non_blocking=True)",
        "detail": "inference_global_optimization",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "inference_global_optimization_batch",
        "description": "inference_global_optimization_batch",
        "peekOfCode": "def get_args_parser():\n    parser = argparse.ArgumentParser('DUST3R training', add_help=False)\n    # model and criterion\n    parser.add_argument('--model', default=\"AsymmetricCroCo3DStereo(patch_embed_cls='ManyAR_PatchEmbed')\",\n                        type=str, help=\"string containing the model to build\")\n    parser.add_argument('--pretrained', default=None, help='path of a starting checkpoint')\n    parser.add_argument('--train_criterion', default=\"ConfLoss(Regr3D(L21, norm_mode='avg_dis'), alpha=0.2)\",\n                        type=str, help=\"train criterion\")\n    parser.add_argument('--test_criterion', default=None, type=str, help=\"test criterion\")\n    # dataset",
        "detail": "inference_global_optimization_batch",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "inference_global_optimization_batch",
        "description": "inference_global_optimization_batch",
        "peekOfCode": "def main(args):\n    print('args', args)\n    misc.init_distributed_mode(args)\n    global_rank = misc.get_rank()\n    world_size = misc.get_world_size()\n    real_batch_size = args.batch_size * world_size\n    print('world size', world_size, 'global_rank', global_rank, 'real_batch_size', real_batch_size)\n    set_device(args.gpu)\n    args.output_dir = get_log_dir_warp(args.output_dir)\n    print(\"output_dir: \"+args.output_dir) # manifold://ondevice_ai_writedata/tree/zgtang/dust3r/logs/torchx-dust3r_train-temp3",
        "detail": "inference_global_optimization_batch",
        "documentation": {}
    },
    {
        "label": "build_dataset",
        "kind": 2,
        "importPath": "inference_global_optimization_batch",
        "description": "inference_global_optimization_batch",
        "peekOfCode": "def build_dataset(dataset, batch_size, num_workers, test=False):\n    split = ['Train', 'Test'][test]\n    print(f'Building {split} Data loader for dataset: ', dataset)\n    loader = get_data_loader(dataset,\n                             batch_size=batch_size,\n                             num_workers=num_workers,\n                             pin_mem=True,\n                             shuffle=not (test),\n                             drop_last=not (test))\n    print(f\"{split} dataset length: \", len(loader))",
        "detail": "inference_global_optimization_batch",
        "documentation": {}
    },
    {
        "label": "save_results",
        "kind": 2,
        "importPath": "inference_global_optimization_batch",
        "description": "inference_global_optimization_batch",
        "peekOfCode": "def save_results(loss_and_others, batch, name_list, args):\n    all_info = loss_and_others\n    other_info = loss_and_others['loss'][1]\n    # view1: img (real_bs * 2 (data aug for symmetry), 3, res=224, res), depthmap, camera_pose (real_bs * 2, 4, 4), camera_intrinsics, dataset, label, instance, idx, true_shape, pts3d (real_bs * 2, res, res, 3), valid_mask, rng\n    # pred1: pts3d, conf\n    # pred2: pts3d_in_other_view, conf\n    g_pathmgr.mkdirs(args.output_dir + '/results')\n    g_pathmgr.mkdirs(args.output_dir + '/videos')\n    bs = all_info['view1']['img'].shape[0] # real_bs * 2 = bs\n    if 'view2s' in all_info.keys(): # MV here",
        "detail": "inference_global_optimization_batch",
        "documentation": {}
    },
    {
        "label": "add_first_best",
        "kind": 2,
        "importPath": "inference_global_optimization_batch",
        "description": "inference_global_optimization_batch",
        "peekOfCode": "def add_first_best(loss_details, n_ref):\n    # import fbvscode\n    # fbvscode.set_trace()\n    ldk = list(loss_details.keys())\n    for k in ldk:\n        if k == 'loss':\n            continue\n        if \"_list\" in k:\n            x_list = np.array(loss_details[k])\n            k_base = k.replace('_list', '')",
        "detail": "inference_global_optimization_batch",
        "documentation": {}
    },
    {
        "label": "test_one_epoch",
        "kind": 2,
        "importPath": "inference_global_optimization_batch",
        "description": "inference_global_optimization_batch",
        "peekOfCode": "def test_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n                   data_loader: Sized, device: torch.device, epoch: int,\n                   train_epoch_size, args, log_writer=None, prefix='test', test_set_id = 0):\n    t_begin1 = -time.time()\n    model.eval()\n    metric_logger = misc.MetricLogger(delimiter=\"  \")\n    metric_logger.meters = defaultdict(lambda: misc.SmoothedValue(window_size=9**9))\n    header = 'Test Epoch: [{}]'.format(epoch)\n    if log_writer is not None:\n        print('log_dir: {}'.format(log_writer.log_dir))",
        "detail": "inference_global_optimization_batch",
        "documentation": {}
    },
    {
        "label": "torch.backends.cuda.matmul.allow_tf32",
        "kind": 5,
        "importPath": "inference_global_optimization_batch",
        "description": "inference_global_optimization_batch",
        "peekOfCode": "torch.backends.cuda.matmul.allow_tf32 = True  # for gpu >= Ampere and pytorch >= 1.12\nimport sys\nsys.path.insert(0, os.path.join(os.path.dirname(__file__)))\nif 'META_INTERNAL' in os.environ.keys() and os.environ['META_INTERNAL'] == \"False\":\n    generate_html = None\n    from dust3r.dummy_io import *\nelse:\n    from meta_internal.io import *\n    from meta_internal.html_gen.run_model_doctor import generate_html\nfrom dust3r.model import AsymmetricCroCo3DStereo, AsymmetricCroCo3DStereoMultiView, inf ",
        "detail": "inference_global_optimization_batch",
        "documentation": {}
    },
    {
        "label": "get_current_timestamp",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def get_current_timestamp(format_str=''):\n    if format_str == '':\n        # Return timestamp as integer in milliseconds\n        return int(time.time() * 1000)\n    else:\n        # Return formatted timestamp string\n        return datetime.now().strftime(format_str)\n'''\n2. File extension\n'''",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "get_filename_components",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def get_filename_components(file_path):\n    file = ''\n    ext = ''\n    file_dir = ''\n    (file_dir, file) = os.path.split(file_path)\n    name_combo = str(file).split('.')\n    if len(name_combo) >= 2:\n        ext = name_combo[-1]\n        file = file.replace('.' + ext, '')\n    elif len(name_combo) == 1:",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "filter_ext",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def filter_ext(filepath_list, filter_out_target=False, ext_set=None):\n    if ext_set is None:\n        return\n    unwanted_elems = list()\n    for pi_path in filepath_list:\n        (file_dir, file, ext) = get_filename_components(pi_path)\n        target_exist = False\n        if (ext in ext_set) and len(ext) > 0:\n            target_exist = True\n        if (target_exist == True) and (filter_out_target == True):",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "filter_folder",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def filter_folder(fid_folder_list, filter_out=False, filter_text=''):\n    rslt = []\n    for ff in fid_folder_list:\n        if (filter_text in ff) and (filter_out == False):\n            rslt.append(ff)\n        elif (not (filter_text in ff)) and (filter_out == True):\n            rslt.append(ff)\n    return rslt\ndef filter_if_dir(filepath_list, filter_out_target=False):\n    #  if isinstance(directory, list):",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "filter_if_dir",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def filter_if_dir(filepath_list, filter_out_target=False):\n    #  if isinstance(directory, list):\n    rslt = []\n    for ff in filepath_list:\n        if file_exist(ff) == False:\n            continue\n        if (os.path.isdir(ff)) and (filter_out_target == False):\n            # need to return folder\n            rslt.append(ff)\n        elif (os.path.isfile(ff)) and (filter_out_target == True):",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "replace_file_ext",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def replace_file_ext(filepath, new_ext, full_path=True, replace_save=False):\n    if (full_path):\n        (filedir, file, ext) = get_filename_components(filepath)\n        new_file = filedir + sep + file + '.' + new_ext\n        if replace_save:\n            move_file(filepath, new_file)\n        return new_file\n    else:\n        (file, ext) = str(filepath).split('.')\n        new_file = file + '.' + new_ext",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "createPath",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def createPath(separator, list_of_dir, file_name=\"\"):\n    if len(list_of_dir) <= 0:\n        return \"\"\n    while '' in list_of_dir:\n        list_of_dir.remove('')\n    path_rslt = separator.join(list_of_dir)\n    if len(file_name) <= 0:\n        return path_rslt\n    else:\n        return path_rslt + separator + file_name",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "getParentDir",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def getParentDir(target_path=''):\n    if len(target_path) < 1:\n        current_path = os.path.dirname(os.path.abspath('__file__'))\n        return current_path\n    else:\n        current_path = os.path.dirname(os.path.abspath(target_path))\n        return current_path\ndef getGrandParentDir(target_path=''):\n    if len(target_path) < 1:\n        current_path = os.path.dirname(os.path.abspath('__file__'))",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "getGrandParentDir",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def getGrandParentDir(target_path=''):\n    if len(target_path) < 1:\n        current_path = os.path.dirname(os.path.abspath('__file__'))\n        return os.path.abspath(os.path.join(current_path,os.path.pardir))\n    else:\n        current_path = os.path.dirname(os.path.abspath(target_path))\n        return os.path.abspath(os.path.join(current_path,os.path.pardir))\ndef traverse_dir(dir, full_path=False, towards_sub=False):\n    rslt = list()\n    if towards_sub == False:",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "traverse_dir",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def traverse_dir(dir, full_path=False, towards_sub=False):\n    rslt = list()\n    if towards_sub == False:\n        file_list = os.listdir(dir)\n        for file_name in file_list:\n            if full_path:\n                rslt.append(os.path.join(dir, file_name))\n            else:\n                rslt.append(file_name)\n        return rslt",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "get_nextsub_from_dir_list",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def get_nextsub_from_dir_list(dir_list, full_path=False):\n    rslt = {}\n    for dir in dir_list:\n        if '.DS_Store' in dir:\n            continue\n        sub_list = traverse_dir(dir, full_path=full_path)\n        sub_list = filter_ext(sub_list, filter_out_target=True, ext_set='.DS_Store')\n        rslt[dir] = sub_list\n    return rslt\ndef file_exist(file_path):",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "file_exist",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def file_exist(file_path):\n    if 'NA' == file_path:\n        return False\n    if (file_path == file_path) == False:\n        return False\n    return os.path.exists(file_path)\ndef check_file_permission(file_path, destination_path):\n    if os.access(file_path, os.R_OK) or os.access(destination_path, os.W_OK):\n        return True\n    else:",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "check_file_permission",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def check_file_permission(file_path, destination_path):\n    if os.access(file_path, os.R_OK) or os.access(destination_path, os.W_OK):\n        return True\n    else:\n        # print(\"Permission denied for file movement.\", file_path)\n        return False\n'''\n4. File/folder copy, paste, delete and create\n'''\ndef ensure_dir(directory):",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "ensure_dir",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def ensure_dir(directory):\n    try:\n        if isinstance(directory, list):\n            directory = createPath(sep, directory)\n        if not os.path.exists(directory):\n            os.makedirs(directory, exist_ok=True)\n    except:\n        print(\"FATAL ERROR: ENSURE_DIR in file_io.py!\")\ndef create_timestamped_folder(base_path=\".\"):\n    \"\"\"",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "create_timestamped_folder",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def create_timestamped_folder(base_path=\".\"):\n    \"\"\"\n    Create a folder with the current timestamp in the format YYYY-MM-DD.\n    If the folder already exists, append a unique number like YYYY-MM-DD_0, YYYY-MM-DD_1, etc.\n    :param base_path: The directory in which to create the folder (default is the current directory).\n    :return: The name of the created folder.\n    \"\"\"\n    # Get current date in YYYY-MM-DD format\n    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n    # Start with the base folder name",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "generate_unique_path",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def generate_unique_path(base_path):\n    \"\"\"\n    Append `_1`, `_2`, etc., to the given path until a unique path is found.\n    Parameters:\n        base_path (str): The initial file or directory path.\n    Returns:\n        str: A unique path that does not already exist.\n    \"\"\"\n    if not os.path.exists(base_path):\n        return base_path  # If the base path doesn't exist, return it as is.",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "delete_file",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def delete_file(path):\n    if os.path.exists(path):  #if file exists\n        os.remove(path)\n    else:\n        print('no such file:%s' % path)\ndef delete_folder(path):\n    if os.path.exists(path):\n        if len(os.listdir(path)) == 0:\n            os.removedirs(path)\n        else:",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "delete_folder",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def delete_folder(path):\n    if os.path.exists(path):\n        if len(os.listdir(path)) == 0:\n            os.removedirs(path)\n        else:\n            try:\n                shutil.rmtree(path)\n            except OSError as e:\n                print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n    else:",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "move_file",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def move_file(from_dir, to_dir, required_ext=''):\n    try:\n        if len(required_ext) < 1:\n            shutil.move(from_dir, to_dir)\n        else:\n            cp_func = '*.' + required_ext\n            shutil.move(from_dir, to_dir, cp_func)\n    except Exception as e:\n        print('ERROR: ', e)\ndef copy_file(from_dir, to_dir, is_dir=False, required_ext=''):",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "copy_file",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def copy_file(from_dir, to_dir, is_dir=False, required_ext=''):\n    if len(required_ext) < 1:\n        if is_dir:\n            shutil.copytree(from_dir, to_dir)\n        else:\n            shutil.copy(from_dir, to_dir)\n    else:\n        cp_func = '*.' + required_ext\n        shutil.copy(from_dir, to_dir, cp_func)\ndef copy_folder(from_dir, to_dir):",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "copy_folder",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def copy_folder(from_dir, to_dir):\n    copy_tree(from_dir, to_dir)\n'''\n5. File savings and readings: to csv\n'''\ndef save_df_to_csv(rslt_df, file_path, mode='a+', encode='utf_8', breakline='', write_head=True):\n    try:\n        header = list(rslt_df.head())\n        with open(file_path, mode, encoding=encode, newline=breakline) as f:\n            writer = csv.writer(f)",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "save_df_to_csv",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def save_df_to_csv(rslt_df, file_path, mode='a+', encode='utf_8', breakline='', write_head=True):\n    try:\n        header = list(rslt_df.head())\n        with open(file_path, mode, encoding=encode, newline=breakline) as f:\n            writer = csv.writer(f)\n            if write_head:\n                header = list(rslt_df.head())\n                writer.writerow(header)\n            for index, row in rslt_df.iterrows():\n                writer.writerow(row)",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "save_dict_to_csv",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def save_dict_to_csv(dict_to_save, file_path, mode='a', encode='utf_8', breakline=''):\n    keyword_list = dict_to_save.keys()\n    try:\n        if not os.path.exists(file_path):\n            with open(file_path, \"w\", newline='', encoding='utf-8') as csvfile:\n                writer = csv.DictWriter(csvfile, fieldnames=keyword_list)\n                writer.writeheader()\n        with open(file_path, mode=mode, newline=breakline, encoding=encode) as csvfile:\n            writer = csv.DictWriter(csvfile, fieldnames=keyword_list)\n            writer.writerow(dict_to_save)",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "save_list_to_csv",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def save_list_to_csv(list_to_save, file_path, mode='a', encode='utf_8'):\n    try:\n        with open(file_path, mode, encoding=encode, newline='') as f:\n            write = csv.writer(f)\n            rows = [[data] for data in list_to_save]\n            write.writerows(rows)\n    except Exception as e:\n        print(\"write error==>\", e)\n        pass\n'''",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "save_dict_to_txt",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def save_dict_to_txt(dictionary, filename, mode='w', encode='utf_8'):\n    try:\n        f = open(filename, mode, encoding=encode)\n        f.write(str(dictionary))\n        f.close()\n    except Exception as e:\n        print(\"write error==>\", e)\n        pass    \ndef read_dict_from_txt(filename, mode='r', encode='utf_8'):\n    try:",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "read_dict_from_txt",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def read_dict_from_txt(filename, mode='r', encode='utf_8'):\n    try:\n        f = open(filename, mode, encoding=encode)\n        a = f.read()\n        dictionary = eval(a)\n        f.close()\n        return dictionary\n    except Exception as e:\n        print(\"write error==>\", e)\n        pass",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "save_str_to_txt",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def save_str_to_txt(str_to_save, file_path, mode='a', encode='utf_8', breakline=''):\n    try:\n        if not os.path.exists(file_path):\n            with open(file_path, \"w\", newline='', encoding='utf-8') as f:\n                f.write(str_to_save)\n        else:\n            with open(file_path, mode=mode, newline=breakline, encoding=encode) as f:\n                f.write(str_to_save)\n    except Exception as e:\n        print(\"write error==>\", e)",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "save_list_to_txt",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def save_list_to_txt(list_to_save, file_path, mode='a', encode='utf_8'):\n    try:\n        with open(file_path, mode, encoding=encode, newline='') as f:\n            for item in list_to_save:\n                list_to_save.write(f\"\\n{item}\")\n    except Exception as e:\n        print(\"write error save_list_to_txt ==>\", e)\n        pass      \ndef read_list_from_txt(filename, mode='r', encode='utf_8'):\n    try:",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "read_list_from_txt",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def read_list_from_txt(filename, mode='r', encode='utf_8'):\n    try:\n        with open(filename, mode, encoding=encode) as f:\n            lines = f.readlines()\n            newlines =[x.strip() for x in lines]\n            return newlines\n    except Exception as e:\n        print(\"read error save_list_to_txt ==>\", e)\n        pass   \n'''",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "image_to_dataframe",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def image_to_dataframe(image_path):\n    frame = inspect.currentframe()\n    print('Running func: {} -- {}', inspect.getframeinfo(frame).function, image_path)\n    colourImg = Image.open(image_path)\n    colourPixels = colourImg.convert(\"RGB\")\n    colourArray = np.array(colourPixels.getdata()).reshape(colourImg.size + (3,))\n    indicesArray = np.moveaxis(np.indices(colourImg.size), 0, 2)\n    allArray = np.dstack((indicesArray, colourArray)).reshape((-1, 5))\n    source_df = pd.DataFrame(allArray, columns=[\"y\", \"x\", \"R\", \"G\", \"B\"])\n    return source_df",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "generate_random_string",
        "kind": 2,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "def generate_random_string(rs_len=8):\n    letters = string.ascii_letters\n    result_str = ''.join(random.choice(letters) for i in range(rs_len))\n    print(\"Random string of length\", rs_len, \"is:\", result_str)\n    return result_str",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "sep",
        "kind": 5,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "sep = os.sep\nimg_ext_set = ['jpg', 'jpeg', 'png', 'JPG', 'JPEG', 'PNG', 'bmp', 'BMP']\ndef get_current_timestamp(format_str=''):\n    if format_str == '':\n        # Return timestamp as integer in milliseconds\n        return int(time.time() * 1000)\n    else:\n        # Return formatted timestamp string\n        return datetime.now().strftime(format_str)\n'''",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "img_ext_set",
        "kind": 5,
        "importPath": "root_file_io",
        "description": "root_file_io",
        "peekOfCode": "img_ext_set = ['jpg', 'jpeg', 'png', 'JPG', 'JPEG', 'PNG', 'bmp', 'BMP']\ndef get_current_timestamp(format_str=''):\n    if format_str == '':\n        # Return timestamp as integer in milliseconds\n        return int(time.time() * 1000)\n    else:\n        # Return formatted timestamp string\n        return datetime.now().strftime(format_str)\n'''\n2. File extension",
        "detail": "root_file_io",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "def get_args_parser():\n    parser = argparse.ArgumentParser('DUST3R training', add_help=False)\n    # model and criterion\n    parser.add_argument('--model', default=\"AsymmetricCroCo3DStereo(patch_embed_cls='ManyAR_PatchEmbed')\",\n                        type=str, help=\"string containing the model to build\")\n    parser.add_argument('--pretrained', default=None, help='path of a starting checkpoint')\n    parser.add_argument('--train_criterion', default=\"ConfLoss(Regr3D(L21, norm_mode='avg_dis'), alpha=0.2)\",\n                        type=str, help=\"train criterion\")\n    parser.add_argument('--test_criterion', default=None, type=str, help=\"test criterion\")\n    # dataset",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "def main(args):\n    print('args', args)\n    misc.init_distributed_mode(args)\n    global_rank = misc.get_rank()\n    world_size = misc.get_world_size()\n    real_batch_size = args.batch_size * world_size\n    print('world size', world_size, 'global_rank', global_rank, 'real_batch_size', real_batch_size)\n    set_device(args.gpu) # 0\n    args.output_dir = get_log_dir_warp(args.output_dir)\n    print(\"output_dir: \"+args.output_dir)",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "save_final_model",
        "kind": 2,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "def save_final_model(args, epoch, model_without_ddp, best_so_far=None):\n    checkpoint_path = os.path.join(args.output_dir, 'checkpoint-final.pth')\n    to_save = {\n        'args': args,\n        'model': model_without_ddp if isinstance(model_without_ddp, dict) else model_without_ddp.cpu().state_dict(),\n        'epoch': epoch\n    }\n    if best_so_far is not None:\n        to_save['best_so_far'] = best_so_far\n    print(f'>> Saving model to {checkpoint_path} ...')",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "build_dataset",
        "kind": 2,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "def build_dataset(dataset, batch_size, num_workers, test=False):\n    split = ['Train', 'Test'][test]\n    print(f'Building {split} Data loader for dataset: ', dataset)\n    loader = get_data_loader(dataset,\n                             batch_size=batch_size,\n                             num_workers=num_workers,\n                             pin_mem=True,\n                             shuffle=not (test),\n                             drop_last=not (test))\n    print(f\"{split} dataset length: \", len(loader))",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "add_first_best",
        "kind": 2,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "def add_first_best(loss_details, n_ref):\n    # import fbvscode\n    # fbvscode.set_trace()\n    ldk = list(loss_details.keys())\n    for k in ldk:\n        if k == 'loss':\n            continue\n        if \"_list\" in k:\n            x_list = np.array(loss_details[k])\n            k_base = k.replace('_list', '')",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "postprocess_batch",
        "kind": 2,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "def postprocess_batch(batch): # here the randomized number of inference views / number of rendered views are applied to the whole batch.\n    nv, nr = batch[0]['random_nv_nr'][0].cpu().numpy() # we are always using the first sample's No. of views / No. of rendered views and apply it to all samples in the batch\n    while len(batch) > nv:\n        del batch[-1]\n    batch = batch[:nv]\n    ni = nv - nr\n    for i in range(ni):\n        batch[i]['only_render'][:] = False\n    for i in range(ni, nv):\n        batch[i]['only_render'][:] = True",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "train_one_epoch",
        "kind": 2,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n                    data_loader: Sized, optimizer: torch.optim.Optimizer,\n                    device: torch.device, epoch: int, loss_scaler, epoch_size,\n                    args,\n                    log_writer=None):\n    t_all = -time.time()\n    assert torch.backends.cuda.matmul.allow_tf32 == True\n    t_misc_1 = -time.time()\n    model.train(True)\n    metric_logger = misc.MetricLogger(delimiter=\"  \")",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "save_results",
        "kind": 2,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "def save_results(loss_and_others, batch, name_list, args):\n    all_info = loss_and_others\n    other_info = loss_and_others['loss'][1]\n    # view1: img (real_bs * 2 (data aug for symmetry), 3, res=224, res), depthmap, camera_pose (real_bs * 2, 4, 4), camera_intrinsics, dataset, label, instance, idx, true_shape, pts3d (real_bs * 2, res, res, 3), valid_mask, rng\n    # pred1: pts3d, conf\n    # pred2: pts3d_in_other_view, conf\n    g_pathmgr.mkdirs(args.output_dir + '/results')\n    g_pathmgr.mkdirs(args.output_dir + '/videos')\n    bs = all_info['view1']['img'].shape[0] # real_bs * 2 = bs\n    if 'view2s' in all_info.keys(): # MV here",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "to_device",
        "kind": 2,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "def to_device(data, device):\n    if torch.is_tensor(data):\n        return data.to(device)\n    elif isinstance(data, dict):\n        return {key: to_device(value, device) for key, value in data.items()}\n    elif isinstance(data, list):\n        return [to_device(element, device) for element in data]\n    elif isinstance(data, tuple):\n        return tuple(to_device(element, device) for element in data)\n    else:",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "update_batch",
        "kind": 2,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "def update_batch(batch, loss_and_others, data_loader):\n    views = [loss_and_others['view1']] + loss_and_others['view2s']\n    ids = views[0]['idx'][0]\n    pts_pred = [loss_and_others['pred1']['pts3d']] + [x['pts3d_in_other_view'] for x in loss_and_others['pred2s']] # [bs, res, res, 3] each\n    pts_pred = torch.stack(pts_pred, dim = 1) # [bs, n_inference, res, res, 3]\n    pts_pred_center_view = pts_pred.mean(dim = (2,3)) # [bs, n_inference, 3]\n    pts_pred_center = pts_pred_center_view.mean(dim = 1) # [bs, 3]\n    view_dis = torch.norm(pts_pred_center_view - pts_pred_center.unsqueeze(1), dim = 2) # [bs, n_inference]\n    nearest_view_id = view_dis.argmin(dim = 1) # [bs]\n    new_batch = [data_loader.dataset.__getitem_bsvd__(x.item(), y.item()) for x, y in zip(ids.long(), nearest_view_id)]",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "test_one_epoch",
        "kind": 2,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "def test_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,\n                   data_loader: Sized, device: torch.device, epoch: int,\n                   train_epoch_size, args, log_writer=None, prefix='test', miter = False, test_set_id = 0):\n    t_begin1 = -time.time()\n    model.eval()\n    metric_logger = misc.MetricLogger(delimiter=\"  \")\n    metric_logger.meters = defaultdict(lambda: misc.SmoothedValue(window_size=9**9))\n    header = 'Test Epoch: [{}]'.format(epoch)\n    if log_writer is not None:\n        print('log_dir: {}'.format(log_writer.log_dir))",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "torch.backends.cuda.matmul.allow_tf32",
        "kind": 5,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "torch.backends.cuda.matmul.allow_tf32 = True  # for gpu >= Ampere and pytorch >= 1.12\nfrom dust3r.model import AsymmetricCroCo3DStereo, AsymmetricCroCo3DStereoMultiView, inf  # noqa: F401, needed when loading the model\nfrom dust3r.datasets import get_data_loader  # noqa\nfrom dust3r.losses import *  # noqa: F401, needed when loading the model\nfrom dust3r.inference import loss_of_one_batch  # noqa\nfrom dust3r.pcd_render import pcd_render, save_video_combined\nfrom dust3r.gs import gs_render\nfrom dust3r.utils.geometry import inv, geotrf\nimport dust3r.utils.path_to_croco  # noqa: F401\nimport croco.utils.misc as misc  # noqa",
        "detail": "train",
        "documentation": {}
    }
]